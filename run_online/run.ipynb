{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# TNBC Drug Cytotoxicity Prediction Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4fbbf5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7f5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pubchempy as pcp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import TransformerConv, global_mean_pool, global_max_pool\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "project_root = Path.cwd()\n",
    "data_dir = project_root / \"data\" / \"raw\"\n",
    "output_dir = project_root\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathway_scores_raw = pd.read_csv(data_dir / \"cell_ge.csv\", index_col=0)\n",
    "gdsc2_df = pd.read_excel(data_dir / \"GDSC2 Fitted Dose Response Oct 27 2023.xlsx\")\n",
    "model_df = pd.read_csv(data_dir / \"DepMap Model Data.csv\")\n",
    "drug_smiles = pd.read_csv(data_dir / \"drugs_with_smiles.csv\")\n",
    "\n",
    "rmse_col = [col for col in gdsc2_df.columns if 'RMSE' in col.upper()][0]\n",
    "gdsc2_filtered = gdsc2_df[gdsc2_df[rmse_col] < 0.3].copy()\n",
    "\n",
    "drug_response = gdsc2_filtered[['DRUG_NAME', 'CELL_LINE_NAME', 'LN_IC50', 'COSMIC_ID']].copy()\n",
    "drug_response.columns = ['DrugName', 'CellLineName', 'LN_IC50', 'COSMICID']\n",
    "\n",
    "pathway_names = pathway_scores_raw.index.tolist()\n",
    "pathway_data = pathway_scores_raw.T.reset_index()\n",
    "pathway_data.columns = ['CellLineName'] + pathway_names\n",
    "\n",
    "cell_name_to_modelid = dict(zip(\n",
    "    model_df['StrippedCellLineName'].str.upper().str.replace('-', '').str.replace('_', ''),\n",
    "    model_df['ModelID']\n",
    "))\n",
    "cosmic_to_modelid = model_df.drop_duplicates(subset='COSMICID', keep='first').set_index('COSMICID')['ModelID'].to_dict()\n",
    "\n",
    "pathway_data['ModelID'] = pathway_data['CellLineName'].apply(\n",
    "    lambda x: cell_name_to_modelid.get(str(x).upper().replace('-', '').replace('_', ''), None)\n",
    ")\n",
    "pathway_data = pathway_data[pathway_data['ModelID'].notna()].copy()\n",
    "\n",
    "drug_response['ModelID'] = drug_response['COSMICID'].apply(lambda x: cosmic_to_modelid.get(x, None))\n",
    "unmapped = drug_response[drug_response['ModelID'].isna()]\n",
    "if len(unmapped) > 0:\n",
    "    drug_response.loc[drug_response['ModelID'].isna(), 'ModelID'] = drug_response.loc[drug_response['ModelID'].isna(), 'CellLineName'].apply(\n",
    "        lambda x: cell_name_to_modelid.get(str(x).upper().replace('-', '').replace('_', ''), None)\n",
    "    )\n",
    "drug_response = drug_response[drug_response['ModelID'].notna()].drop(columns=['COSMICID'])\n",
    "\n",
    "pan_cancer_pathway = drug_response.merge(\n",
    "    pathway_data[['ModelID'] + pathway_names],\n",
    "    on='ModelID',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "drug_smiles_renamed = drug_smiles.rename(columns={'DRUG_NAME': 'DrugName'})\n",
    "pan_cancer_pathway = pan_cancer_pathway.merge(\n",
    "    drug_smiles_renamed[['DrugName', 'SMILES']],\n",
    "    on='DrugName',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "pan_cancer_pathway = pan_cancer_pathway[pan_cancer_pathway['LN_IC50'].notna()].copy()\n",
    "\n",
    "pan_cancer_pathway = pan_cancer_pathway.merge(\n",
    "    model_df[['ModelID', 'StrippedCellLineName', 'OncotreeLineage', 'OncotreePrimaryDisease']],\n",
    "    on='ModelID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "drug_name_col = 'DrugName'\n",
    "ln_ic50_col = 'LN_IC50'\n",
    "pathway_cols = pathway_names\n",
    "drugs_with_smiles = drug_smiles_renamed[['DrugName', 'SMILES']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atom_features(atom):\n",
    "    \"\"\"\n",
    "    Extract atom features for GNN.\n",
    "    \n",
    "    Args:\n",
    "        atom: RDKit atom object\n",
    "        \n",
    "    Returns:\n",
    "        List of atom features\n",
    "    \"\"\"\n",
    "    features = [\n",
    "        atom.GetAtomicNum(),\n",
    "        atom.GetDegree(),\n",
    "        atom.GetFormalCharge(),\n",
    "        atom.GetHybridization().real,\n",
    "        atom.GetIsAromatic(),\n",
    "        atom.GetTotalNumHs(),\n",
    "        atom.GetNumRadicalElectrons(),\n",
    "        atom.IsInRing(),\n",
    "        atom.GetChiralTag().real,\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def get_bond_features(bond):\n",
    "    \"\"\"\n",
    "    Extract bond features for GNN.\n",
    "    \n",
    "    Args:\n",
    "        bond: RDKit bond object\n",
    "        \n",
    "    Returns:\n",
    "        List of bond features\n",
    "    \"\"\"\n",
    "    features = [\n",
    "        bond.GetBondTypeAsDouble(),\n",
    "        bond.GetIsConjugated(),\n",
    "        bond.IsInRing(),\n",
    "        bond.GetStereo().real,\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def smiles_to_graph(smiles_string):\n",
    "    \"\"\"\n",
    "    Convert SMILES string to PyTorch Geometric graph.\n",
    "    \n",
    "    Args:\n",
    "        smiles_string: SMILES representation of molecule\n",
    "        \n",
    "    Returns:\n",
    "        torch_geometric.data.Data object or None if invalid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles_string)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        \n",
    "        mol = Chem.AddHs(mol)\n",
    "        \n",
    "        node_features = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            node_features.append(get_atom_features(atom))\n",
    "        \n",
    "        if len(node_features) == 0:\n",
    "            return None\n",
    "            \n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "        \n",
    "        edge_indices = []\n",
    "        edge_features = []\n",
    "        \n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            \n",
    "            edge_indices.append([i, j])\n",
    "            edge_indices.append([j, i])\n",
    "            \n",
    "            bond_feats = get_bond_features(bond)\n",
    "            edge_features.append(bond_feats)\n",
    "            edge_features.append(bond_feats)\n",
    "        \n",
    "        if len(edge_indices) == 0:\n",
    "            edge_indices = [[0, 0]]\n",
    "            edge_features = [[0.0] * 4]\n",
    "        \n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "        \n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        \n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_drugs(drugs_df, drug_name_col, smiles_col, save_path):\n",
    "    \"\"\"\n",
    "    Convert all drugs to graphs and cache.\n",
    "    \n",
    "    Args:\n",
    "        drugs_df: DataFrame with drug names and SMILES\n",
    "        drug_name_col: Column name for drug names\n",
    "        smiles_col: Column name for SMILES strings\n",
    "        save_path: Path to save cached graphs\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping drug names to graph data\n",
    "    \"\"\"\n",
    "    drug_graphs = {}\n",
    "    failed_drugs = []\n",
    "    \n",
    "    for idx, row in tqdm(drugs_df.iterrows(), total=len(drugs_df), desc=\"Converting SMILES\"):\n",
    "        drug_name = row[drug_name_col]\n",
    "        smiles = row[smiles_col]\n",
    "        \n",
    "        graph = smiles_to_graph(smiles)\n",
    "        \n",
    "        if graph is not None:\n",
    "            drug_graphs[drug_name] = {\n",
    "                'drug_name': drug_name,\n",
    "                'smiles': smiles,\n",
    "                'graph_data': graph,\n",
    "                'node_dim': graph.x.shape[1],\n",
    "                'edge_dim': graph.edge_attr.shape[1] if graph.edge_attr is not None else 0,\n",
    "                'num_atoms': graph.x.shape[0],\n",
    "                'num_bonds': graph.edge_index.shape[1]\n",
    "            }\n",
    "        else:\n",
    "            failed_drugs.append(drug_name)\n",
    "    \n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(drug_graphs, f)\n",
    "    \n",
    "    return drug_graphs\n",
    "\n",
    "drug_graphs_path = data_dir.parent / \"processed\" / \"drug_graphs.pkl\"\n",
    "drug_graphs = preprocess_drugs(drugs_with_smiles, drugs_with_smiles.columns[0], 'SMILES', drug_graphs_path)\n",
    "\n",
    "valid_drugs = set(drug_graphs.keys())\n",
    "pan_cancer_pathway = pan_cancer_pathway[pan_cancer_pathway['DrugName'].isin(valid_drugs)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbee4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_line_kfold_split(dataframe, n_splits=5, random_seed=42):\n",
    "    \"\"\"\n",
    "    K-fold cross-validation split by cell lines (no data leakage).\n",
    "    \n",
    "    Args:\n",
    "        dataframe: DataFrame with ModelID column\n",
    "        n_splits: Number of folds\n",
    "        random_seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        List of (train_idx, val_idx) tuples for each fold\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    cell_lines = dataframe['ModelID'].unique()\n",
    "    n_cells = len(cell_lines)\n",
    "    shuffled_cells = np.random.permutation(cell_lines)\n",
    "    \n",
    "    fold_size = n_cells // n_splits\n",
    "    folds = []\n",
    "    \n",
    "    for i in range(n_splits):\n",
    "        start_idx = i * fold_size\n",
    "        end_idx = (i + 1) * fold_size if i < n_splits - 1 else n_cells\n",
    "        \n",
    "        val_cells = set(shuffled_cells[start_idx:end_idx])\n",
    "        train_cells = set(shuffled_cells[:start_idx]) | set(shuffled_cells[end_idx:])\n",
    "        \n",
    "        train_idx = dataframe[dataframe['ModelID'].isin(train_cells)].index.values\n",
    "        val_idx = dataframe[dataframe['ModelID'].isin(val_cells)].index.values\n",
    "        \n",
    "        train_cell_set = set(dataframe.loc[train_idx, 'ModelID'].unique())\n",
    "        val_cell_set = set(dataframe.loc[val_idx, 'ModelID'].unique())\n",
    "        \n",
    "        assert len(train_cell_set & val_cell_set) == 0, f\"Cell line overlap in fold {i+1}!\"\n",
    "        \n",
    "        folds.append((train_idx, val_idx))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "def train_with_kfold_cv(\n",
    "    model_class,\n",
    "    model_init_kwargs,\n",
    "    dataset,\n",
    "    device,\n",
    "    phase_name,\n",
    "    n_splits=5,\n",
    "    drug_graphs=None,\n",
    "    drug_col='DrugName',\n",
    "    pathway_cols=None,\n",
    "    batch_size=256,\n",
    "    use_prebatched=True,\n",
    "    num_epochs=50,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    patience=20,\n",
    "    scheduler_patience=5,\n",
    "    output_dir=None,\n",
    "    random_seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train model with k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        model_class: Model class to instantiate\n",
    "        model_init_kwargs: Dictionary of kwargs for model initialization\n",
    "        dataset: DataFrame with data\n",
    "        device: Device to train on\n",
    "        phase_name: Name of the phase\n",
    "        n_splits: Number of folds\n",
    "        drug_graphs: Dictionary of drug graphs\n",
    "        drug_col: Column name for drugs\n",
    "        pathway_cols: List of pathway columns\n",
    "        batch_size: Batch size\n",
    "        use_prebatched: Whether to use pre-batched data\n",
    "        num_epochs: Maximum epochs\n",
    "        lr: Learning rate\n",
    "        weight_decay: Weight decay\n",
    "        patience: Early stopping patience\n",
    "        scheduler_patience: LR scheduler patience\n",
    "        output_dir: Directory for outputs\n",
    "        random_seed: Random seed\n",
    "        \n",
    "    Returns:\n",
    "        List of fold results (metrics, histories, models)\n",
    "    \"\"\"\n",
    "    if output_dir is None:\n",
    "        output_dir = Path.cwd()\n",
    "    \n",
    "    folds = cell_line_kfold_split(dataset, n_splits=n_splits, random_seed=random_seed)\n",
    "    fold_results = []\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"K-Fold Cross-Validation ({n_splits} folds)\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(folds):\n",
    "        print(f\"\\nFold {fold_idx+1}/{n_splits}\")\n",
    "        print(f\"Train: {len(train_idx)} samples, Val: {len(val_idx)} samples\")\n",
    "        \n",
    "        train_dataset = DrugResponsePathwayDataset(\n",
    "            dataset.iloc[train_idx].reset_index(drop=True),\n",
    "            drug_graphs,\n",
    "            drug_col=drug_col,\n",
    "            pathway_cols=pathway_cols\n",
    "        )\n",
    "        val_dataset = DrugResponsePathwayDataset(\n",
    "            dataset.iloc[val_idx].reset_index(drop=True),\n",
    "            drug_graphs,\n",
    "            drug_col=drug_col,\n",
    "            pathway_cols=pathway_cols\n",
    "        )\n",
    "        \n",
    "        loaders = create_pathway_dataloaders(\n",
    "            train_dataset,\n",
    "            np.arange(len(train_dataset)),\n",
    "            np.arange(len(train_dataset)),\n",
    "            np.arange(len(val_dataset)),\n",
    "            batch_size=batch_size,\n",
    "            use_prebatched=use_prebatched,\n",
    "            phase_name=f\"{phase_name}_fold{fold_idx+1}\",\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        model = model_class(**model_init_kwargs).to(device)\n",
    "        checkpoint_path = output_dir / \"models\" / f\"{phase_name}_fold{fold_idx+1}.pt\"\n",
    "        \n",
    "        model, history = train_phase_pathway(\n",
    "            model=model,\n",
    "            train_loader=loaders['train'],\n",
    "            val_loader=loaders['val'],\n",
    "            device=device,\n",
    "            phase_name=f\"{phase_name} - Fold {fold_idx+1}/{n_splits}\",\n",
    "            num_epochs=num_epochs,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            patience=patience,\n",
    "            scheduler_patience=scheduler_patience,\n",
    "            checkpoint_path=str(checkpoint_path),\n",
    "            prebatched_datasets=loaders.get('datasets', None)\n",
    "        )\n",
    "        \n",
    "        val_result = evaluate_model(model, loaders['val'], device, is_prebatched=use_prebatched)\n",
    "        \n",
    "        fold_results.append({\n",
    "            'fold': fold_idx + 1,\n",
    "            'history': history,\n",
    "            'val_metrics': val_result['metrics'],\n",
    "            'checkpoint_path': checkpoint_path,\n",
    "            'model': model\n",
    "        })\n",
    "        \n",
    "        print(f\"Fold {fold_idx+1} Val R²: {val_result['metrics']['r2']:.4f}\")\n",
    "    \n",
    "    avg_r2 = np.mean([r['val_metrics']['r2'] for r in fold_results])\n",
    "    std_r2 = np.std([r['val_metrics']['r2'] for r in fold_results])\n",
    "    avg_pearson = np.mean([r['val_metrics']['pearson'] for r in fold_results])\n",
    "    std_pearson = np.std([r['val_metrics']['pearson'] for r in fold_results])\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"K-Fold CV Results ({n_splits} folds)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Average Val R²: {avg_r2:.4f} ± {std_r2:.4f}\")\n",
    "    print(f\"Average Val Pearson: {avg_pearson:.4f} ± {std_pearson:.4f}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a8265",
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_pathway = pan_cancer_pathway[\n",
    "    pan_cancer_pathway['OncotreeLineage'] == 'Breast'\n",
    "].copy()\n",
    "\n",
    "her2_positive = ['SKBR3', 'HCC1419', 'HCC1954', 'HCC1569', 'AU565', 'JIMT1', 'BT474', \n",
    "                 'MDA-MB-453', 'UACC812', 'ZR7530', 'HCC2218', 'MDA-MB-361', 'EFM19']\n",
    "er_positive = ['MCF7', 'T47D', 'ZR751', 'BT483', 'CAMA1', 'HCC1428', 'MDA-MB-415', \n",
    "               'MDA-MB-175VII', 'MDA-MB-134VI']\n",
    "\n",
    "exclude_cells = set()\n",
    "for cell_name in her2_positive + er_positive:\n",
    "    matching = breast_cancer_pathway[\n",
    "        breast_cancer_pathway['StrippedCellLineName'].str.upper().str.replace('-', '').str.replace('_', '') == \n",
    "        cell_name.upper().replace('-', '').replace('_', '')\n",
    "    ]['ModelID'].unique()\n",
    "    exclude_cells.update(matching)\n",
    "\n",
    "tnbc_pathway = breast_cancer_pathway[~breast_cancer_pathway['ModelID'].isin(exclude_cells)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_TYPE = 'cell_line'\n",
    "\n",
    "def cell_line_split(dataframe, train_ratio=0.8, val_ratio=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split by cell lines (GPDRP method).\n",
    "    No cell line appears in multiple splits.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: DataFrame with ModelID column\n",
    "        train_ratio: Proportion of cell lines for training\n",
    "        val_ratio: Proportion of cell lines for validation\n",
    "        random_seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        train_idx, val_idx, test_idx: Index arrays for each split\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    cell_lines = dataframe['ModelID'].unique()\n",
    "    n_cells = len(cell_lines)\n",
    "    shuffled_cells = np.random.permutation(cell_lines)\n",
    "    \n",
    "    n_train = int(n_cells * train_ratio)\n",
    "    n_val = int(n_cells * val_ratio)\n",
    "    \n",
    "    train_cells = set(shuffled_cells[:n_train])\n",
    "    val_cells = set(shuffled_cells[n_train:n_train + n_val])\n",
    "    test_cells = set(shuffled_cells[n_train + n_val:])\n",
    "    \n",
    "    train_idx = dataframe[dataframe['ModelID'].isin(train_cells)].index.values\n",
    "    val_idx = dataframe[dataframe['ModelID'].isin(val_cells)].index.values\n",
    "    test_idx = dataframe[dataframe['ModelID'].isin(test_cells)].index.values\n",
    "    \n",
    "    train_cell_set = set(dataframe.loc[train_idx, 'ModelID'].unique())\n",
    "    val_cell_set = set(dataframe.loc[val_idx, 'ModelID'].unique())\n",
    "    test_cell_set = set(dataframe.loc[test_idx, 'ModelID'].unique())\n",
    "    \n",
    "    assert len(train_cell_set & val_cell_set) == 0, \"Cell line overlap between train and val!\"\n",
    "    assert len(train_cell_set & test_cell_set) == 0, \"Cell line overlap between train and test!\"\n",
    "    assert len(val_cell_set & test_cell_set) == 0, \"Cell line overlap between val and test!\"\n",
    "    \n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "def random_split(dataframe, train_ratio=0.8, val_ratio=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    Random split of samples (standard train/test split).\n",
    "    Note: This allows data leakage as the same cell line can appear in multiple splits.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: DataFrame with samples\n",
    "        train_ratio: Proportion of samples for training\n",
    "        val_ratio: Proportion of samples for validation\n",
    "        random_seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        train_idx, val_idx, test_idx: Index arrays for each split\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    indices = np.arange(len(dataframe))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    n_train = int(len(indices) * train_ratio)\n",
    "    n_val = int(len(indices) * val_ratio)\n",
    "    \n",
    "    train_idx = dataframe.index[indices[:n_train]].values\n",
    "    val_idx = dataframe.index[indices[n_train:n_train + n_val]].values\n",
    "    test_idx = dataframe.index[indices[n_train + n_val:]].values\n",
    "    \n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "if SPLIT_TYPE == 'cell_line':\n",
    "    pan_train_idx, pan_val_idx, pan_test_idx = cell_line_split(pan_cancer_pathway, random_seed=42)\n",
    "elif SPLIT_TYPE == 'random':\n",
    "    pan_train_idx, pan_val_idx, pan_test_idx = random_split(pan_cancer_pathway, random_seed=42)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown SPLIT_TYPE: {SPLIT_TYPE}. Must be 'cell_line' or 'random'\")\n",
    "\n",
    "if SPLIT_TYPE == 'cell_line':\n",
    "    pan_train_cells = set(pan_cancer_pathway.loc[pan_train_idx, 'ModelID'].unique())\n",
    "    pan_val_cells = set(pan_cancer_pathway.loc[pan_val_idx, 'ModelID'].unique())\n",
    "    pan_test_cells = set(pan_cancer_pathway.loc[pan_test_idx, 'ModelID'].unique())\n",
    "    \n",
    "    breast_train_data = breast_cancer_pathway[breast_cancer_pathway['ModelID'].isin(pan_train_cells)].copy()\n",
    "    breast_val_data = breast_cancer_pathway[breast_cancer_pathway['ModelID'].isin(pan_val_cells)].copy()\n",
    "    breast_test_data = breast_cancer_pathway[breast_cancer_pathway['ModelID'].isin(pan_test_cells)].copy()\n",
    "    \n",
    "    tnbc_train_data = tnbc_pathway[tnbc_pathway['ModelID'].isin(pan_train_cells)].copy()\n",
    "    tnbc_val_data = tnbc_pathway[tnbc_pathway['ModelID'].isin(pan_val_cells)].copy()\n",
    "    tnbc_test_data = tnbc_pathway[tnbc_pathway['ModelID'].isin(pan_test_cells)].copy()\n",
    "    \n",
    "    breast_train_idx = breast_train_data.index.values\n",
    "    breast_val_idx = breast_val_data.index.values\n",
    "    breast_test_idx = breast_test_data.index.values\n",
    "    \n",
    "    tnbc_train_idx = tnbc_train_data.index.values\n",
    "    tnbc_val_idx = tnbc_val_data.index.values\n",
    "    tnbc_test_idx = tnbc_test_data.index.values\n",
    "    \n",
    "    breast_train_cells = set(breast_train_data['ModelID'].unique())\n",
    "    breast_val_cells = set(breast_val_data['ModelID'].unique())\n",
    "    breast_test_cells = set(breast_test_data['ModelID'].unique())\n",
    "    \n",
    "    tnbc_train_cells = set(tnbc_train_data['ModelID'].unique())\n",
    "    tnbc_val_cells = set(tnbc_val_data['ModelID'].unique())\n",
    "    tnbc_test_cells = set(tnbc_test_data['ModelID'].unique())\n",
    "    \n",
    "    assert len(breast_train_cells & pan_val_cells) == 0, \"Data leakage: breast train cells in pan val!\"\n",
    "    assert len(breast_train_cells & pan_test_cells) == 0, \"Data leakage: breast train cells in pan test!\"\n",
    "    assert len(tnbc_train_cells & pan_val_cells) == 0, \"Data leakage: TNBC train cells in pan val!\"\n",
    "    assert len(tnbc_train_cells & pan_test_cells) == 0, \"Data leakage: TNBC train cells in pan test!\"\n",
    "    \n",
    "elif SPLIT_TYPE == 'random':\n",
    "    breast_train_idx, breast_val_idx, breast_test_idx = random_split(breast_cancer_pathway, random_seed=42)\n",
    "    tnbc_train_idx, tnbc_val_idx, tnbc_test_idx = random_split(tnbc_pathway, random_seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugResponsePathwayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for drug response prediction using GSVA pathway scores.\n",
    "    Performs per-sample z-score normalization of pathway scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, drug_graphs_dict, drug_col='DRUG_NAME', pathway_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize pathway-based dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataframe: DataFrame with pathway scores, ModelID, DRUG_NAME, LN_IC50\n",
    "            drug_graphs_dict: Dictionary mapping drug names to graph data\n",
    "            drug_col: Column name for drug names\n",
    "            pathway_cols: List of pathway column names (if None, will infer)\n",
    "        \"\"\"\n",
    "        self.original_data = dataframe.copy()\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.drug_graphs = drug_graphs_dict\n",
    "        self.drug_col = drug_col\n",
    "        \n",
    "        if pathway_cols is not None:\n",
    "            self.pathway_cols = [c for c in pathway_cols if c in dataframe.columns]\n",
    "        else:\n",
    "            exclude_cols = ['ModelID', 'COSMICID', 'StrippedCellLineName', 'OncotreeLineage', \n",
    "                           'OncotreePrimaryDisease', drug_col, 'SMILES', 'LN_IC50', 'CellLineName']\n",
    "            numeric_cols = dataframe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            self.pathway_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
    "        for col in self.pathway_cols:\n",
    "            if not pd.api.types.is_numeric_dtype(dataframe[col]):\n",
    "                raise ValueError(f\"Pathway column '{col}' is not numeric\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        drug_name = row[self.drug_col]\n",
    "        \n",
    "        if drug_name not in self.drug_graphs:\n",
    "            raise ValueError(f\"Drug {drug_name} not found in drug_graphs\")\n",
    "        drug_graph = self.drug_graphs[drug_name]['graph_data']\n",
    "        \n",
    "        pathway_scores = row[self.pathway_cols].values.astype(np.float32)\n",
    "        mean = pathway_scores.mean()\n",
    "        std = pathway_scores.std()\n",
    "        if std > 1e-8:\n",
    "            pathway_scores = (pathway_scores - mean) / std\n",
    "        else:\n",
    "            pathway_scores = np.zeros_like(pathway_scores)\n",
    "        \n",
    "        pathway_tensor = torch.tensor(pathway_scores, dtype=torch.float32)\n",
    "        ic50 = torch.tensor([row['LN_IC50']], dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            'drug_graph': drug_graph,\n",
    "            'cell_expr': pathway_tensor,  # Keep same key name for compatibility\n",
    "            'ic50': ic50,\n",
    "            'drug_name': drug_name,\n",
    "            'cell_id': row['ModelID']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd686297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Neural Network encoder for molecular SMILES structures.\n",
    "    Uses TransformerConv layers to generate 256-dim drug embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, node_feature_dim=9, edge_feature_dim=4, hidden_dim=256, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Initialize DrugEncoder.\n",
    "        \n",
    "        Args:\n",
    "            node_feature_dim: Number of atom features\n",
    "            edge_feature_dim: Number of bond features\n",
    "            hidden_dim: Hidden layer dimension (output is 256)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(DrugEncoder, self).__init__()\n",
    "        \n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device('mps')\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        \n",
    "        self.conv1 = TransformerConv(node_feature_dim, 128, heads=4, dropout=dropout, edge_dim=edge_feature_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(128 * 4)\n",
    "        \n",
    "        self.conv2 = TransformerConv(128 * 4, 256, heads=8, dropout=dropout, edge_dim=edge_feature_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(256 * 8)\n",
    "        \n",
    "        self.conv3 = TransformerConv(256 * 8, 256, heads=8, dropout=dropout, edge_dim=edge_feature_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(256 * 8)\n",
    "        \n",
    "        self.conv4 = TransformerConv(256 * 8, 256, heads=4, concat=False, dropout=dropout, edge_dim=edge_feature_dim)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.attention_weights = nn.Linear(256, 1)\n",
    "        \n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass through drug encoder.\n",
    "        \n",
    "        Args:\n",
    "            data: PyTorch Geometric Data/Batch object\n",
    "            \n",
    "        Returns:\n",
    "            Drug embeddings of shape (batch_size, 256)\n",
    "        \"\"\"\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        x = x.to(self.device, non_blocking=True)\n",
    "        edge_index = edge_index.to(self.device, non_blocking=True)\n",
    "        edge_attr = edge_attr.to(self.device, non_blocking=True)\n",
    "        batch = batch.to(self.device, non_blocking=True)\n",
    "        \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        attention_scores = self.attention_weights(x)\n",
    "        attention_scores = torch.softmax(attention_scores, dim=0)\n",
    "        \n",
    "        x_weighted = x * attention_scores\n",
    "        x_mean = global_mean_pool(x_weighted, batch)\n",
    "        x_max = global_max_pool(x, batch)\n",
    "        \n",
    "        embedding = (x_mean + x_max) / 2\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "class CellEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Feedforward neural network to encode cell line pathway scores.\n",
    "    Uses skip connection to preserve direct pathway signal.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=1329, hidden_dim=512, output_dim=256, dropout1=0.4, dropout2=0.3):\n",
    "        \"\"\"\n",
    "        Initialize CellEncoder.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Number of pathway features\n",
    "            hidden_dim: First hidden layer dimension\n",
    "            output_dim: Output embedding dimension\n",
    "            dropout1: Dropout rate for first layer\n",
    "            dropout2: Dropout rate for second layer\n",
    "        \"\"\"\n",
    "        super(CellEncoder, self).__init__()\n",
    "        \n",
    "        # Device selection\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device('mps')\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(output_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.skip = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through cell encoder.\n",
    "        \n",
    "        Args:\n",
    "            x: Pathway scores tensor of shape (batch_size, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Cell embeddings of shape (batch_size, 256)\n",
    "        \"\"\"\n",
    "        x = x.to(self.device, non_blocking=True)\n",
    "        \n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        # Skip connection\n",
    "        skip_out = self.skip(x)\n",
    "        \n",
    "        # Residual connection\n",
    "        embedding = out + skip_out\n",
    "        \n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6699b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugResponsePathwayGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    GNN model for drug response prediction using pathway activity scores.\n",
    "    Uses 1,329 pathway features instead of raw gene expression.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, drug_node_dim=9, drug_edge_dim=4, cell_input_dim=1329, \n",
    "                 hidden_dim=256, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Initialize DrugResponsePathwayGNN.\n",
    "        \n",
    "        Args:\n",
    "            drug_node_dim: Number of atom features (9)\n",
    "            drug_edge_dim: Number of bond features (4)\n",
    "            cell_input_dim: Number of pathway features (1329)\n",
    "            hidden_dim: Embedding dimension (256)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(DrugResponsePathwayGNN, self).__init__()\n",
    "        \n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device('mps')\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        \n",
    "        self.drug_encoder = DrugEncoder(\n",
    "            node_feature_dim=drug_node_dim,\n",
    "            edge_feature_dim=drug_edge_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.cell_encoder = CellEncoder(\n",
    "            input_dim=cell_input_dim,\n",
    "            hidden_dim=512,\n",
    "            output_dim=hidden_dim,\n",
    "            dropout1=0.4,\n",
    "            dropout2=0.3\n",
    "        )\n",
    "        \n",
    "        self.attention_query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.attention_key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.attention_value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Combined embedding dimension\n",
    "        combined_dim = hidden_dim * 2  # 512\n",
    "        \n",
    "        # Head A: IC50 Regression (primary task)\n",
    "        self.ic50_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        self.reconstruction_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, combined_dim)\n",
    "        )\n",
    "        \n",
    "        self.to(self.device)\n",
    "    \n",
    "    def integrate_with_attention(self, drug_emb, cell_emb):\n",
    "        \"\"\"\n",
    "        Integrate drug and cell embeddings using attention mechanism.\n",
    "        \n",
    "        Args:\n",
    "            drug_emb: Drug embeddings (batch_size, 256)\n",
    "            cell_emb: Cell embeddings (batch_size, 256)\n",
    "            \n",
    "        Returns:\n",
    "            Combined embedding (batch_size, 512)\n",
    "        \"\"\"\n",
    "        query = self.attention_query(drug_emb)\n",
    "        key = self.attention_key(cell_emb)\n",
    "        value = self.attention_value(cell_emb)\n",
    "        \n",
    "        attention_scores = torch.matmul(query.unsqueeze(1), key.unsqueeze(2))\n",
    "        attention_scores = attention_scores / (drug_emb.size(-1) ** 0.5)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        attended_cell = attention_weights.squeeze(-1) * value\n",
    "        \n",
    "        combined = torch.cat([drug_emb, attended_cell], dim=1)\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def forward(self, drug_batch, cell_batch, return_embeddings=False):\n",
    "        \"\"\"\n",
    "        Forward pass through complete model.\n",
    "        \n",
    "        Args:\n",
    "            drug_batch: PyTorch Geometric batch of molecular graphs\n",
    "            cell_batch: Pathway scores tensor (batch_size, 1329)\n",
    "            return_embeddings: Whether to return intermediate embeddings\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with predictions and optionally embeddings\n",
    "        \"\"\"\n",
    "        # Encode drug and cell\n",
    "        drug_emb = self.drug_encoder(drug_batch)\n",
    "        cell_emb = self.cell_encoder(cell_batch)\n",
    "        \n",
    "        # Integrate with attention\n",
    "        combined = self.integrate_with_attention(drug_emb, cell_emb)  # (batch_size, 512)\n",
    "        \n",
    "        ic50_pred = self.ic50_head(combined)\n",
    "        class_pred = self.classification_head(combined)\n",
    "        recon_pred = self.reconstruction_head(combined)\n",
    "        \n",
    "        results = {\n",
    "            'ic50': ic50_pred,\n",
    "            'classification': class_pred,\n",
    "            'reconstruction': recon_pred\n",
    "        }\n",
    "        \n",
    "        if return_embeddings:\n",
    "            results['embeddings'] = {\n",
    "                'drug': drug_emb,\n",
    "                'cell': cell_emb,\n",
    "                'combined': combined\n",
    "            }\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d8ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for drug graphs.\"\"\"\n",
    "    drug_graphs = [item['drug_graph'] for item in batch]\n",
    "    cell_exprs = torch.stack([item['cell_expr'] for item in batch])\n",
    "    ic50s = torch.stack([item['ic50'] for item in batch])\n",
    "    drug_batch = Batch.from_data_list(drug_graphs)\n",
    "    return {\n",
    "        'drug_batch': drug_batch,\n",
    "        'cell_batch': cell_exprs,\n",
    "        'ic50': ic50s\n",
    "    }\n",
    "\n",
    "def prebatched_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for pre-batched data.\n",
    "    Since batch_size=1, each item is already a complete batch, just return it.\n",
    "    \"\"\"\n",
    "    return batch[0]  # batch_size=1 means batch is a list with one item\n",
    "\n",
    "class PrebatchedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for pre-batched data with epoch-level batch shuffling.\n",
    "    \"\"\"\n",
    "    def __init__(self, prebatched_batches):\n",
    "        \"\"\"\n",
    "        Initialize pre-batched dataset.\n",
    "        \n",
    "        Args:\n",
    "            prebatched_batches: List of pre-batched dictionaries\n",
    "        \"\"\"\n",
    "        self.batches = prebatched_batches\n",
    "        self.current_order = list(range(len(self.batches)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.batches[self.current_order[idx]]\n",
    "    \n",
    "    def shuffle_batches(self, random_seed=None):\n",
    "        \"\"\"\n",
    "        Shuffle batch order for epoch-level randomization.\n",
    "        \n",
    "        Args:\n",
    "            random_seed: Optional random seed for reproducibility\n",
    "        \"\"\"\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "        np.random.shuffle(self.current_order)\n",
    "\n",
    "def create_prebatched_data(dataset, batch_size, split_name, phase_name, output_dir, shuffle_samples=True):\n",
    "    \"\"\"\n",
    "    Pre-compute and save batched data to disk.\n",
    "    \n",
    "    Args:\n",
    "        dataset: DrugResponsePathwayDataset\n",
    "        batch_size: Batch size\n",
    "        split_name: Name of split (train/val/test)\n",
    "        phase_name: Name of training phase (phase1/phase2/phase3)\n",
    "        output_dir: Directory to save pre-batched data\n",
    "        shuffle_samples: Whether to shuffle samples before batching\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved batch file\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    # Include split type in path to avoid overwriting when switching between random and cell_line splits\n",
    "    split_type = globals().get('SPLIT_TYPE', 'unknown')\n",
    "    prebatched_dir = output_dir / \"prebatched_data\" / f\"{phase_name}_{split_type}\"\n",
    "    prebatched_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    batch_file = prebatched_dir / f\"{split_name}_batches.pkl\"\n",
    "    \n",
    "    if batch_file.exists():\n",
    "        print(f\"Pre-batched data already exists: {batch_file}\")\n",
    "        return batch_file\n",
    "    \n",
    "    print(f\"Pre-batching {split_name} data for {phase_name}...\")\n",
    "    \n",
    "    indices = np.arange(len(dataset))\n",
    "    if shuffle_samples:\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    batches = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(indices), batch_size), desc=f\"Pre-batching {split_name}\"):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "        \n",
    "        batch_items = [dataset[idx] for idx in batch_indices]\n",
    "        \n",
    "        drug_graphs = [item['drug_graph'] for item in batch_items]\n",
    "        cell_exprs = torch.stack([item['cell_expr'] for item in batch_items])\n",
    "        ic50s = torch.stack([item['ic50'] for item in batch_items])\n",
    "        \n",
    "        # Batch graphs once\n",
    "        drug_batch = Batch.from_data_list(drug_graphs)\n",
    "        \n",
    "        # Store pre-batched data\n",
    "        batched_data = {\n",
    "            'drug_batch': drug_batch,\n",
    "            'cell_batch': cell_exprs,\n",
    "            'ic50': ic50s\n",
    "        }\n",
    "        batches.append(batched_data)\n",
    "    \n",
    "    # Save pre-batched data\n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batches, f)\n",
    "    \n",
    "    print(f\"Pre-batching complete! Saved {len(batches)} batches to {batch_file}\")\n",
    "    return batch_file\n",
    "\n",
    "def compute_loss(predictions, targets, median_ic50, loss_weights=(1.0, 0.3, 0.3)):\n",
    "    \"\"\"\n",
    "    Compute multi-task loss.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Dictionary with 'ic50', 'classification', 'reconstruction'\n",
    "        targets: Dictionary with 'ic50', 'embeddings'\n",
    "        median_ic50: Threshold for classification\n",
    "        loss_weights: Weights for (IC50, classification, reconstruction)\n",
    "        \n",
    "    Returns:\n",
    "        Total loss and individual losses\n",
    "    \"\"\"\n",
    "    ic50_pred = predictions['ic50'].squeeze()\n",
    "    ic50_target = targets['ic50'].squeeze()\n",
    "    \n",
    "    mse_loss = nn.MSELoss()(ic50_pred, ic50_target)\n",
    "    \n",
    "    class_pred = predictions['classification'].squeeze()\n",
    "    class_target = (ic50_target > median_ic50).float()\n",
    "    bce_loss = nn.BCEWithLogitsLoss()(class_pred, class_target)\n",
    "    \n",
    "    recon_pred = predictions['reconstruction']\n",
    "    recon_target = targets['embeddings']\n",
    "    recon_loss = nn.MSELoss()(recon_pred, recon_target)\n",
    "    \n",
    "    total_loss = loss_weights[0] * mse_loss + loss_weights[1] * bce_loss + loss_weights[2] * recon_loss\n",
    "    \n",
    "    return total_loss, {\n",
    "        'mse': mse_loss.item(),\n",
    "        'bce': bce_loss.item(),\n",
    "        'recon': recon_loss.item()\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, dataloader, device, median_ic50=None, is_prebatched=False):\n",
    "    \"\"\"\n",
    "    Evaluate model on a dataloader.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        dataloader: DataLoader to evaluate on\n",
    "        device: Device to run on\n",
    "        median_ic50: Classification threshold (if None, computed from dataloader)\n",
    "        is_prebatched: Whether using pre-batched data (batch_size=1)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metrics, predictions, and targets\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    if median_ic50 is None:\n",
    "        all_ic50s = []\n",
    "        for batch in dataloader:\n",
    "            # Batch is already unwrapped by prebatched_collate_fn if using pre-batched data\n",
    "            all_ic50s.append(batch['ic50'].cpu().numpy())\n",
    "        if len(all_ic50s) == 0:\n",
    "            raise ValueError(\"Cannot compute median_ic50: dataloader is empty\")\n",
    "        median_ic50 = np.median(np.concatenate(all_ic50s))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Batch is already unwrapped by prebatched_collate_fn if using pre-batched data\n",
    "            drug_batch = batch['drug_batch'].to(device)\n",
    "            cell_batch = batch['cell_batch'].to(device)\n",
    "            ic50_target = batch['ic50'].to(device)\n",
    "            \n",
    "            outputs = model(drug_batch, cell_batch, return_embeddings=True)\n",
    "            combined_emb = outputs['embeddings']['combined']\n",
    "            \n",
    "            targets = {\n",
    "                'ic50': ic50_target,\n",
    "                'embeddings': combined_emb\n",
    "            }\n",
    "            \n",
    "            loss, _ = compute_loss(outputs, targets, median_ic50, loss_weights=(1.0, 0.3, 0.3))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_preds.append(outputs['ic50'].cpu().numpy())\n",
    "            all_targets.append(ic50_target.cpu().numpy())\n",
    "    \n",
    "    # Handle empty dataloader case\n",
    "    if len(all_preds) == 0:\n",
    "        return {\n",
    "            'metrics': {\n",
    "                'loss': 0.0,\n",
    "                'pearson': np.nan,\n",
    "                'spearman': np.nan,\n",
    "                'r2': np.nan,\n",
    "                'rmse': np.nan,\n",
    "                'mae': np.nan\n",
    "            },\n",
    "            'predictions': np.array([]),\n",
    "            'targets': np.array([])\n",
    "        }\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds).flatten()\n",
    "    all_targets = np.concatenate(all_targets).flatten()\n",
    "    \n",
    "    pearson_r, _ = pearsonr(all_targets, all_preds)\n",
    "    spearman_r, _ = spearmanr(all_targets, all_preds)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    \n",
    "    return {\n",
    "        'metrics': {\n",
    "            'loss': total_loss / len(dataloader) if len(dataloader) > 0 else 0.0,\n",
    "            'pearson': pearson_r,\n",
    "            'spearman': spearman_r,\n",
    "            'r2': r2,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae\n",
    "        },\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets\n",
    "    }\n",
    "\n",
    "def create_pathway_dataloaders(dataset, train_idx, val_idx, test_idx, batch_size=128, \n",
    "                                use_prebatched=False, phase_name=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Create dataloaders using pre-defined cell-line based splits.\n",
    "    \n",
    "    Args:\n",
    "        dataset: DrugResponsePathwayDataset\n",
    "        train_idx: Training indices (from original dataframe)\n",
    "        val_idx: Validation indices (from original dataframe)\n",
    "        test_idx: Test indices (from original dataframe)\n",
    "        batch_size: Batch size\n",
    "        use_prebatched: Whether to use pre-batched data\n",
    "        phase_name: Name of training phase (required if use_prebatched=True)\n",
    "        output_dir: Directory for pre-batched data (required if use_prebatched=True)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with train/val/test loaders and datasets (if pre-batched)\n",
    "    \"\"\"\n",
    "    train_data = dataset.original_data.loc[train_idx].reset_index(drop=True)\n",
    "    val_data = dataset.original_data.loc[val_idx].reset_index(drop=True)\n",
    "    test_data = dataset.original_data.loc[test_idx].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = DrugResponsePathwayDataset(train_data, dataset.drug_graphs, dataset.drug_col, pathway_cols=dataset.pathway_cols)\n",
    "    val_dataset = DrugResponsePathwayDataset(val_data, dataset.drug_graphs, dataset.drug_col, pathway_cols=dataset.pathway_cols)\n",
    "    test_dataset = DrugResponsePathwayDataset(test_data, dataset.drug_graphs, dataset.drug_col, pathway_cols=dataset.pathway_cols)\n",
    "    \n",
    "    if use_prebatched:\n",
    "        if phase_name is None or output_dir is None:\n",
    "            raise ValueError(\"phase_name and output_dir required when use_prebatched=True\")\n",
    "        \n",
    "        train_batch_file = create_prebatched_data(train_dataset, batch_size, 'train', phase_name, output_dir, shuffle_samples=True)\n",
    "        val_batch_file = create_prebatched_data(val_dataset, batch_size, 'val', phase_name, output_dir, shuffle_samples=False)\n",
    "        test_batch_file = create_prebatched_data(test_dataset, batch_size, 'test', phase_name, output_dir, shuffle_samples=False)\n",
    "        \n",
    "        with open(train_batch_file, 'rb') as f:\n",
    "            train_batches = pickle.load(f)\n",
    "        with open(val_batch_file, 'rb') as f:\n",
    "            val_batches = pickle.load(f)\n",
    "        with open(test_batch_file, 'rb') as f:\n",
    "            test_batches = pickle.load(f)\n",
    "        \n",
    "        train_prebatched = PrebatchedDataset(train_batches)\n",
    "        val_prebatched = PrebatchedDataset(val_batches)\n",
    "        test_prebatched = PrebatchedDataset(test_batches)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_prebatched,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            collate_fn=prebatched_collate_fn,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_prebatched,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            collate_fn=prebatched_collate_fn,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_prebatched,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            collate_fn=prebatched_collate_fn,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'train': train_loader,\n",
    "            'val': val_loader,\n",
    "            'test': test_loader,\n",
    "            'datasets': {\n",
    "                'train': train_prebatched,\n",
    "                'val': val_prebatched,\n",
    "                'test': test_prebatched\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        # Standard dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'train': train_loader,\n",
    "            'val': val_loader,\n",
    "            'test': test_loader\n",
    "        }\n",
    "\n",
    "def train_phase_pathway(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    phase_name,\n",
    "    num_epochs=50,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    patience=20,\n",
    "    scheduler_patience=5,\n",
    "    checkpoint_path=None,\n",
    "    prebatched_datasets=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a phase of the pathway-based model.\n",
    "    \n",
    "    Args:\n",
    "        model: DrugResponsePathwayGNN model\n",
    "        train_loader: Training DataLoader\n",
    "        val_loader: Validation DataLoader\n",
    "        device: Device to train on\n",
    "        phase_name: Name of the phase\n",
    "        num_epochs: Maximum epochs\n",
    "        lr: Learning rate\n",
    "        weight_decay: Weight decay\n",
    "        patience: Early stopping patience\n",
    "        scheduler_patience: LR scheduler patience\n",
    "        checkpoint_path: Path to save best model\n",
    "        prebatched_datasets: Dictionary with 'train' PrebatchedDataset (for epoch-level shuffling)\n",
    "        \n",
    "    Returns:\n",
    "        Trained model and training history\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=scheduler_patience)  # Less aggressive: 0.7 instead of 0.5\n",
    "    \n",
    "    # Check if using pre-batched data\n",
    "    is_prebatched = prebatched_datasets is not None and 'train' in prebatched_datasets\n",
    "    \n",
    "    all_ic50s = []\n",
    "    for batch in train_loader:\n",
    "        # Batch is already unwrapped by prebatched_collate_fn if using pre-batched data\n",
    "        all_ic50s.append(batch['ic50'].cpu().numpy())\n",
    "    median_ic50 = np.median(np.concatenate(all_ic50s))\n",
    "    \n",
    "    best_val_r2 = float('-inf')\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_r2': [], 'val_pearson': []}\n",
    "    \n",
    "    print(f\"\\n{phase_name} - Starting training...\")\n",
    "    print(f\"Training samples: {len(train_loader.dataset)}, Validation samples: {len(val_loader.dataset)}\")\n",
    "    print(f\"Learning rate: {lr}, Weight decay: {weight_decay}, Patience: {patience}\")\n",
    "    if is_prebatched:\n",
    "        print(\"Using pre-batched data with epoch-level batch shuffling\\n\")\n",
    "    else:\n",
    "        print()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle batches at start of each epoch (for pre-batched data)\n",
    "        if is_prebatched:\n",
    "            prebatched_datasets['train'].shuffle_batches(random_seed=epoch)\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "            # Batch is already unwrapped by prebatched_collate_fn if using pre-batched data\n",
    "            drug_batch = batch['drug_batch'].to(device)\n",
    "            cell_batch = batch['cell_batch'].to(device)\n",
    "            ic50_target = batch['ic50'].to(device)\n",
    "            \n",
    "            outputs = model(drug_batch, cell_batch, return_embeddings=True)\n",
    "            combined_emb = outputs['embeddings']['combined']\n",
    "            \n",
    "            targets = {\n",
    "                'ic50': ic50_target,\n",
    "                'embeddings': combined_emb.detach()\n",
    "            }\n",
    "            \n",
    "            loss, _ = compute_loss(outputs, targets, median_ic50, loss_weights=(1.0, 0.3, 0.3))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        eval_result = evaluate_model(model, val_loader, device, median_ic50, is_prebatched=is_prebatched)\n",
    "        val_metrics = eval_result['metrics']\n",
    "        val_r2 = val_metrics['r2']\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_r2'].append(val_r2)\n",
    "        history['val_pearson'].append(val_metrics['pearson'])\n",
    "        \n",
    "        scheduler.step(val_r2)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_metrics['loss']:.4f} | \"\n",
    "              f\"Val R²: {val_r2:.4f} | Pearson: {val_metrics['pearson']:.4f} | RMSE: {val_metrics['rmse']:.4f}\")\n",
    "        \n",
    "        if val_r2 > best_val_r2:\n",
    "            best_val_r2 = val_r2\n",
    "            patience_counter = 0\n",
    "            if checkpoint_path:\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'val_metrics': val_metrics,\n",
    "                    'classification_threshold': median_ic50,\n",
    "                    'epoch': epoch + 1\n",
    "                }, checkpoint_path)\n",
    "                print(f\"  ✓ Best model saved (R² = {val_r2:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs (no improvement for {patience} epochs)\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    if checkpoint_path and Path(checkpoint_path).exists():\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"\\n{phase_name} - Training completed!\")\n",
    "        print(f\"Best validation R²: {best_val_r2:.4f} (epoch {checkpoint['epoch']})\")\n",
    "        print(f\"Total epochs trained: {len(history['train_loss'])}\\n\")\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c06dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(\n",
    "    model_class,\n",
    "    model_init_kwargs,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    phase_name,\n",
    "    n_models=5,\n",
    "    seeds=None,\n",
    "    num_epochs=50,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    patience=20,\n",
    "    scheduler_patience=5,\n",
    "    checkpoint_dir=None,\n",
    "    prebatched_datasets=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train an ensemble of models with different random seeds.\n",
    "    \n",
    "    Args:\n",
    "        model_class: Model class to instantiate\n",
    "        model_init_kwargs: Dictionary of kwargs for model initialization\n",
    "        train_loader: Training DataLoader\n",
    "        val_loader: Validation DataLoader\n",
    "        device: Device to train on\n",
    "        phase_name: Name of the phase\n",
    "        n_models: Number of models in ensemble\n",
    "        seeds: List of random seeds (if None, generates n_models seeds)\n",
    "        num_epochs: Maximum epochs per model\n",
    "        lr: Learning rate\n",
    "        weight_decay: Weight decay\n",
    "        patience: Early stopping patience\n",
    "        scheduler_patience: LR scheduler patience\n",
    "        checkpoint_dir: Directory to save model checkpoints\n",
    "        prebatched_datasets: Dictionary with 'train' PrebatchedDataset\n",
    "        \n",
    "    Returns:\n",
    "        List of checkpoint paths and list of training histories\n",
    "    \"\"\"\n",
    "    if seeds is None:\n",
    "        seeds = [42 + i * 100 for i in range(n_models)]\n",
    "    \n",
    "    if checkpoint_dir is None:\n",
    "        checkpoint_dir = output_dir / \"models\" / \"ensemble\" / phase_name.replace(\" \", \"_\").replace(\":\", \"\")\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    checkpoint_paths = []\n",
    "    histories = []\n",
    "    \n",
    "    for i, seed in enumerate(seeds):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Training Ensemble Model {i+1}/{n_models} (seed={seed})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "        model = model_class(**model_init_kwargs).to(device)\n",
    "        checkpoint_path = checkpoint_dir / f\"model_{i+1}_seed_{seed}.pt\"\n",
    "        \n",
    "        model, history = train_phase_pathway(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "            phase_name=f\"{phase_name} - Ensemble {i+1}/{n_models}\",\n",
    "            num_epochs=num_epochs,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            patience=patience,\n",
    "            scheduler_patience=scheduler_patience,\n",
    "            checkpoint_path=str(checkpoint_path),\n",
    "            prebatched_datasets=prebatched_datasets\n",
    "        )\n",
    "        \n",
    "        checkpoint_paths.append(checkpoint_path)\n",
    "        histories.append(history)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Ensemble Training Complete!\")\n",
    "    print(f\"Trained {n_models} models with seeds: {seeds}\")\n",
    "    print(f\"Checkpoints saved to: {checkpoint_dir}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return checkpoint_paths, histories\n",
    "\n",
    "def ensemble_predict(checkpoint_paths, dataloader, device, model_class, model_init_kwargs, is_prebatched=False):\n",
    "    \"\"\"\n",
    "    Make ensemble predictions by averaging predictions from multiple models.\n",
    "    \n",
    "    Args:\n",
    "        checkpoint_paths: List of paths to model checkpoints\n",
    "        dataloader: DataLoader to predict on\n",
    "        device: Device to run on\n",
    "        model_class: Model class to instantiate\n",
    "        model_init_kwargs: Dictionary of kwargs for model initialization\n",
    "        is_prebatched: Whether using pre-batched data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with ensemble predictions, individual predictions, and targets\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    for checkpoint_path in checkpoint_paths:\n",
    "        model = model_class(**model_init_kwargs).to(device)\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    \n",
    "    all_preds_individual = [[] for _ in range(len(models))]\n",
    "    all_targets = []\n",
    "    \n",
    "    median_ic50 = None\n",
    "    all_ic50s = []\n",
    "    for batch in dataloader:\n",
    "        all_ic50s.append(batch['ic50'].cpu().numpy())\n",
    "    if len(all_ic50s) > 0:\n",
    "        median_ic50 = np.median(np.concatenate(all_ic50s))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            drug_batch = batch['drug_batch'].to(device)\n",
    "            cell_batch = batch['cell_batch'].to(device)\n",
    "            ic50_target = batch['ic50'].to(device)\n",
    "            \n",
    "            batch_preds = []\n",
    "            for model in models:\n",
    "                outputs = model(drug_batch, cell_batch, return_embeddings=False)\n",
    "                pred = outputs['ic50'].cpu().numpy()\n",
    "                batch_preds.append(pred)\n",
    "            \n",
    "            ensemble_pred = np.mean(batch_preds, axis=0)\n",
    "            \n",
    "            for i, pred in enumerate(batch_preds):\n",
    "                all_preds_individual[i].append(pred)\n",
    "            all_targets.append(ic50_target.cpu().numpy())\n",
    "    \n",
    "    all_preds_individual = [np.concatenate(preds) for preds in all_preds_individual]\n",
    "    all_targets = np.concatenate(all_targets)\n",
    "    ensemble_preds = np.mean(all_preds_individual, axis=0)\n",
    "    \n",
    "    ensemble_metrics = {\n",
    "        'pearson': pearsonr(ensemble_preds.flatten(), all_targets.flatten())[0],\n",
    "        'spearman': spearmanr(ensemble_preds.flatten(), all_targets.flatten())[0],\n",
    "        'r2': r2_score(all_targets.flatten(), ensemble_preds.flatten()),\n",
    "        'rmse': np.sqrt(mean_squared_error(all_targets.flatten(), ensemble_preds.flatten())),\n",
    "        'mae': mean_absolute_error(all_targets.flatten(), ensemble_preds.flatten())\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'ensemble_predictions': ensemble_preds,\n",
    "        'individual_predictions': all_preds_individual,\n",
    "        'targets': all_targets,\n",
    "        'metrics': ensemble_metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81156e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(\n",
    "    model_class,\n",
    "    model_init_kwargs,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    phase_name,\n",
    "    loaders_dict=None,\n",
    "    n_trials=200,\n",
    "    timeout=None,\n",
    "    study_name=None,\n",
    "    storage=None,\n",
    "    direction='maximize'\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters using Optuna.\n",
    "    \n",
    "    Args:\n",
    "        model_class: Model class to instantiate\n",
    "        model_init_kwargs: Dictionary of kwargs for model initialization\n",
    "        train_loader: Training DataLoader\n",
    "        val_loader: Validation DataLoader\n",
    "        device: Device to train on\n",
    "        phase_name: Name of the phase\n",
    "        loaders_dict: Dictionary containing loaders and datasets (for prebatched_datasets)\n",
    "        n_trials: Number of Optuna trials\n",
    "        timeout: Maximum time in seconds for optimization (None for no limit)\n",
    "        study_name: Name for the Optuna study\n",
    "        storage: Storage URL for Optuna study (None for in-memory)\n",
    "        direction: Optimization direction ('maximize' for R², 'minimize' for loss)\n",
    "        \n",
    "    Returns:\n",
    "        Best hyperparameters dictionary and Optuna study object\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "        patience = trial.suggest_int('patience', 10, 30)\n",
    "        scheduler_patience = trial.suggest_int('scheduler_patience', 5, 15)\n",
    "        num_epochs = trial.suggest_int('num_epochs', 30, 100)\n",
    "        \n",
    "        model = model_class(**model_init_kwargs).to(device)\n",
    "        temp_checkpoint = Path(f\"/tmp/optuna_trial_{trial.number}.pt\")\n",
    "        prebatched_datasets = loaders_dict.get('datasets', None) if loaders_dict else None\n",
    "        \n",
    "        try:\n",
    "            model, history = train_phase_pathway(\n",
    "                model=model,\n",
    "                train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                device=device,\n",
    "                phase_name=f\"{phase_name} (Trial {trial.number})\",\n",
    "                num_epochs=num_epochs,\n",
    "                lr=lr,\n",
    "                weight_decay=weight_decay,\n",
    "                patience=patience,\n",
    "                scheduler_patience=scheduler_patience,\n",
    "                checkpoint_path=str(temp_checkpoint),\n",
    "                prebatched_datasets=prebatched_datasets\n",
    "            )\n",
    "            \n",
    "            best_val_r2 = max(history['val_r2']) if history['val_r2'] else float('-inf')\n",
    "            trial.report(best_val_r2, step=len(history['val_r2']))\n",
    "            \n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "            \n",
    "            return best_val_r2\n",
    "            \n",
    "        except optuna.TrialPruned:\n",
    "            raise\n",
    "        except Exception:\n",
    "            return float('-inf')\n",
    "        finally:\n",
    "            if temp_checkpoint.exists():\n",
    "                temp_checkpoint.unlink()\n",
    "    \n",
    "    study_name = study_name or f\"{phase_name}_optuna_study\"\n",
    "    pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction=direction,\n",
    "        study_name=study_name,\n",
    "        storage=storage,\n",
    "        load_if_exists=True,\n",
    "        pruner=pruner\n",
    "    )\n",
    "    \n",
    "    study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Optuna Optimization Complete for {phase_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Best validation R²: {study.best_value:.4f}\")\n",
    "    print(f\"Best hyperparameters:\")\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return study.best_params, study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d17ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "caffeinate_process = subprocess.Popen(\n",
    "    ['caffeinate', '-dims'],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce64b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "models_dir = output_dir / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "split_type = globals().get('SPLIT_TYPE', 'unknown')\n",
    "phase1_checkpoint = models_dir / f\"trial2_phase1_pathway_{split_type}.pt\"\n",
    "phase2_checkpoint = models_dir / f\"trial2_phase2_breast_pathway_{split_type}.pt\"\n",
    "phase3_checkpoint = models_dir / f\"trial2_phase3_tnbc_pathway_{split_type}.pt\"\n",
    "\n",
    "actual_pathway_count = len(pathway_cols)\n",
    "pan_dataset_pathway = DrugResponsePathwayDataset(pan_cancer_pathway, drug_graphs, drug_col=drug_name_col, pathway_cols=pathway_cols)\n",
    "breast_dataset_pathway = DrugResponsePathwayDataset(breast_cancer_pathway, drug_graphs, drug_col=drug_name_col, pathway_cols=pathway_cols)\n",
    "tnbc_dataset_pathway = DrugResponsePathwayDataset(tnbc_pathway, drug_graphs, drug_col=drug_name_col, pathway_cols=pathway_cols)\n",
    "\n",
    "pan_loaders = create_pathway_dataloaders(\n",
    "    pan_dataset_pathway, pan_train_idx, pan_val_idx, pan_test_idx, \n",
    "    batch_size=256, use_prebatched=True, phase_name=\"phase1\", output_dir=output_dir\n",
    ")\n",
    "breast_loaders = create_pathway_dataloaders(\n",
    "    breast_dataset_pathway, breast_train_idx, breast_val_idx, breast_test_idx, \n",
    "    batch_size=256, use_prebatched=True, phase_name=\"phase2\", output_dir=output_dir\n",
    ")\n",
    "tnbc_loaders = create_pathway_dataloaders(\n",
    "    tnbc_dataset_pathway, tnbc_train_idx, tnbc_val_idx, tnbc_test_idx, \n",
    "    batch_size=256, use_prebatched=True, phase_name=\"phase3\", output_dir=output_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_phase1 = DrugResponsePathwayGNN(cell_input_dim=actual_pathway_count).to(device)\n",
    "\n",
    "if phase1_checkpoint.exists():\n",
    "    checkpoint = torch.load(phase1_checkpoint, map_location=device, weights_only=False)\n",
    "    model_phase1.load_state_dict(checkpoint['model_state_dict'])\n",
    "else:\n",
    "    model_phase1, phase1_history = train_phase_pathway(\n",
    "        model=model_phase1,\n",
    "        train_loader=pan_loaders['train'],\n",
    "        val_loader=pan_loaders['val'],\n",
    "        device=device,\n",
    "        phase_name=\"Phase 1: Pan-Cancer\",\n",
    "        num_epochs=50,\n",
    "        lr=1.5e-3,\n",
    "        weight_decay=1e-4,\n",
    "        patience=20,\n",
    "        scheduler_patience=5,\n",
    "        checkpoint_path=phase1_checkpoint,\n",
    "        prebatched_datasets=pan_loaders.get('datasets', None)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a49879",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_phase2 = DrugResponsePathwayGNN(cell_input_dim=actual_pathway_count).to(device)\n",
    "\n",
    "if phase1_checkpoint.exists():\n",
    "    checkpoint = torch.load(phase1_checkpoint, map_location=device, weights_only=False)\n",
    "    model_phase2.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "if phase2_checkpoint.exists():\n",
    "    checkpoint = torch.load(phase2_checkpoint, map_location=device, weights_only=False)\n",
    "    model_phase2.load_state_dict(checkpoint['model_state_dict'])\n",
    "else:\n",
    "    model_phase2, phase2_history = train_phase_pathway(\n",
    "        model=model_phase2,\n",
    "        train_loader=breast_loaders['train'],\n",
    "        val_loader=breast_loaders['val'],\n",
    "        device=device,\n",
    "        phase_name=\"Phase 2: Breast Cancer\",\n",
    "        num_epochs=50,\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-5,\n",
    "        patience=25,\n",
    "        scheduler_patience=5,\n",
    "        checkpoint_path=phase2_checkpoint,\n",
    "        prebatched_datasets=breast_loaders.get('datasets', None)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_phase3 = DrugResponsePathwayGNN(cell_input_dim=actual_pathway_count).to(device)\n",
    "\n",
    "if phase2_checkpoint.exists():\n",
    "    checkpoint = torch.load(phase2_checkpoint, map_location=device, weights_only=False)\n",
    "    model_phase3.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "if phase3_checkpoint.exists():\n",
    "    checkpoint = torch.load(phase3_checkpoint, map_location=device, weights_only=False)\n",
    "    model_phase3.load_state_dict(checkpoint['model_state_dict'])\n",
    "else:\n",
    "    model_phase3, phase3_history = train_phase_pathway(\n",
    "        model=model_phase3,\n",
    "        train_loader=tnbc_loaders['train'],\n",
    "        val_loader=tnbc_loaders['val'],\n",
    "        device=device,\n",
    "        phase_name=\"Phase 3: TNBC\",\n",
    "        num_epochs=50,\n",
    "        lr=5e-5,\n",
    "        weight_decay=1e-5,\n",
    "        patience=25,\n",
    "        scheduler_patience=5,\n",
    "        checkpoint_path=phase3_checkpoint,\n",
    "        prebatched_datasets=tnbc_loaders.get('datasets', None)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50207cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_phase_pathway(model_path, test_loader, phase_name, device):\n",
    "    \"\"\"\n",
    "    Evaluate a phase on its test set.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to model checkpoint\n",
    "        test_loader: Test DataLoader\n",
    "        phase_name: Name of the phase\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metrics and predictions\n",
    "    \"\"\"\n",
    "    n_samples = len(test_loader.dataset)\n",
    "    \n",
    "    # Check if test set is empty\n",
    "    if n_samples == 0:\n",
    "        print(f\"Warning: {phase_name} test set is empty - returning NaN metrics\")\n",
    "        return {\n",
    "            'phase_name': phase_name,\n",
    "            'metrics': {\n",
    "                'loss': 0.0,\n",
    "                'pearson': np.nan,\n",
    "                'spearman': np.nan,\n",
    "                'r2': np.nan,\n",
    "                'rmse': np.nan,\n",
    "                'mae': np.nan\n",
    "            },\n",
    "            'predictions': np.array([]),\n",
    "            'targets': np.array([]),\n",
    "            'n_samples': 0,\n",
    "            'n_cells': 0\n",
    "        }\n",
    "    \n",
    "    # Get pathway count from dataset or use global pathway_cols\n",
    "    if isinstance(test_loader.dataset, PrebatchedDataset):\n",
    "        # For pre-batched data, use the global pathway_cols length\n",
    "        pathway_dim = len(pathway_cols)\n",
    "    else:\n",
    "        pathway_dim = len(test_loader.dataset.pathway_cols)\n",
    "    \n",
    "    model = DrugResponsePathwayGNN(cell_input_dim=pathway_dim).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    classification_threshold = checkpoint.get('classification_threshold', None)\n",
    "    \n",
    "    # Check if using pre-batched data (batch_size=1 indicates pre-batched)\n",
    "    is_prebatched = isinstance(test_loader.dataset, PrebatchedDataset) or test_loader.batch_size == 1\n",
    "    \n",
    "    # Additional check: verify dataloader actually has batches (without consuming iterator)\n",
    "    if isinstance(test_loader.dataset, PrebatchedDataset):\n",
    "        # For PrebatchedDataset, check the batches list directly\n",
    "        if len(test_loader.dataset.batches) == 0:\n",
    "            print(f\"Warning: {phase_name} test loader has no batches - returning NaN metrics\")\n",
    "            return {\n",
    "                'phase_name': phase_name,\n",
    "                'metrics': {\n",
    "                    'loss': 0.0,\n",
    "                    'pearson': np.nan,\n",
    "                    'spearman': np.nan,\n",
    "                    'r2': np.nan,\n",
    "                    'rmse': np.nan,\n",
    "                    'mae': np.nan\n",
    "                },\n",
    "                'predictions': np.array([]),\n",
    "                'targets': np.array([]),\n",
    "                'n_samples': n_samples,\n",
    "                'n_cells': 0\n",
    "            }\n",
    "    \n",
    "    # Try to evaluate, catch any errors (e.g., empty dataloader issues)\n",
    "    try:\n",
    "        results = evaluate_model(model, test_loader, device, classification_threshold, is_prebatched=is_prebatched)\n",
    "    except (ValueError, IndexError) as e:\n",
    "        if \"concatenate\" in str(e) or \"empty\" in str(e).lower():\n",
    "            print(f\"Warning: {phase_name} evaluation failed due to empty data - returning NaN metrics\")\n",
    "            return {\n",
    "                'phase_name': phase_name,\n",
    "                'metrics': {\n",
    "                    'loss': 0.0,\n",
    "                    'pearson': np.nan,\n",
    "                    'spearman': np.nan,\n",
    "                    'r2': np.nan,\n",
    "                    'rmse': np.nan,\n",
    "                    'mae': np.nan\n",
    "                },\n",
    "                'predictions': np.array([]),\n",
    "                'targets': np.array([]),\n",
    "                'n_samples': n_samples,\n",
    "                'n_cells': 0\n",
    "            }\n",
    "        else:\n",
    "            raise  # Re-raise if it's a different error\n",
    "    \n",
    "    # Handle pre-batched datasets (they don't have .data attribute)\n",
    "    if isinstance(test_loader.dataset, PrebatchedDataset):\n",
    "        # Estimate n_cells from batch size and number of batches\n",
    "        n_cells = None  # Can't easily determine from pre-batched data\n",
    "    else:\n",
    "        n_cells = test_loader.dataset.data['ModelID'].nunique()\n",
    "    \n",
    "    return {\n",
    "        'phase_name': phase_name,\n",
    "        'metrics': results['metrics'],\n",
    "        'predictions': results['predictions'],\n",
    "        'targets': results['targets'],\n",
    "        'n_samples': n_samples,\n",
    "        'n_cells': n_cells\n",
    "    }\n",
    "\n",
    "# Evaluate all phases\n",
    "phase1_results = evaluate_phase_pathway(phase1_checkpoint, pan_loaders['test'], \"Phase 1: Pan-Cancer\", device)\n",
    "phase2_results = evaluate_phase_pathway(phase2_checkpoint, breast_loaders['test'], \"Phase 2: Breast Cancer\", device)\n",
    "phase3_results = evaluate_phase_pathway(phase3_checkpoint, tnbc_loaders['test'], \"Phase 3: TNBC\", device)\n",
    "\n",
    "# Results summary\n",
    "results_df = pd.DataFrame({\n",
    "    'Phase': ['Phase 1: Pan-Cancer', 'Phase 2: Breast Cancer', 'Phase 3: TNBC'],\n",
    "    'Pearson': [phase1_results['metrics']['pearson'], phase2_results['metrics']['pearson'], phase3_results['metrics']['pearson']],\n",
    "    'Spearman': [phase1_results['metrics']['spearman'], phase2_results['metrics']['spearman'], phase3_results['metrics']['spearman']],\n",
    "    'R²': [phase1_results['metrics']['r2'], phase2_results['metrics']['r2'], phase3_results['metrics']['r2']],\n",
    "    'RMSE': [phase1_results['metrics']['rmse'], phase2_results['metrics']['rmse'], phase3_results['metrics']['rmse']],\n",
    "    'MAE': [phase1_results['metrics']['mae'], phase2_results['metrics']['mae'], phase3_results['metrics']['mae']]\n",
    "})\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "print(f\"\\nGPDRP baseline: Pearson=0.8833, RMSE=0.0321\")\n",
    "pearson1 = phase1_results['metrics']['pearson']\n",
    "rmse1 = phase1_results['metrics']['rmse']\n",
    "# Format values properly - can't use conditional in format specifier\n",
    "pearson1_str = f\"{pearson1:.4f}\" if not np.isnan(pearson1) else 'N/A'\n",
    "rmse1_str = f\"{rmse1:.4f}\" if not np.isnan(rmse1) else 'N/A'\n",
    "print(f\"Phase 1: Pearson={pearson1_str}, RMSE={rmse1_str}\")\n",
    "if phase3_results['n_samples'] == 0:\n",
    "    print(f\"\\nWarning: Phase 3 (TNBC) test set is empty - no test data available for evaluation\")\n",
    "\n",
    "# Stop caffeinate after training completes\n",
    "try:\n",
    "    caffeinate_process.terminate()\n",
    "    caffeinate_process.wait(timeout=5)\n",
    "    print(\"\\nCaffeinate stopped - system can sleep normally now\")\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save performance metrics from current run\n",
    "split_suffix = f\"_{SPLIT_TYPE}_split\" if 'SPLIT_TYPE' in globals() else \"\"\n",
    "performance_csv_path = output_dir / f\"performance_metrics{split_suffix}.csv\"\n",
    "performance_pkl_path = output_dir / f\"performance_metrics{split_suffix}.pkl\"\n",
    "\n",
    "results_df.to_csv(performance_csv_path, index=False)\n",
    "print(f\"Performance metrics saved to CSV: {performance_csv_path}\")\n",
    "\n",
    "with open(performance_pkl_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'results_df': results_df,\n",
    "        'phase1_results': phase1_results,\n",
    "        'phase2_results': phase2_results,\n",
    "        'phase3_results': phase3_results,\n",
    "        'split_type': SPLIT_TYPE if 'SPLIT_TYPE' in globals() else 'unknown'\n",
    "    }, f)\n",
    "print(f\"Full performance results saved to pickle: {performance_pkl_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c874971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Classification Head Function\n",
    "# This function is used by Cell 19 to evaluate classification head performance\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, classification_report\n",
    "\n",
    "def evaluate_classification_head(model_path, test_loader, device, phase_name, median_ic50=None):\n",
    "    \"\"\"\n",
    "    Evaluate classification head predictions.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to model checkpoint\n",
    "        test_loader: Test DataLoader\n",
    "        device: Device to run on\n",
    "        phase_name: Name of the phase\n",
    "        median_ic50: Classification threshold (if None, computed from test data)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with predictions, probabilities, and ground truth\n",
    "    \"\"\"\n",
    "    # Get pathway count\n",
    "    if isinstance(test_loader.dataset, PrebatchedDataset):\n",
    "        pathway_dim = len(pathway_cols)\n",
    "    else:\n",
    "        pathway_dim = len(test_loader.dataset.pathway_cols)\n",
    "    \n",
    "    model = DrugResponsePathwayGNN(cell_input_dim=pathway_dim).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if median_ic50 is None:\n",
    "        checkpoint_median = checkpoint.get('classification_threshold', None)\n",
    "        if checkpoint_median is not None:\n",
    "            median_ic50 = checkpoint_median\n",
    "        else:\n",
    "            # Compute from test data\n",
    "            all_ic50s = []\n",
    "            for batch in test_loader:\n",
    "                all_ic50s.append(batch['ic50'].cpu().numpy())\n",
    "            median_ic50 = np.median(np.concatenate(all_ic50s))\n",
    "    \n",
    "    is_prebatched = isinstance(test_loader.dataset, PrebatchedDataset) or test_loader.batch_size == 1\n",
    "    \n",
    "    model.eval()\n",
    "    all_class_logits = []\n",
    "    all_ic50_preds = []\n",
    "    all_ic50_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            drug_batch = batch['drug_batch'].to(device)\n",
    "            cell_batch = batch['cell_batch'].to(device)\n",
    "            ic50_target = batch['ic50'].to(device)\n",
    "            \n",
    "            outputs = model(drug_batch, cell_batch, return_embeddings=False)\n",
    "            \n",
    "            all_class_logits.append(outputs['classification'].cpu().numpy())\n",
    "            all_ic50_preds.append(outputs['ic50'].cpu().numpy())\n",
    "            all_ic50_targets.append(ic50_target.cpu().numpy())\n",
    "    \n",
    "    class_logits = np.concatenate(all_class_logits).flatten()\n",
    "    class_probs = torch.sigmoid(torch.tensor(class_logits)).numpy()\n",
    "    ic50_preds = np.concatenate(all_ic50_preds).flatten()\n",
    "    ic50_targets = np.concatenate(all_ic50_targets).flatten()\n",
    "    \n",
    "    # Ground truth labels: 1 if IC50 > median (resistant), 0 if IC50 <= median (sensitive)\n",
    "    y_true = (ic50_targets > median_ic50).astype(int)\n",
    "    \n",
    "    # Binary predictions using 0.5 threshold\n",
    "    y_pred = (class_probs >= 0.5).astype(int)\n",
    "    \n",
    "    return {\n",
    "        'phase_name': phase_name,\n",
    "        'logits': class_logits,\n",
    "        'probabilities': class_probs,\n",
    "        'predictions': y_pred,\n",
    "        'ground_truth': y_true,\n",
    "        'ic50_targets': ic50_targets,\n",
    "        'ic50_preds': ic50_preds,\n",
    "        'median_ic50': median_ic50\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ce20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Recreate All Plots for CELL-LINE Split (Cell Blind)\n",
    "# This cell regenerates all visualizations for cell-line based split\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Recreating All Plots for CELL-LINE Split (Cell Blind)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# FORCE cell_line split\n",
    "SPLIT_TYPE = 'cell_line'\n",
    "print(f'Using SPLIT_TYPE: {SPLIT_TYPE}')\n",
    "\n",
    "# Check if data loaders exist, if not recreate them\n",
    "if 'pan_loaders' not in globals() or 'breast_loaders' not in globals() or 'tnbc_loaders' not in globals():\n",
    "    print(\"\\nData loaders not found. Recreating data splits and loaders with cell_line split...\")\n",
    "    \n",
    "    # Recreate splits using cell_line_split\n",
    "    pan_train_idx, pan_val_idx, pan_test_idx = cell_line_split(pan_cancer_pathway, random_seed=42)\n",
    "    \n",
    "    # Filter breast cancer and TNBC data to only include cell lines from appropriate global split\n",
    "    pan_train_cells = set(pan_cancer_pathway.loc[pan_train_idx, 'ModelID'].unique())\n",
    "    pan_val_cells = set(pan_cancer_pathway.loc[pan_val_idx, 'ModelID'].unique())\n",
    "    pan_test_cells = set(pan_cancer_pathway.loc[pan_test_idx, 'ModelID'].unique())\n",
    "    \n",
    "    breast_train_data = breast_cancer_pathway[breast_cancer_pathway['ModelID'].isin(pan_train_cells)].copy()\n",
    "    breast_val_data = breast_cancer_pathway[breast_cancer_pathway['ModelID'].isin(pan_val_cells)].copy()\n",
    "    breast_test_data = breast_cancer_pathway[breast_cancer_pathway['ModelID'].isin(pan_test_cells)].copy()\n",
    "    \n",
    "    tnbc_train_data = tnbc_pathway[tnbc_pathway['ModelID'].isin(pan_train_cells)].copy()\n",
    "    tnbc_val_data = tnbc_pathway[tnbc_pathway['ModelID'].isin(pan_val_cells)].copy()\n",
    "    tnbc_test_data = tnbc_pathway[tnbc_pathway['ModelID'].isin(pan_test_cells)].copy()\n",
    "    \n",
    "    breast_train_idx = breast_train_data.index.values\n",
    "    breast_val_idx = breast_val_data.index.values\n",
    "    breast_test_idx = breast_test_data.index.values\n",
    "    \n",
    "    tnbc_train_idx = tnbc_train_data.index.values\n",
    "    tnbc_val_idx = tnbc_val_data.index.values\n",
    "    tnbc_test_idx = tnbc_test_data.index.values\n",
    "    \n",
    "    # Recreate datasets\n",
    "    pan_dataset_pathway = DrugResponsePathwayDataset(pan_cancer_pathway, drug_graphs, drug_col=drug_name_col, pathway_cols=pathway_cols)\n",
    "    breast_dataset_pathway = DrugResponsePathwayDataset(breast_cancer_pathway, drug_graphs, drug_col=drug_name_col, pathway_cols=pathway_cols)\n",
    "    tnbc_dataset_pathway = DrugResponsePathwayDataset(tnbc_pathway, drug_graphs, drug_col=drug_name_col, pathway_cols=pathway_cols)\n",
    "    \n",
    "    # Recreate loaders\n",
    "    pan_loaders = create_pathway_dataloaders(\n",
    "        pan_dataset_pathway, pan_train_idx, pan_val_idx, pan_test_idx, \n",
    "        batch_size=256, use_prebatched=True, phase_name=\"phase1\", output_dir=output_dir\n",
    "    )\n",
    "    breast_loaders = create_pathway_dataloaders(\n",
    "        breast_dataset_pathway, breast_train_idx, breast_val_idx, breast_test_idx, \n",
    "        batch_size=256, use_prebatched=True, phase_name=\"phase2\", output_dir=output_dir\n",
    "    )\n",
    "    tnbc_loaders = create_pathway_dataloaders(\n",
    "        tnbc_dataset_pathway, tnbc_train_idx, tnbc_val_idx, tnbc_test_idx, \n",
    "        batch_size=256, use_prebatched=True, phase_name=\"phase3\", output_dir=output_dir\n",
    "    )\n",
    "    print(\"Data loaders recreated successfully with cell_line split!\")\n",
    "else:\n",
    "    print(\"Data loaders found. Using existing loaders.\")\n",
    "\n",
    "# Verify model checkpoints exist\n",
    "# Verify model checkpoints exist\n",
    "models_dir = output_dir / \"models\"\n",
    "split_type = SPLIT_TYPE if 'SPLIT_TYPE' in globals() else 'unknown'\n",
    "\n",
    "# Check for existing models - try current split_type first, then fallback to cellsplit (old naming)\n",
    "def find_checkpoint(phase_num, split_type):\n",
    "    \"\"\"Find checkpoint file - uses cellsplit models (cell-line split).\"\"\"\n",
    "    models_dir = output_dir / \"models\"\n",
    "    \n",
    "    # cellsplit = old name for cell_line split\n",
    "    if phase_num == 1:\n",
    "        name = \"trial2_phase1_pathway_cellsplit.pt\"\n",
    "    elif phase_num == 2:\n",
    "        name = \"trial2_phase2_breast_pathway_cellsplit.pt\"\n",
    "    elif phase_num == 3:\n",
    "        name = \"trial2_phase3_tnbc_pathway_cellsplit.pt\"\n",
    "    \n",
    "    path = models_dir / name\n",
    "    if not path.exists():\n",
    "        print(f\"ERROR: Cell-line checkpoint not found: {path}\")\n",
    "    else:\n",
    "        print(f\"Using cell-line model: {name}\")\n",
    "    return path\n",
    "phase1_checkpoint = find_checkpoint(1, split_type)\n",
    "phase2_checkpoint = find_checkpoint(2, split_type)\n",
    "phase3_checkpoint = find_checkpoint(3, split_type)\n",
    "\n",
    "if not checkpoints_exist:\n",
    "    print(\"\\nWarning: Some model checkpoints are missing!\")\n",
    "    print(f\"Phase 1 exists: {phase1_checkpoint.exists()}\")\n",
    "    print(f\"Phase 2 exists: {phase2_checkpoint.exists()}\")\n",
    "    print(f\"Phase 3 exists: {phase3_checkpoint.exists()}\")\n",
    "else:\n",
    "    print(\"\\nAll model checkpoints found. Proceeding with plot generation...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 1: Regenerating Performance Metrics Evaluation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Re-evaluate all phases\n",
    "phase1_results = evaluate_phase_pathway(phase1_checkpoint, pan_loaders['test'], \"Phase 1: Pan-Cancer\", device)\n",
    "phase2_results = evaluate_phase_pathway(phase2_checkpoint, breast_loaders['test'], \"Phase 2: Breast Cancer\", device)\n",
    "phase3_results = evaluate_phase_pathway(phase3_checkpoint, tnbc_loaders['test'], \"Phase 3: TNBC\", device)\n",
    "\n",
    "# Recreate results dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Phase': ['Phase 1: Pan-Cancer', 'Phase 2: Breast Cancer', 'Phase 3: TNBC'],\n",
    "    'Pearson': [phase1_results['metrics']['pearson'], phase2_results['metrics']['pearson'], phase3_results['metrics']['pearson']],\n",
    "    'Spearman': [phase1_results['metrics']['spearman'], phase2_results['metrics']['spearman'], phase3_results['metrics']['spearman']],\n",
    "    'R²': [phase1_results['metrics']['r2'], phase2_results['metrics']['r2'], phase3_results['metrics']['r2']],\n",
    "    'RMSE': [phase1_results['metrics']['rmse'], phase2_results['metrics']['rmse'], phase3_results['metrics']['rmse']],\n",
    "    'MAE': [phase1_results['metrics']['mae'], phase2_results['metrics']['mae'], phase3_results['metrics']['mae']]\n",
    "})\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save updated performance metrics\n",
    "split_suffix = f\"_{SPLIT_TYPE}_split\"\n",
    "performance_csv_path = output_dir / f\"performance_metrics{split_suffix}.csv\"\n",
    "performance_pkl_path = output_dir / f\"performance_metrics{split_suffix}.pkl\"\n",
    "\n",
    "results_df.to_csv(performance_csv_path, index=False)\n",
    "print(f\"\\nPerformance metrics saved to CSV: {performance_csv_path}\")\n",
    "\n",
    "with open(performance_pkl_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'results_df': results_df,\n",
    "        'phase1_results': phase1_results,\n",
    "        'phase2_results': phase2_results,\n",
    "        'phase3_results': phase3_results,\n",
    "        'split_type': SPLIT_TYPE\n",
    "    }, f)\n",
    "print(f\"Full performance results saved to pickle: {performance_pkl_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Step 2: Regenerating Classification Head Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Re-evaluate classification head\n",
    "class_results_phase1 = evaluate_classification_head(phase1_checkpoint, pan_loaders['test'], device, \"Phase 1: Pan-Cancer\")\n",
    "class_results_phase2 = evaluate_classification_head(phase2_checkpoint, breast_loaders['test'], device, \"Phase 2: Breast Cancer\")\n",
    "class_results_phase3 = evaluate_classification_head(phase3_checkpoint, tnbc_loaders['test'], device, \"Phase 3: TNBC\")\n",
    "\n",
    "# Recreate classification visualization (same as Cell 18)\n",
    "fig = plt.figure(figsize=(20, 15))\n",
    "\n",
    "results = class_results_phase3\n",
    "y_true = results['ground_truth']\n",
    "y_pred = results['predictions']\n",
    "y_probs = results['probabilities']\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1, cbar_kws={'label': 'Count'})\n",
    "ax1.set_xlabel('Predicted (0=Sensitive, 1=Resistant)')\n",
    "ax1.set_ylabel('True (0=Sensitive, 1=Resistant)')\n",
    "ax1.set_title(f'{results[\"phase_name\"]}\\nConfusion Matrix')\n",
    "\n",
    "# 2. ROC Curve\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "ax2.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "ax2.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC Curve')\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Precision-Recall Curve\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
    "pr_auc = auc(recall, precision)\n",
    "ax3.plot(recall, precision, color='darkgreen', lw=2, label=f'PR curve (AUC = {pr_auc:.3f})')\n",
    "baseline = np.sum(y_true) / len(y_true)\n",
    "ax3.axhline(y=baseline, color='navy', linestyle='--', label=f'Baseline = {baseline:.3f}')\n",
    "ax3.set_xlabel('Recall')\n",
    "ax3.set_ylabel('Precision')\n",
    "ax3.set_title('Precision-Recall Curve')\n",
    "ax3.legend(loc=\"lower left\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Probability Distribution by Class\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "sensitive_probs = y_probs[y_true == 0]\n",
    "resistant_probs = y_probs[y_true == 1]\n",
    "ax4.hist(sensitive_probs, bins=30, alpha=0.6, label='Sensitive (IC50 ≤ median)', color='green', density=True)\n",
    "ax4.hist(resistant_probs, bins=30, alpha=0.6, label='Resistant (IC50 > median)', color='red', density=True)\n",
    "ax4.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold (0.5)')\n",
    "ax4.set_xlabel('Predicted Probability')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.set_title('Probability Distribution by True Class')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Classification Report\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "ax5.axis('off')\n",
    "report_text = classification_report(y_true, y_pred, target_names=['Sensitive', 'Resistant'])\n",
    "ax5.text(0.1, 0.5, report_text, fontsize=10, family='monospace', verticalalignment='center')\n",
    "ax5.set_title('Classification Report')\n",
    "\n",
    "# 6. IC50 vs Classification Probability\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "scatter = ax6.scatter(results['ic50_targets'], y_probs, c=y_true, cmap='RdYlGn', alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "ax6.axhline(y=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "ax6.axvline(x=results['median_ic50'], color='blue', linestyle='--', linewidth=2, label=f'Median IC50 = {results[\"median_ic50\"]:.2f}')\n",
    "ax6.set_xlabel('True IC50 (LN_IC50)')\n",
    "ax6.set_ylabel('Predicted Classification Probability')\n",
    "ax6.set_title('IC50 vs Classification Probability')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(scatter, ax=ax6)\n",
    "cbar.set_label('True Label (0=Sensitive, 1=Resistant)')\n",
    "\n",
    "# 7. ROC Comparison\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "fpr_p1, tpr_p1, _ = roc_curve(class_results_phase1['ground_truth'], class_results_phase1['probabilities'])\n",
    "roc_auc_p1 = auc(fpr_p1, tpr_p1)\n",
    "fpr_p2, tpr_p2, _ = roc_curve(class_results_phase2['ground_truth'], class_results_phase2['probabilities'])\n",
    "roc_auc_p2 = auc(fpr_p2, tpr_p2)\n",
    "fpr_p3, tpr_p3, _ = roc_curve(y_true, y_probs)\n",
    "roc_auc_p3 = auc(fpr_p3, tpr_p3)\n",
    "\n",
    "ax7.plot(fpr_p1, tpr_p1, label=f'Phase 1 (AUC={roc_auc_p1:.3f})', lw=2)\n",
    "ax7.plot(fpr_p2, tpr_p2, label=f'Phase 2 (AUC={roc_auc_p2:.3f})', lw=2)\n",
    "ax7.plot(fpr_p3, tpr_p3, label=f'Phase 3 (AUC={roc_auc_p3:.3f})', lw=2)\n",
    "ax7.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "ax7.set_xlabel('False Positive Rate')\n",
    "ax7.set_ylabel('True Positive Rate')\n",
    "ax7.set_title('ROC Curves: All Phases')\n",
    "ax7.legend()\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Metrics Comparison\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "phases = ['Phase 1', 'Phase 2', 'Phase 3']\n",
    "all_results = [class_results_phase1, class_results_phase2, class_results_phase3]\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "for res in all_results:\n",
    "    y_t = res['ground_truth']\n",
    "    y_p = res['predictions']\n",
    "    accuracies.append(accuracy_score(y_t, y_p))\n",
    "    precisions.append(precision_score(y_t, y_p, zero_division=0))\n",
    "    recalls.append(recall_score(y_t, y_p, zero_division=0))\n",
    "    f1_scores.append(f1_score(y_t, y_p, zero_division=0))\n",
    "\n",
    "x = np.arange(len(phases))\n",
    "width = 0.2\n",
    "ax8.bar(x - 1.5*width, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "ax8.bar(x - 0.5*width, precisions, width, label='Precision', alpha=0.8)\n",
    "ax8.bar(x + 0.5*width, recalls, width, label='Recall', alpha=0.8)\n",
    "ax8.bar(x + 1.5*width, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "ax8.set_xlabel('Phase')\n",
    "ax8.set_ylabel('Score')\n",
    "ax8.set_title('Classification Metrics Comparison')\n",
    "ax8.set_xticks(x)\n",
    "ax8.set_xticklabels(phases)\n",
    "ax8.legend()\n",
    "ax8.set_ylim([0, 1.1])\n",
    "ax8.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 9. Probability Calibration\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "n_bins = 10\n",
    "bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "bin_lowers = bin_boundaries[:-1]\n",
    "bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "fraction_of_positives = []\n",
    "mean_predicted_value = []\n",
    "\n",
    "for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "    mask = (y_probs > bin_lower) & (y_probs <= bin_upper)\n",
    "    if mask.sum() > 0:\n",
    "        fraction_of_positives.append(y_true[mask].mean())\n",
    "        mean_predicted_value.append(y_probs[mask].mean())\n",
    "    else:\n",
    "        fraction_of_positives.append(0)\n",
    "        mean_predicted_value.append(0)\n",
    "\n",
    "ax9.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "ax9.plot(mean_predicted_value, fraction_of_positives, 's-', label='Model Calibration', markersize=8)\n",
    "ax9.set_xlabel('Mean Predicted Probability')\n",
    "ax9.set_ylabel('Fraction of Positives')\n",
    "ax9.set_title('Calibration Plot')\n",
    "ax9.legend()\n",
    "ax9.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "classification_plot_path = output_dir / 'classification_head_analysis.png'\n",
    "plt.savefig(classification_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nClassification head analysis saved to: {classification_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Classification Head Performance Summary\")\n",
    "print(\"=\"*80)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "for phase_name, res in zip(['Phase 1: Pan-Cancer', 'Phase 2: Breast Cancer', 'Phase 3: TNBC'], \n",
    "                           [class_results_phase1, class_results_phase2, class_results_phase3]):\n",
    "    y_t = res['ground_truth']\n",
    "    y_p = res['predictions']\n",
    "    y_probs = res['probabilities']\n",
    "    print(f\"\\n{phase_name}:\")\n",
    "    print(f\"  Accuracy:  {accuracy_score(y_t, y_p):.4f}\")\n",
    "    print(f\"  Precision: {precision_score(y_t, y_p, zero_division=0):.4f}\")\n",
    "    print(f\"  Recall:    {recall_score(y_t, y_p, zero_division=0):.4f}\")\n",
    "    print(f\"  F1-Score:  {f1_score(y_t, y_p, zero_division=0):.4f}\")\n",
    "    print(f\"  ROC-AUC:   {roc_auc_score(y_t, y_probs):.4f}\")\n",
    "    print(f\"  Samples:   {len(y_t)} (Sensitive: {np.sum(y_t==0)}, Resistant: {np.sum(y_t==1)})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"All plots regenerated successfully!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Pan-Cancer Model vs Fine-tuned TNBC Model on TNBC Test Data\n",
    "# This analysis shows the improvement from fine-tuning on TNBC-specific data\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Comparing Pan-Cancer Model vs Fine-tuned TNBC Model on TNBC Test Data\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure we have the checkpoint paths\n",
    "models_dir = output_dir / \"models\"\n",
    "split_type = SPLIT_TYPE if 'SPLIT_TYPE' in globals() else 'cell_line'\n",
    "\n",
    "# Use cellsplit models (cell-line split)\n",
    "phase1_checkpoint = models_dir / \"trial2_phase1_pathway_cellsplit.pt\"\n",
    "phase3_checkpoint = models_dir / \"trial2_phase3_tnbc_pathway_cellsplit.pt\"\n",
    "\n",
    "print(f\"\\nUsing split type: {split_type}\")\n",
    "print(f\"Phase 1 (Pan-Cancer) checkpoint: {phase1_checkpoint}\")\n",
    "print(f\"Phase 3 (TNBC Fine-tuned) checkpoint: {phase3_checkpoint}\")\n",
    "\n",
    "# Ensure TNBC test loader exists\n",
    "if 'tnbc_loaders' not in globals():\n",
    "    print(\"\\nWarning: TNBC loaders not found. Recreating...\")\n",
    "    # Recreate TNBC splits if needed\n",
    "    if 'tnbc_test_idx' not in globals():\n",
    "        pan_train_idx, pan_val_idx, pan_test_idx = cell_line_split(pan_cancer_pathway, random_seed=42)\n",
    "        pan_train_cells = set(pan_cancer_pathway.loc[pan_train_idx, 'ModelID'].unique())\n",
    "        pan_val_cells = set(pan_cancer_pathway.loc[pan_val_idx, 'ModelID'].unique())\n",
    "        pan_test_cells = set(pan_cancer_pathway.loc[pan_test_idx, 'ModelID'].unique())\n",
    "        \n",
    "        tnbc_test_data = tnbc_pathway[tnbc_pathway['ModelID'].isin(pan_test_cells)].copy()\n",
    "        tnbc_test_idx = tnbc_test_data.index.values\n",
    "    \n",
    "    tnbc_dataset_pathway = DrugResponsePathwayDataset(tnbc_pathway, drug_graphs, drug_col=drug_name_col, pathway_cols=pathway_cols)\n",
    "    tnbc_loaders = create_pathway_dataloaders(\n",
    "        tnbc_dataset_pathway, tnbc_train_idx, tnbc_val_idx, tnbc_test_idx, \n",
    "        batch_size=256, use_prebatched=True, phase_name=\"phase3\", output_dir=output_dir\n",
    "    )\n",
    "\n",
    "# Evaluate Phase 1 (Pan-Cancer) model on TNBC test data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Evaluating Phase 1 (Pan-Cancer) Model on TNBC Test Data\")\n",
    "print(\"=\"*80)\n",
    "phase1_on_tnbc = evaluate_phase_pathway(phase1_checkpoint, tnbc_loaders['test'], \"Phase 1 (Pan-Cancer) on TNBC\", device)\n",
    "\n",
    "# Evaluate Phase 3 (TNBC Fine-tuned) model on TNBC test data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Evaluating Phase 3 (TNBC Fine-tuned) Model on TNBC Test Data\")\n",
    "print(\"=\"*80)\n",
    "phase3_on_tnbc = evaluate_phase_pathway(phase3_checkpoint, tnbc_loaders['test'], \"Phase 3 (TNBC Fine-tuned) on TNBC\", device)\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Phase 1: Pan-Cancer', 'Phase 3: TNBC Fine-tuned', 'Improvement'],\n",
    "    'Pearson': [\n",
    "        phase1_on_tnbc['metrics']['pearson'],\n",
    "        phase3_on_tnbc['metrics']['pearson'],\n",
    "        phase3_on_tnbc['metrics']['pearson'] - phase1_on_tnbc['metrics']['pearson']\n",
    "    ],\n",
    "    'Spearman': [\n",
    "        phase1_on_tnbc['metrics']['spearman'],\n",
    "        phase3_on_tnbc['metrics']['spearman'],\n",
    "        phase3_on_tnbc['metrics']['spearman'] - phase1_on_tnbc['metrics']['spearman']\n",
    "    ],\n",
    "    'R²': [\n",
    "        phase1_on_tnbc['metrics']['r2'],\n",
    "        phase3_on_tnbc['metrics']['r2'],\n",
    "        phase3_on_tnbc['metrics']['r2'] - phase1_on_tnbc['metrics']['r2']\n",
    "    ],\n",
    "    'RMSE': [\n",
    "        phase1_on_tnbc['metrics']['rmse'],\n",
    "        phase3_on_tnbc['metrics']['rmse'],\n",
    "        phase1_on_tnbc['metrics']['rmse'] - phase3_on_tnbc['metrics']['rmse']  # Lower is better\n",
    "    ],\n",
    "    'MAE': [\n",
    "        phase1_on_tnbc['metrics']['mae'],\n",
    "        phase3_on_tnbc['metrics']['mae'],\n",
    "        phase1_on_tnbc['metrics']['mae'] - phase3_on_tnbc['metrics']['mae']  # Lower is better\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Performance Comparison on TNBC Test Data\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. Metrics Comparison Bar Chart\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "metrics = ['Pearson', 'Spearman', 'R²']\n",
    "metric_keys = {'Pearson': 'pearson', 'Spearman': 'spearman', 'R²': 'r2'}\n",
    "phase1_vals = [phase1_on_tnbc['metrics'][metric_keys[m]] for m in metrics]\n",
    "phase3_vals = [phase3_on_tnbc['metrics'][metric_keys[m]] for m in metrics]\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, phase1_vals, width, label='Phase 1: Pan-Cancer', alpha=0.8, color='steelblue')\n",
    "ax1.bar(x + width/2, phase3_vals, width, label='Phase 3: TNBC Fine-tuned', alpha=0.8, color='darkgreen')\n",
    "ax1.set_xlabel('Metric')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Performance Comparison: Correlation Metrics')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(metrics)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.set_ylim([0, 1.0])\n",
    "\n",
    "# 2. Error Metrics Comparison\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "error_metrics = ['RMSE', 'MAE']\n",
    "phase1_errors = [phase1_on_tnbc['metrics'][m.lower()] for m in error_metrics]\n",
    "phase3_errors = [phase3_on_tnbc['metrics'][m.lower()] for m in error_metrics]\n",
    "x2 = np.arange(len(error_metrics))\n",
    "ax2.bar(x2 - width/2, phase1_errors, width, label='Phase 1: Pan-Cancer', alpha=0.8, color='steelblue')\n",
    "ax2.bar(x2 + width/2, phase3_errors, width, label='Phase 3: TNBC Fine-tuned', alpha=0.8, color='darkgreen')\n",
    "ax2.set_xlabel('Metric')\n",
    "ax2.set_ylabel('Error')\n",
    "ax2.set_title('Performance Comparison: Error Metrics')\n",
    "ax2.set_xticks(x2)\n",
    "ax2.set_xticklabels(error_metrics)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Scatter Plot: Phase 1 Predictions vs True\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "preds_p1 = phase1_on_tnbc['predictions']\n",
    "targets = phase1_on_tnbc['targets']\n",
    "ax3.scatter(targets, preds_p1, alpha=0.5, s=20, color='steelblue', edgecolors='black', linewidth=0.3)\n",
    "min_val = min(targets.min(), preds_p1.min())\n",
    "max_val = max(targets.max(), preds_p1.max())\n",
    "ax3.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "ax3.set_xlabel('True IC50 (LN_IC50)')\n",
    "ax3.set_ylabel('Predicted IC50 (LN_IC50)')\n",
    "ax3.set_title(f'Phase 1: Pan-Cancer\\nPearson={phase1_on_tnbc[\"metrics\"][\"pearson\"]:.4f}, R²={phase1_on_tnbc[\"metrics\"][\"r2\"]:.4f}')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Scatter Plot: Phase 3 Predictions vs True\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "preds_p3 = phase3_on_tnbc['predictions']\n",
    "ax4.scatter(targets, preds_p3, alpha=0.5, s=20, color='darkgreen', edgecolors='black', linewidth=0.3)\n",
    "min_val = min(targets.min(), preds_p3.min())\n",
    "max_val = max(targets.max(), preds_p3.max())\n",
    "ax4.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "ax4.set_xlabel('True IC50 (LN_IC50)')\n",
    "ax4.set_ylabel('Predicted IC50 (LN_IC50)')\n",
    "ax4.set_title(f'Phase 3: TNBC Fine-tuned\\nPearson={phase3_on_tnbc[\"metrics\"][\"pearson\"]:.4f}, R²={phase3_on_tnbc[\"metrics\"][\"r2\"]:.4f}')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Residual Comparison\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "residuals_p1 = targets - preds_p1\n",
    "residuals_p3 = targets - preds_p3\n",
    "ax5.hist(residuals_p1, bins=50, alpha=0.6, label='Phase 1: Pan-Cancer', color='steelblue', density=True)\n",
    "ax5.hist(residuals_p3, bins=50, alpha=0.6, label='Phase 3: TNBC Fine-tuned', color='darkgreen', density=True)\n",
    "ax5.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "ax5.set_xlabel('Residual (True - Predicted)')\n",
    "ax5.set_ylabel('Density')\n",
    "ax5.set_title('Residual Distribution Comparison')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Improvement Summary\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "ax6.axis('off')\n",
    "improvement_text = f\"\"\"\n",
    "Performance Improvement from Fine-tuning:\n",
    "\n",
    "Pearson Correlation: {phase3_on_tnbc['metrics']['pearson'] - phase1_on_tnbc['metrics']['pearson']:+.4f}\n",
    "Spearman Correlation: {phase3_on_tnbc['metrics']['spearman'] - phase1_on_tnbc['metrics']['spearman']:+.4f}\n",
    "R² Score: {phase3_on_tnbc['metrics']['r2'] - phase1_on_tnbc['metrics']['r2']:+.4f}\n",
    "RMSE Reduction: {phase1_on_tnbc['metrics']['rmse'] - phase3_on_tnbc['metrics']['rmse']:+.4f}\n",
    "MAE Reduction: {phase1_on_tnbc['metrics']['mae'] - phase3_on_tnbc['metrics']['mae']:+.4f}\n",
    "\n",
    "Test Samples: {len(targets)}\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.5, improvement_text, fontsize=12, family='monospace', verticalalignment='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "ax6.set_title('Fine-tuning Improvement Summary', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_plot_path = output_dir / 'pan_cancer_vs_tnbc_comparison.png'\n",
    "plt.savefig(comparison_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nComparison plot saved to: {comparison_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Save comparison results\n",
    "comparison_csv_path = output_dir / f'pan_cancer_vs_tnbc_comparison_{split_type}.csv'\n",
    "comparison_df.to_csv(comparison_csv_path, index=False)\n",
    "print(f\"Comparison results saved to CSV: {comparison_csv_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Comparison Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Classification Performance: Pan-Cancer Model vs Fine-tuned TNBC Model on TNBC Test Data\n",
    "# This analysis shows the improvement in classification performance from fine-tuning\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Comparing Classification Performance: Pan-Cancer vs Fine-tuned TNBC Model on TNBC Test Data\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ensure we have the checkpoint paths\n",
    "models_dir = output_dir / \"models\"\n",
    "split_type = SPLIT_TYPE if 'SPLIT_TYPE' in globals() else 'cell_line'\n",
    "\n",
    "# Use cellsplit models (cell-line split)\n",
    "phase1_checkpoint = models_dir / \"trial2_phase1_pathway_cellsplit.pt\"\n",
    "phase3_checkpoint = models_dir / \"trial2_phase3_tnbc_pathway_cellsplit.pt\"\n",
    "\n",
    "print(f\"\\nUsing split type: {split_type}\")\n",
    "print(f\"Phase 1 (Pan-Cancer) checkpoint: {phase1_checkpoint}\")\n",
    "print(f\"Phase 3 (TNBC Fine-tuned) checkpoint: {phase3_checkpoint}\")\n",
    "\n",
    "# Ensure TNBC test loader exists\n",
    "if 'tnbc_loaders' not in globals():\n",
    "    print(\"\\nWarning: TNBC loaders not found. Recreating...\")\n",
    "    if 'tnbc_test_idx' not in globals():\n",
    "        pan_train_idx, pan_val_idx, pan_test_idx = cell_line_split(pan_cancer_pathway, random_seed=42)\n",
    "        pan_train_cells = set(pan_cancer_pathway.loc[pan_train_idx, 'ModelID'].unique())\n",
    "        pan_val_cells = set(pan_cancer_pathway.loc[pan_val_idx, 'ModelID'].unique())\n",
    "        pan_test_cells = set(pan_cancer_pathway.loc[pan_test_idx, 'ModelID'].unique())\n",
    "        \n",
    "        tnbc_test_data = tnbc_pathway[tnbc_pathway['ModelID'].isin(pan_test_cells)].copy()\n",
    "        tnbc_test_idx = tnbc_test_data.index.values\n",
    "    \n",
    "    tnbc_dataset_pathway = DrugResponsePathwayDataset(tnbc_pathway, drug_graphs, drug_col=drug_name_col, pathway_cols=pathway_cols)\n",
    "    tnbc_loaders = create_pathway_dataloaders(\n",
    "        tnbc_dataset_pathway, tnbc_train_idx, tnbc_val_idx, tnbc_test_idx, \n",
    "        batch_size=256, use_prebatched=True, phase_name=\"phase3\", output_dir=output_dir\n",
    "    )\n",
    "\n",
    "# Compute median_ic50 from TNBC test data to ensure fair comparison\n",
    "# Both models will use the same threshold for ground truth labels\n",
    "print(\"\\nComputing classification threshold from TNBC test data...\")\n",
    "all_tnbc_ic50s = []\n",
    "for batch in tnbc_loaders['test']:\n",
    "    all_tnbc_ic50s.append(batch['ic50'].cpu().numpy())\n",
    "tnbc_test_median_ic50 = np.median(np.concatenate(all_tnbc_ic50s))\n",
    "print(f\"Using median IC50 threshold: {tnbc_test_median_ic50:.4f} (computed from TNBC test data)\")\n",
    "\n",
    "# Evaluate Phase 1 (Pan-Cancer) classification head on TNBC test data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Evaluating Phase 1 (Pan-Cancer) Classification Head on TNBC Test Data\")\n",
    "print(\"=\"*80)\n",
    "class_phase1_on_tnbc = evaluate_classification_head(phase1_checkpoint, tnbc_loaders['test'], device, \"Phase 1 (Pan-Cancer) on TNBC\", median_ic50=tnbc_test_median_ic50)\n",
    "\n",
    "# Evaluate Phase 3 (TNBC Fine-tuned) classification head on TNBC test data\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Evaluating Phase 3 (TNBC Fine-tuned) Classification Head on TNBC Test Data\")\n",
    "print(\"=\"*80)\n",
    "class_phase3_on_tnbc = evaluate_classification_head(phase3_checkpoint, tnbc_loaders['test'], device, \"Phase 3 (TNBC Fine-tuned) on TNBC\", median_ic50=tnbc_test_median_ic50)\n",
    "\n",
    "# Calculate classification metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "y_true = class_phase1_on_tnbc['ground_truth']\n",
    "y_pred_p1 = class_phase1_on_tnbc['predictions']\n",
    "y_probs_p1 = class_phase1_on_tnbc['probabilities']\n",
    "y_pred_p3 = class_phase3_on_tnbc['predictions']\n",
    "y_probs_p3 = class_phase3_on_tnbc['probabilities']\n",
    "\n",
    "metrics_p1 = {\n",
    "    'accuracy': accuracy_score(y_true, y_pred_p1),\n",
    "    'precision': precision_score(y_true, y_pred_p1, zero_division=0),\n",
    "    'recall': recall_score(y_true, y_pred_p1, zero_division=0),\n",
    "    'f1': f1_score(y_true, y_pred_p1, zero_division=0),\n",
    "    'roc_auc': roc_auc_score(y_true, y_probs_p1)\n",
    "}\n",
    "\n",
    "metrics_p3 = {\n",
    "    'accuracy': accuracy_score(y_true, y_pred_p3),\n",
    "    'precision': precision_score(y_true, y_pred_p3, zero_division=0),\n",
    "    'recall': recall_score(y_true, y_pred_p3, zero_division=0),\n",
    "    'f1': f1_score(y_true, y_pred_p3, zero_division=0),\n",
    "    'roc_auc': roc_auc_score(y_true, y_probs_p3)\n",
    "}\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_class_df = pd.DataFrame({\n",
    "    'Model': ['Phase 1: Pan-Cancer', 'Phase 3: TNBC Fine-tuned', 'Improvement'],\n",
    "    'Accuracy': [\n",
    "        metrics_p1['accuracy'],\n",
    "        metrics_p3['accuracy'],\n",
    "        metrics_p3['accuracy'] - metrics_p1['accuracy']\n",
    "    ],\n",
    "    'Precision': [\n",
    "        metrics_p1['precision'],\n",
    "        metrics_p3['precision'],\n",
    "        metrics_p3['precision'] - metrics_p1['precision']\n",
    "    ],\n",
    "    'Recall': [\n",
    "        metrics_p1['recall'],\n",
    "        metrics_p3['recall'],\n",
    "        metrics_p3['recall'] - metrics_p1['recall']\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        metrics_p1['f1'],\n",
    "        metrics_p3['f1'],\n",
    "        metrics_p3['f1'] - metrics_p1['f1']\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        metrics_p1['roc_auc'],\n",
    "        metrics_p3['roc_auc'],\n",
    "        metrics_p3['roc_auc'] - metrics_p1['roc_auc']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Classification Performance Comparison on TNBC Test Data\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_class_df.to_string(index=False))\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. Classification Metrics Comparison Bar Chart\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "class_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "metric_keys = {'Accuracy': 'accuracy', 'Precision': 'precision', 'Recall': 'recall', 'F1-Score': 'f1', 'ROC-AUC': 'roc_auc'}\n",
    "phase1_class_vals = [metrics_p1[metric_keys[m]] for m in class_metrics]\n",
    "phase3_class_vals = [metrics_p3[metric_keys[m]] for m in class_metrics]\n",
    "x = np.arange(len(class_metrics))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, phase1_class_vals, width, label='Phase 1: Pan-Cancer', alpha=0.8, color='steelblue')\n",
    "ax1.bar(x + width/2, phase3_class_vals, width, label='Phase 3: TNBC Fine-tuned', alpha=0.8, color='darkgreen')\n",
    "ax1.set_xlabel('Metric')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Classification Metrics Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(class_metrics, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.set_ylim([0, 1.1])\n",
    "\n",
    "# 2. ROC Curve Comparison\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "fpr_p1, tpr_p1, _ = roc_curve(y_true, y_probs_p1)\n",
    "roc_auc_p1 = auc(fpr_p1, tpr_p1)\n",
    "fpr_p3, tpr_p3, _ = roc_curve(y_true, y_probs_p3)\n",
    "roc_auc_p3 = auc(fpr_p3, tpr_p3)\n",
    "\n",
    "ax2.plot(fpr_p1, tpr_p1, label=f'Phase 1 (AUC={roc_auc_p1:.3f})', lw=2, color='steelblue')\n",
    "ax2.plot(fpr_p3, tpr_p3, label=f'Phase 3 (AUC={roc_auc_p3:.3f})', lw=2, color='darkgreen')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', label='Random', lw=1)\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC Curves Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Precision-Recall Curve Comparison\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "precision_p1, recall_p1, _ = precision_recall_curve(y_true, y_probs_p1)\n",
    "pr_auc_p1 = auc(recall_p1, precision_p1)\n",
    "precision_p3, recall_p3, _ = precision_recall_curve(y_true, y_probs_p3)\n",
    "pr_auc_p3 = auc(recall_p3, precision_p3)\n",
    "\n",
    "baseline = np.sum(y_true) / len(y_true)\n",
    "ax3.plot(recall_p1, precision_p1, label=f'Phase 1 (AUC={pr_auc_p1:.3f})', lw=2, color='steelblue')\n",
    "ax3.plot(recall_p3, precision_p3, label=f'Phase 3 (AUC={pr_auc_p3:.3f})', lw=2, color='darkgreen')\n",
    "ax3.axhline(y=baseline, color='navy', linestyle='--', label=f'Baseline={baseline:.3f}')\n",
    "ax3.set_xlabel('Recall')\n",
    "ax3.set_ylabel('Precision')\n",
    "ax3.set_title('Precision-Recall Curves Comparison')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Confusion Matrix: Phase 1\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "cm_p1 = confusion_matrix(y_true, y_pred_p1)\n",
    "sns.heatmap(cm_p1, annot=True, fmt='d', cmap='Blues', ax=ax4, cbar_kws={'label': 'Count'})\n",
    "ax4.set_xlabel('Predicted (0=Sensitive, 1=Resistant)')\n",
    "ax4.set_ylabel('True (0=Sensitive, 1=Resistant)')\n",
    "ax4.set_title(f'Phase 1: Pan-Cancer\\nAccuracy={metrics_p1[\"accuracy\"]:.4f}')\n",
    "\n",
    "# 5. Confusion Matrix: Phase 3\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "cm_p3 = confusion_matrix(y_true, y_pred_p3)\n",
    "sns.heatmap(cm_p3, annot=True, fmt='d', cmap='Greens', ax=ax5, cbar_kws={'label': 'Count'})\n",
    "ax5.set_xlabel('Predicted (0=Sensitive, 1=Resistant)')\n",
    "ax5.set_ylabel('True (0=Sensitive, 1=Resistant)')\n",
    "ax5.set_title(f'Phase 3: TNBC Fine-tuned\\nAccuracy={metrics_p3[\"accuracy\"]:.4f}')\n",
    "\n",
    "# 6. Probability Distribution Comparison\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "sensitive_probs_p1 = y_probs_p1[y_true == 0]\n",
    "resistant_probs_p1 = y_probs_p1[y_true == 1]\n",
    "sensitive_probs_p3 = y_probs_p3[y_true == 0]\n",
    "resistant_probs_p3 = y_probs_p3[y_true == 1]\n",
    "\n",
    "ax6.hist(sensitive_probs_p1, bins=30, alpha=0.4, label='Phase 1 Sensitive', color='lightblue', density=True)\n",
    "ax6.hist(resistant_probs_p1, bins=30, alpha=0.4, label='Phase 1 Resistant', color='lightcoral', density=True)\n",
    "ax6.hist(sensitive_probs_p3, bins=30, alpha=0.4, label='Phase 3 Sensitive', color='lightgreen', density=True, histtype='step', linewidth=2)\n",
    "ax6.hist(resistant_probs_p3, bins=30, alpha=0.4, label='Phase 3 Resistant', color='darkgreen', density=True, histtype='step', linewidth=2)\n",
    "ax6.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "ax6.set_xlabel('Predicted Probability')\n",
    "ax6.set_ylabel('Density')\n",
    "ax6.set_title('Probability Distribution Comparison')\n",
    "ax6.legend(fontsize=8)\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_class_plot_path = output_dir / 'classification_pan_cancer_vs_tnbc_comparison.png'\n",
    "plt.savefig(comparison_class_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"\\nClassification comparison plot saved to: {comparison_class_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Create improvement summary figure\n",
    "fig2 = plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Improvement metrics\n",
    "ax_imp = plt.subplot(2, 2, 1)\n",
    "improvements = {\n",
    "    'Accuracy': metrics_p3['accuracy'] - metrics_p1['accuracy'],\n",
    "    'Precision': metrics_p3['precision'] - metrics_p1['precision'],\n",
    "    'Recall': metrics_p3['recall'] - metrics_p1['recall'],\n",
    "    'F1-Score': metrics_p3['f1'] - metrics_p1['f1'],\n",
    "    'ROC-AUC': metrics_p3['roc_auc'] - metrics_p1['roc_auc']\n",
    "}\n",
    "colors = ['green' if v > 0 else 'red' for v in improvements.values()]\n",
    "ax_imp.barh(list(improvements.keys()), list(improvements.values()), color=colors, alpha=0.7)\n",
    "ax_imp.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "ax_imp.set_xlabel('Improvement')\n",
    "ax_imp.set_title('Classification Performance Improvement\\nfrom Fine-tuning')\n",
    "ax_imp.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Percentage improvement\n",
    "ax_pct = plt.subplot(2, 2, 2)\n",
    "pct_improvements = {\n",
    "    'Accuracy': (metrics_p3['accuracy'] - metrics_p1['accuracy']) / metrics_p1['accuracy'] * 100 if metrics_p1['accuracy'] > 0 else 0,\n",
    "    'Precision': (metrics_p3['precision'] - metrics_p1['precision']) / metrics_p1['precision'] * 100 if metrics_p1['precision'] > 0 else 0,\n",
    "    'Recall': (metrics_p3['recall'] - metrics_p1['recall']) / metrics_p1['recall'] * 100 if metrics_p1['recall'] > 0 else 0,\n",
    "    'F1-Score': (metrics_p3['f1'] - metrics_p1['f1']) / metrics_p1['f1'] * 100 if metrics_p1['f1'] > 0 else 0,\n",
    "    'ROC-AUC': (metrics_p3['roc_auc'] - metrics_p1['roc_auc']) / metrics_p1['roc_auc'] * 100 if metrics_p1['roc_auc'] > 0 else 0\n",
    "}\n",
    "colors_pct = ['green' if v > 0 else 'red' for v in pct_improvements.values()]\n",
    "ax_pct.barh(list(pct_improvements.keys()), list(pct_improvements.values()), color=colors_pct, alpha=0.7)\n",
    "ax_pct.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "ax_pct.set_xlabel('Percentage Improvement (%)')\n",
    "ax_pct.set_title('Percentage Improvement from Fine-tuning')\n",
    "ax_pct.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Summary text\n",
    "ax_summary = plt.subplot(2, 2, (3, 4))\n",
    "ax_summary.axis('off')\n",
    "summary_text = f\"\"\"\n",
    "Classification Performance Improvement Summary:\n",
    "\n",
    "Absolute Improvements:\n",
    "  Accuracy:  {improvements['Accuracy']:+.4f} ({pct_improvements['Accuracy']:+.2f}%)\n",
    "  Precision: {improvements['Precision']:+.4f} ({pct_improvements['Precision']:+.2f}%)\n",
    "  Recall:    {improvements['Recall']:+.4f} ({pct_improvements['Recall']:+.2f}%)\n",
    "  F1-Score:  {improvements['F1-Score']:+.4f} ({pct_improvements['F1-Score']:+.2f}%)\n",
    "  ROC-AUC:   {improvements['ROC-AUC']:+.4f} ({pct_improvements['ROC-AUC']:+.2f}%)\n",
    "\n",
    "Test Samples: {len(y_true)}\n",
    "  Sensitive (IC50 ≤ median): {np.sum(y_true == 0)}\n",
    "  Resistant (IC50 > median): {np.sum(y_true == 1)}\n",
    "\n",
    "Phase 1 Performance:\n",
    "  Accuracy:  {metrics_p1['accuracy']:.4f}\n",
    "  Precision: {metrics_p1['precision']:.4f}\n",
    "  Recall:    {metrics_p1['recall']:.4f}\n",
    "  F1-Score:  {metrics_p1['f1']:.4f}\n",
    "  ROC-AUC:   {metrics_p1['roc_auc']:.4f}\n",
    "\n",
    "Phase 3 Performance:\n",
    "  Accuracy:  {metrics_p3['accuracy']:.4f}\n",
    "  Precision: {metrics_p3['precision']:.4f}\n",
    "  Recall:    {metrics_p3['recall']:.4f}\n",
    "  F1-Score:  {metrics_p3['f1']:.4f}\n",
    "  ROC-AUC:   {metrics_p3['roc_auc']:.4f}\n",
    "\"\"\"\n",
    "ax_summary.text(0.05, 0.5, summary_text, fontsize=11, family='monospace', verticalalignment='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "improvement_plot_path = output_dir / 'classification_improvement_summary.png'\n",
    "plt.savefig(improvement_plot_path, dpi=300, bbox_inches='tight')\n",
    "print(f\"Improvement summary plot saved to: {improvement_plot_path}\")\n",
    "plt.show()\n",
    "\n",
    "# Save comparison results\n",
    "comparison_class_csv_path = output_dir / f'classification_pan_cancer_vs_tnbc_comparison_{split_type}.csv'\n",
    "comparison_class_df.to_csv(comparison_class_csv_path, index=False)\n",
    "print(f\"Classification comparison results saved to CSV: {comparison_class_csv_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Classification Comparison Complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ee62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis: Pathway Importance per Drug\n",
    "# This analysis identifies which pathways drive predictions for each drug\n",
    "\n",
    "from collections import defaultdict\n",
    "xs = True\n",
    "\n",
    "if xs == True:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SHAP Analysis: Identifying Pathway Importance per Drug\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    def create_pathway_predictor(model, drug_graph, drug_name, pathway_cols, normalization_stats=None):\n",
    "        \"\"\"\n",
    "        Create a wrapper function for SHAP that fixes the drug and varies pathways.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained DrugResponsePathwayGNN model\n",
    "            drug_graph: Fixed drug graph (PyTorch Geometric Data object)\n",
    "            drug_name: Name of the drug (for reference)\n",
    "            pathway_cols: List of pathway column names\n",
    "            normalization_stats: Dict with 'means' and 'stds' for normalization (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Function that takes pathway scores array and returns IC50 predictions\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        drug_graph = drug_graph.to(device)\n",
    "        \n",
    "        # Create a batch with the fixed drug graph\n",
    "        drug_batch = Batch.from_data_list([drug_graph]).to(device)\n",
    "        \n",
    "        def pathway_predictor(pathway_scores_array):\n",
    "            \"\"\"\n",
    "            Predict IC50 given pathway scores.\n",
    "            \n",
    "            Args:\n",
    "                pathway_scores_array: numpy array of shape (n_samples, n_pathways)\n",
    "                \n",
    "            Returns:\n",
    "                numpy array of IC50 predictions (n_samples,)\n",
    "            \"\"\"\n",
    "            # Convert to tensor\n",
    "            if isinstance(pathway_scores_array, np.ndarray):\n",
    "                pathway_tensor = torch.FloatTensor(pathway_scores_array).to(device)\n",
    "            else:\n",
    "                pathway_tensor = pathway_scores_array.to(device)\n",
    "            \n",
    "            # Apply normalization if provided\n",
    "            if normalization_stats is not None:\n",
    "                means = torch.FloatTensor(normalization_stats['means']).to(device)\n",
    "                stds = torch.FloatTensor(normalization_stats['stds']).to(device)\n",
    "                pathway_tensor = (pathway_tensor - means) / stds\n",
    "            \n",
    "            # Expand drug batch to match batch size\n",
    "            batch_size = pathway_tensor.shape[0]\n",
    "            expanded_drug_batch = Batch.from_data_list([drug_graph] * batch_size).to(device)\n",
    "            \n",
    "            # Make predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = model(expanded_drug_batch, pathway_tensor)\n",
    "                predictions = outputs['ic50'].cpu().numpy().flatten()\n",
    "            \n",
    "            return predictions\n",
    "        \n",
    "        return pathway_predictor\n",
    "    \n",
    "    def compute_shap_for_drug(model, drug_name, drug_graph, test_data, pathway_cols, \n",
    "                              normalization_stats=None, n_samples=100, n_background=50):\n",
    "        \"\"\"\n",
    "        Compute SHAP values for a specific drug.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained model\n",
    "            drug_name: Name of the drug\n",
    "            drug_graph: Drug graph (PyTorch Geometric Data)\n",
    "            test_data: DataFrame with test samples (filtered for this drug)\n",
    "            pathway_cols: List of pathway column names\n",
    "            normalization_stats: Normalization statistics\n",
    "            n_samples: Number of samples to explain\n",
    "            n_background: Number of background samples for SHAP\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with SHAP values and pathway names\n",
    "        \"\"\"\n",
    "        if len(test_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Sample test data\n",
    "        if len(test_data) > n_samples:\n",
    "            test_sample = test_data.sample(n=min(n_samples, len(test_data)), random_state=42)\n",
    "        else:\n",
    "            test_sample = test_data.copy()\n",
    "        \n",
    "        # Get pathway scores\n",
    "        X_test = test_sample[pathway_cols].values.astype(np.float32)\n",
    "        \n",
    "        # Create background dataset (sample from training data)\n",
    "        # Use pan-cancer training data as background\n",
    "        if SPLIT_TYPE == 'cell_line':\n",
    "            background_data = pan_cancer_pathway.loc[pan_train_idx]\n",
    "        else:\n",
    "            background_data = pan_cancer_pathway.loc[pan_train_idx] if len(pan_train_idx) > 0 else pan_cancer_pathway\n",
    "        \n",
    "        # Filter to same drug if available, otherwise use all drugs\n",
    "        drug_background = background_data[background_data['DrugName'] == drug_name]\n",
    "        if len(drug_background) == 0:\n",
    "            drug_background = background_data.sample(n=min(n_background, len(background_data)), random_state=42)\n",
    "        else:\n",
    "            drug_background = drug_background.sample(n=min(n_background, len(drug_background)), random_state=42)\n",
    "        \n",
    "        X_background = drug_background[pathway_cols].values.astype(np.float32)\n",
    "        \n",
    "        # Create predictor function\n",
    "        predictor = create_pathway_predictor(model, drug_graph, drug_name, pathway_cols, normalization_stats)\n",
    "        \n",
    "        # Compute SHAP values using KernelExplainer\n",
    "        print(f\"Computing SHAP values for {drug_name} ({len(test_sample)} samples)...\")\n",
    "        explainer = shap.KernelExplainer(predictor, X_background)\n",
    "        shap_values = explainer.shap_values(X_test, nsamples=min(100, len(X_test)))\n",
    "        \n",
    "        # Get mean absolute SHAP values per pathway\n",
    "        mean_shap = np.abs(shap_values).mean(axis=0)\n",
    "        \n",
    "        # Create results dictionary\n",
    "        results = {\n",
    "            'drug_name': drug_name,\n",
    "            'shap_values': shap_values,\n",
    "            'mean_abs_shap': mean_shap,\n",
    "            'pathway_names': pathway_cols,\n",
    "            'test_samples': len(test_sample),\n",
    "            'background_samples': len(X_background)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    # Load the best model (Phase 3 TNBC model)\n",
    "    print(\"\\nLoading Phase 3 TNBC model for SHAP analysis...\")\n",
    "\n",
    "    # Define checkpoint paths - use cellsplit models for cell_line split\n",
    "    models_dir = output_dir / \"models\"\n",
    "    split_type = SPLIT_TYPE if 'SPLIT_TYPE' in globals() else 'cell_line'\n",
    "    \n",
    "    # For cell_line split, use cellsplit models (old naming convention)\n",
    "    if split_type == 'cell_line':\n",
    "        phase2_checkpoint = models_dir / \"trial2_phase2_breast_pathway_cellsplit.pt\"\n",
    "        phase3_checkpoint = models_dir / \"trial2_phase3_tnbc_pathway_cellsplit.pt\"\n",
    "    else:\n",
    "        phase2_checkpoint = models_dir / f\"trial2_phase2_breast_pathway_{split_type}.pt\"\n",
    "        phase3_checkpoint = models_dir / f\"trial2_phase3_tnbc_pathway_{split_type}.pt\"\n",
    "    \n",
    "    print(f\"Using split type: {split_type}\")\n",
    "    print(f\"Looking for Phase 3 checkpoint: {phase3_checkpoint}\")\n",
    "    \n",
    "    if not phase3_checkpoint.exists():\n",
    "        print(f\"Warning: Phase 3 checkpoint not found at {phase3_checkpoint}\")\n",
    "        print(\"Checking for alternative checkpoint names...\")\n",
    "        # Try cellsplit as fallback\n",
    "        fallback_checkpoint = models_dir / \"trial2_phase3_tnbc_pathway_cellsplit.pt\"\n",
    "        if fallback_checkpoint.exists():\n",
    "            print(f\"Found fallback checkpoint: {fallback_checkpoint}\")\n",
    "            phase3_checkpoint = fallback_checkpoint\n",
    "    \n",
    "    # Use actual pathway count (should be available from cell 12, or use len(pathway_cols))\n",
    "    shap_pathway_count = actual_pathway_count if 'actual_pathway_count' in globals() else len(pathway_cols)\n",
    "    model_shap = DrugResponsePathwayGNN(cell_input_dim=shap_pathway_count).to(device)\n",
    "    if phase3_checkpoint.exists():\n",
    "        checkpoint = torch.load(phase3_checkpoint, map_location=device, weights_only=False)\n",
    "        model_shap.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Model loaded successfully!\")\n",
    "    else:\n",
    "        print(\"Warning: Phase 3 checkpoint not found. Using Phase 2 model...\")\n",
    "        if phase2_checkpoint.exists():\n",
    "            checkpoint = torch.load(phase2_checkpoint, map_location=device, weights_only=False)\n",
    "            model_shap.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Compute normalization stats from training data\n",
    "    print(\"\\nComputing normalization statistics from training data...\")\n",
    "    if SPLIT_TYPE == 'cell_line':\n",
    "        train_data = pan_cancer_pathway.loc[pan_train_idx]\n",
    "    else:\n",
    "        train_data = pan_cancer_pathway.loc[pan_train_idx] if len(pan_train_idx) > 0 else pan_cancer_pathway\n",
    "    \n",
    "    normalization_stats = {\n",
    "        'means': train_data[pathway_cols].values.mean(axis=0).astype(np.float32),\n",
    "        'stds': train_data[pathway_cols].values.std(axis=0).astype(np.float32) + 1e-8\n",
    "    }\n",
    "    \n",
    "    # Get test data (use TNBC test data if available, otherwise breast cancer)\n",
    "    if phase3_results['n_samples'] > 0:\n",
    "        if SPLIT_TYPE == 'cell_line':\n",
    "            test_data_full = tnbc_pathway.loc[tnbc_test_idx] if len(tnbc_test_idx) > 0 else tnbc_pathway\n",
    "        else:\n",
    "            test_data_full = tnbc_pathway.loc[tnbc_test_idx] if len(tnbc_test_idx) > 0 else tnbc_pathway\n",
    "        print(f\"Using TNBC test data: {len(test_data_full)} samples\")\n",
    "    else:\n",
    "        if SPLIT_TYPE == 'cell_line':\n",
    "            test_data_full = breast_cancer_pathway.loc[breast_test_idx] if len(breast_test_idx) > 0 else breast_cancer_pathway\n",
    "        else:\n",
    "            test_data_full = breast_cancer_pathway.loc[breast_test_idx] if len(breast_test_idx) > 0 else breast_cancer_pathway\n",
    "        print(f\"Using Breast Cancer test data: {len(test_data_full)} samples\")\n",
    "    \n",
    "    # Get unique drugs in test set\n",
    "    unique_drugs = test_data_full['DrugName'].unique()\n",
    "    print(f\"\\nFound {len(unique_drugs)} unique drugs in test set\")\n",
    "    \n",
    "    # Define high-priority drugs for analysis\n",
    "    high_priority_drugs = [\n",
    "        'Paclitaxel',     # Microtubule - should show mitotic pathways\n",
    "        'Cisplatin',      # DNA crosslinker - should show DNA repair\n",
    "        'Olaparib',       # PARP inhibitor - should show DNA repair\n",
    "        'Trametinib',     # MEK inhibitor - should show MAPK/ERK\n",
    "        'Lapatinib',      # EGFR/HER2 - should show EGFR signaling\n",
    "    ]\n",
    "    \n",
    "    # Prioritize high-priority drugs, then analyze all remaining drugs\n",
    "    priority_drugs_in_set = [drug for drug in high_priority_drugs if drug in unique_drugs]\n",
    "    other_drugs = [drug for drug in unique_drugs if drug not in high_priority_drugs]\n",
    "    \n",
    "    # Combine: high-priority first, then all others\n",
    "    unique_drugs = priority_drugs_in_set + other_drugs\n",
    "    \n",
    "    if priority_drugs_in_set:\n",
    "        print(f\"High-priority drugs found in test set ({len(priority_drugs_in_set)}): {priority_drugs_in_set}\")\n",
    "    print(f\"Analyzing all {len(unique_drugs)} drugs in test set\")\n",
    "    \n",
    "    # Compute SHAP values for each drug\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Computing SHAP values for each drug...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    drug_shap_results = {}\n",
    "    \n",
    "    for drug_name in tqdm(unique_drugs, desc=\"Processing drugs\"):\n",
    "        # Filter test data for this drug\n",
    "        drug_test_data = test_data_full[test_data_full['DrugName'] == drug_name].copy()\n",
    "        \n",
    "        if len(drug_test_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get drug graph\n",
    "        if drug_name not in drug_graphs:\n",
    "            print(f\"Warning: Drug {drug_name} not found in drug_graphs, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        drug_graph_data = drug_graphs[drug_name]['graph_data']\n",
    "        \n",
    "        # Compute SHAP values\n",
    "        try:\n",
    "            shap_result = compute_shap_for_drug(\n",
    "                model_shap, drug_name, drug_graph_data, drug_test_data,\n",
    "                pathway_cols, normalization_stats, n_samples=50, n_background=50\n",
    "            )\n",
    "            \n",
    "            if shap_result is not None:\n",
    "                drug_shap_results[drug_name] = shap_result\n",
    "        except Exception as e:\n",
    "            print(f\"Error computing SHAP for {drug_name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nSuccessfully computed SHAP values for {len(drug_shap_results)} drugs\")\n",
    "    \n",
    "    # Analyze and visualize results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SHAP Analysis Results\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create summary: Top pathways per drug\n",
    "    print(\"\\nTop 10 Pathways per Drug (by mean |SHAP| value):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for drug_name, result in drug_shap_results.items():\n",
    "        mean_shap = result['mean_abs_shap']\n",
    "        pathway_names = result['pathway_names']\n",
    "        \n",
    "        # Get top pathways\n",
    "        top_indices = np.argsort(mean_shap)[::-1][:10]\n",
    "        top_pathways = [(pathway_names[i], mean_shap[i]) for i in top_indices]\n",
    "        \n",
    "        print(f\"\\n{drug_name}:\")\n",
    "        for pathway, importance in top_pathways:\n",
    "            print(f\"  {pathway}: {importance:.4f}\")\n",
    "    \n",
    "    # Aggregate: Which pathways are most important across all drugs?\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Aggregate Analysis: Most Important Pathways Across All Drugs\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Aggregate SHAP values across all drugs\n",
    "    pathway_importance_aggregate = defaultdict(list)\n",
    "    \n",
    "    for drug_name, result in drug_shap_results.items():\n",
    "        mean_shap = result['mean_abs_shap']\n",
    "        pathway_names = result['pathway_names']\n",
    "        \n",
    "        for pathway, importance in zip(pathway_names, mean_shap):\n",
    "            pathway_importance_aggregate[pathway].append(importance)\n",
    "    \n",
    "    # Compute mean importance per pathway\n",
    "    pathway_mean_importance = {\n",
    "        pathway: np.mean(importances) \n",
    "        for pathway, importances in pathway_importance_aggregate.items()\n",
    "    }\n",
    "    \n",
    "    # Sort by mean importance\n",
    "    sorted_pathways = sorted(pathway_mean_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(\"\\nTop 20 Pathways Overall (by mean |SHAP| across all drugs):\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, (pathway, importance) in enumerate(sorted_pathways[:20], 1):\n",
    "        n_drugs = len(pathway_importance_aggregate[pathway])\n",
    "        print(f\"{i:2d}. {pathway:50s} | Mean |SHAP|: {importance:.4f} | Appears in {n_drugs} drugs\")\n",
    "    \n",
    "    # Save results to file\n",
    "    shap_results_path = output_dir / \"shap_analysis_results.pkl\"\n",
    "    with open(shap_results_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'drug_shap_results': drug_shap_results,\n",
    "            'pathway_mean_importance': pathway_mean_importance,\n",
    "            'normalization_stats': normalization_stats\n",
    "        }, f)\n",
    "    \n",
    "    print(f\"\\nSHAP results saved to: {shap_results_path}\")\n",
    "    \n",
    "    # Create visualization: Heatmap of top pathways vs drugs\n",
    "    print(\"\\nCreating visualization...\")\n",
    "    \n",
    "    # Select top pathways and drugs for visualization\n",
    "    top_pathways_viz = [p[0] for p in sorted_pathways[:15]]\n",
    "    top_drugs_viz = list(drug_shap_results.keys())[:15]\n",
    "    \n",
    "    # Create heatmap data\n",
    "    heatmap_data = []\n",
    "    for drug_name in top_drugs_viz:\n",
    "        if drug_name in drug_shap_results:\n",
    "            result = drug_shap_results[drug_name]\n",
    "            pathway_names = result['pathway_names']\n",
    "            mean_shap = result['mean_abs_shap']\n",
    "            \n",
    "            row = []\n",
    "            for pathway in top_pathways_viz:\n",
    "                if pathway in pathway_names:\n",
    "                    idx = pathway_names.index(pathway)\n",
    "                    row.append(mean_shap[idx])\n",
    "                else:\n",
    "                    row.append(0.0)\n",
    "            heatmap_data.append(row)\n",
    "    \n",
    "    if len(heatmap_data) > 0:\n",
    "        heatmap_array = np.array(heatmap_data)\n",
    "        \n",
    "        # Create heatmap\n",
    "        plt.figure(figsize=(16, 10))\n",
    "        plt.imshow(heatmap_array, aspect='auto', cmap='Reds', interpolation='nearest')\n",
    "        plt.colorbar(label='Mean |SHAP| Value')\n",
    "        plt.xlabel('Pathway', fontsize=10)\n",
    "        plt.ylabel('Drug', fontsize=10)\n",
    "        plt.title('SHAP Analysis: Pathway Importance per Drug\\n(Top 15 Pathways × Top 15 Drugs)', fontsize=14, fontweight='bold')\n",
    "        plt.xticks(range(len(top_pathways_viz)), top_pathways_viz, rotation=45, ha='right', fontsize=8)\n",
    "        plt.yticks(range(len(top_drugs_viz)), top_drugs_viz, fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        heatmap_path = output_dir / \"shap_heatmap.png\"\n",
    "        plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Heatmap saved to: {heatmap_path}\")\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SHAP Analysis Complete!\")\n",
    "    print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
