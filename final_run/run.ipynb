{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a05d7303",
   "metadata": {},
   "source": [
    "# TNBC Drug Cytotoxicity Prediction Model\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This notebook implements a three-phase transfer learning pipeline for predicting drug cytotoxicity in Triple-Negative Breast Cancer (TNBC) cell lines. The model employs Graph Neural Networks (GNNs) to encode drug molecular structures and pathway-based features to represent cell line characteristics.\n",
    "\n",
    "**Methodology:**\n",
    "- Phase 1: Pan-cancer pre-training with 5-fold cross-validation\n",
    "- Phase 2: Breast cancer fine-tuning with 5-fold cross-validation\n",
    "- Phase 3: TNBC-specific fine-tuning (single split due to limited sample size)\n",
    "- Cell-line based data splitting to prevent data leakage\n",
    "- Evaluation metrics with confidence intervals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7285ceb",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "\n",
    "Import required libraries and configure paths for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7f5f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pubchempy as pcp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import TransformerConv, global_mean_pool, global_max_pool\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy import stats\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import shap\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 300,  # High resolution for publication\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.pad_inches': 0.1,\n",
    "    'font.size': 10,\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': ['Arial', 'Helvetica', 'DejaVu Sans'],\n",
    "    'axes.linewidth': 1.0,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.labelsize': 11,\n",
    "    'axes.titlesize': 12,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 9,\n",
    "    'legend.frameon': False,\n",
    "    'legend.loc': 'best',\n",
    "    'lines.linewidth': 1.5,\n",
    "    'lines.markersize': 4,\n",
    "    'patch.linewidth': 0.5,\n",
    "    'grid.alpha': 0.3,\n",
    "    'grid.linewidth': 0.5,\n",
    "    'xtick.major.width': 1.0,\n",
    "    'ytick.major.width': 1.0,\n",
    "    'xtick.minor.width': 0.5,\n",
    "    'ytick.minor.width': 0.5,\n",
    "})\n",
    "\n",
    "NATURE_COLORS = {\n",
    "    'black': '#000000',\n",
    "    'dark_gray': '#404040',\n",
    "    'medium_gray': '#808080',\n",
    "    'light_gray': '#C0C0C0',\n",
    "    'white': '#FFFFFF'\n",
    "}\n",
    "\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "\n",
    "# Use current directory as root\n",
    "project_root = Path.cwd()\n",
    "data_dir = project_root / \"data\" / \"raw\"\n",
    "output_dir = project_root / \"results\"\n",
    "models_dir = project_root / \"models\"\n",
    "splits_dir = project_root / \"data_splits\"\n",
    "prebatched_dir = project_root / \"prebatched_data\"\n",
    "\n",
    "for dir_path in [output_dir, models_dir, splits_dir, prebatched_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235218f",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "Load pathway scores, drug response data, and cell line metadata from GDSC2 dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathway_scores_raw = pd.read_csv(data_dir / \"cell_ge.csv\", index_col=0)\n",
    "gdsc2_df = pd.read_excel(data_dir / \"GDSC2 Fitted Dose Response Oct 27 2023.xlsx\")\n",
    "model_df = pd.read_csv(data_dir / \"DepMap Model Data.csv\")\n",
    "drug_smiles = pd.read_csv(data_dir / \"drugs_with_smiles.csv\")\n",
    "\n",
    "rmse_col = [col for col in gdsc2_df.columns if 'RMSE' in col.upper()][0]\n",
    "gdsc2_filtered = gdsc2_df[gdsc2_df[rmse_col] < 0.3].copy()\n",
    "\n",
    "drug_response = gdsc2_filtered[['DRUG_NAME', 'CELL_LINE_NAME', 'LN_IC50', 'COSMIC_ID']].copy()\n",
    "drug_response.columns = ['DrugName', 'CellLineName', 'LN_IC50', 'COSMICID']\n",
    "\n",
    "pathway_names = pathway_scores_raw.index.tolist()\n",
    "pathway_data = pathway_scores_raw.T.reset_index()\n",
    "pathway_data.columns = ['CellLineName'] + pathway_names\n",
    "\n",
    "cell_name_to_modelid = dict(zip(\n",
    "    model_df['StrippedCellLineName'].str.upper().str.replace('-', '').str.replace('_', ''),\n",
    "    model_df['ModelID']\n",
    "))\n",
    "cosmic_to_modelid = model_df.drop_duplicates(subset='COSMICID', keep='first').set_index('COSMICID')['ModelID'].to_dict()\n",
    "\n",
    "pathway_data['ModelID'] = pathway_data['CellLineName'].apply(\n",
    "    lambda x: cell_name_to_modelid.get(str(x).upper().replace('-', '').replace('_', ''), None)\n",
    ")\n",
    "pathway_data = pathway_data[pathway_data['ModelID'].notna()].copy()\n",
    "\n",
    "drug_response['ModelID'] = drug_response['COSMICID'].apply(lambda x: cosmic_to_modelid.get(x, None))\n",
    "unmapped = drug_response[drug_response['ModelID'].isna()]\n",
    "if len(unmapped) > 0:\n",
    "    drug_response.loc[drug_response['ModelID'].isna(), 'ModelID'] = drug_response.loc[drug_response['ModelID'].isna(), 'CellLineName'].apply(\n",
    "        lambda x: cell_name_to_modelid.get(str(x).upper().replace('-', '').replace('_', ''), None)\n",
    "    )\n",
    "drug_response = drug_response[drug_response['ModelID'].notna()].drop(columns=['COSMICID'])\n",
    "\n",
    "pan_cancer_pathway = drug_response.merge(\n",
    "    pathway_data[['ModelID'] + pathway_names],\n",
    "    on='ModelID',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "drug_smiles_renamed = drug_smiles.rename(columns={'DRUG_NAME': 'DrugName'})\n",
    "pan_cancer_pathway = pan_cancer_pathway.merge(\n",
    "    drug_smiles_renamed[['DrugName', 'SMILES']],\n",
    "    on='DrugName',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "pan_cancer_pathway = pan_cancer_pathway[pan_cancer_pathway['LN_IC50'].notna()].copy()\n",
    "\n",
    "pan_cancer_pathway = pan_cancer_pathway.merge(\n",
    "    model_df[['ModelID', 'StrippedCellLineName', 'OncotreeLineage', 'OncotreePrimaryDisease']],\n",
    "    on='ModelID',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "drug_name_col = 'DrugName'\n",
    "ln_ic50_col = 'LN_IC50'\n",
    "pathway_cols = pathway_names\n",
    "drugs_with_smiles = drug_smiles_renamed[['DrugName', 'SMILES']].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8391c0",
   "metadata": {},
   "source": [
    "### 2.1 Drug Molecular Graph Construction\n",
    "\n",
    "Convert SMILES strings to molecular graphs using RDKit. Each drug is represented as a graph where nodes are atoms and edges are bonds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMILES to Graph Conversion\n",
    "\n",
    "def get_atom_features(atom):\n",
    "    \"\"\"\n",
    "    Extract atom features for GNN.\n",
    "    \n",
    "    Args:\n",
    "        atom: RDKit atom object\n",
    "        \n",
    "    Returns:\n",
    "        List of atom features\n",
    "    \"\"\"\n",
    "    features = [\n",
    "        atom.GetAtomicNum(),\n",
    "        atom.GetDegree(),\n",
    "        atom.GetFormalCharge(),\n",
    "        atom.GetHybridization().real,\n",
    "        atom.GetIsAromatic(),\n",
    "        atom.GetTotalNumHs(),\n",
    "        atom.GetNumRadicalElectrons(),\n",
    "        atom.IsInRing(),\n",
    "        atom.GetChiralTag().real,\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def get_bond_features(bond):\n",
    "    \"\"\"\n",
    "    Extract bond features for GNN.\n",
    "    \n",
    "    Args:\n",
    "        bond: RDKit bond object\n",
    "        \n",
    "    Returns:\n",
    "        List of bond features\n",
    "    \"\"\"\n",
    "    features = [\n",
    "        bond.GetBondTypeAsDouble(),\n",
    "        bond.GetIsConjugated(),\n",
    "        bond.IsInRing(),\n",
    "        bond.GetStereo().real,\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "def smiles_to_graph(smiles_string):\n",
    "    \"\"\"\n",
    "    Convert SMILES string to PyTorch Geometric graph.\n",
    "    \n",
    "    Args:\n",
    "        smiles_string: SMILES representation of molecule\n",
    "        \n",
    "    Returns:\n",
    "        torch_geometric.data.Data object or None if invalid\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smiles_string)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        \n",
    "        mol = Chem.AddHs(mol)\n",
    "        \n",
    "        # Extract node features\n",
    "        node_features = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            node_features.append(get_atom_features(atom))\n",
    "        \n",
    "        if len(node_features) == 0:\n",
    "            return None\n",
    "            \n",
    "        x = torch.tensor(node_features, dtype=torch.float)\n",
    "        \n",
    "        # Extract edge indices and features\n",
    "        edge_indices = []\n",
    "        edge_features = []\n",
    "        \n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            \n",
    "            edge_indices.append([i, j])\n",
    "            edge_indices.append([j, i])\n",
    "            \n",
    "            bond_feats = get_bond_features(bond)\n",
    "            edge_features.append(bond_feats)\n",
    "            edge_features.append(bond_feats)\n",
    "        \n",
    "        if len(edge_indices) == 0:\n",
    "            edge_indices = [[0, 0]]\n",
    "            edge_features = [[0.0] * 4]\n",
    "        \n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
    "        \n",
    "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        \n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess and Cache Drug Graphs\n",
    "\n",
    "def preprocess_drugs(drugs_df, drug_name_col, smiles_col, save_path):\n",
    "    \"\"\"\n",
    "    Convert all drugs to graphs and cache.\n",
    "    \n",
    "    Args:\n",
    "        drugs_df: DataFrame with drug names and SMILES\n",
    "        drug_name_col: Column name for drug names\n",
    "        smiles_col: Column name for SMILES strings\n",
    "        save_path: Path to save cached graphs\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping drug names to graph data\n",
    "    \"\"\"\n",
    "    drug_graphs = {}\n",
    "    failed_drugs = []\n",
    "    \n",
    "    for idx, row in tqdm(drugs_df.iterrows(), total=len(drugs_df), desc=\"Converting SMILES\"):\n",
    "        drug_name = row[drug_name_col]\n",
    "        smiles = row[smiles_col]\n",
    "        \n",
    "        graph = smiles_to_graph(smiles)\n",
    "        \n",
    "        if graph is not None:\n",
    "            drug_graphs[drug_name] = {\n",
    "                'drug_name': drug_name,\n",
    "                'smiles': smiles,\n",
    "                'graph_data': graph,\n",
    "                'node_dim': graph.x.shape[1],\n",
    "                'edge_dim': graph.edge_attr.shape[1] if graph.edge_attr is not None else 0,\n",
    "                'num_atoms': graph.x.shape[0],\n",
    "                'num_bonds': graph.edge_index.shape[1]\n",
    "            }\n",
    "        else:\n",
    "            failed_drugs.append(drug_name)\n",
    "    \n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(drug_graphs, f)\n",
    "    \n",
    "    \n",
    "    return drug_graphs\n",
    "\n",
    "drug_graphs_path = project_root / \"data\" / \"processed\" / \"drug_graphs.pkl\"\n",
    "drug_graphs = preprocess_drugs(drugs_with_smiles, drugs_with_smiles.columns[0], 'SMILES', drug_graphs_path)\n",
    "\n",
    "# Filter pan_cancer_pathway to only include drugs that successfully converted to graphs\n",
    "valid_drugs = set(drug_graphs.keys())\n",
    "before_count = len(pan_cancer_pathway)\n",
    "before_drugs = set(pan_cancer_pathway['DrugName'].unique())\n",
    "pan_cancer_pathway = pan_cancer_pathway[pan_cancer_pathway['DrugName'].isin(valid_drugs)].copy()\n",
    "after_count = len(pan_cancer_pathway)\n",
    "removed_drugs = before_drugs - valid_drugs\n",
    "if len(removed_drugs) > 0:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21512be1",
   "metadata": {},
   "source": [
    "### 2.2 Dataset Creation\n",
    "\n",
    "Create pan-cancer, breast cancer, and TNBC-specific datasets. TNBC dataset excludes HER2+ and ER+ cell lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a8265",
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_pathway = pan_cancer_pathway[\n",
    "    pan_cancer_pathway['OncotreeLineage'] == 'Breast'\n",
    "].copy()\n",
    "\n",
    "her2_positive = ['SKBR3', 'HCC1419', 'HCC1954', 'HCC1569', 'AU565', 'JIMT1', 'BT474', \n",
    "                 'MDA-MB-453', 'UACC812', 'ZR7530', 'HCC2218', 'MDA-MB-361', 'EFM19']\n",
    "er_positive = ['MCF7', 'T47D', 'ZR751', 'BT483', 'CAMA1', 'HCC1428', 'MDA-MB-415', \n",
    "               'MDA-MB-175VII', 'MDA-MB-134VI']\n",
    "\n",
    "exclude_cells = set()\n",
    "for cell_name in her2_positive + er_positive:\n",
    "    matching = breast_cancer_pathway[\n",
    "        breast_cancer_pathway['StrippedCellLineName'].str.upper().str.replace('-', '').str.replace('_', '') == \n",
    "        cell_name.upper().replace('-', '').replace('_', '')\n",
    "    ]['ModelID'].unique()\n",
    "    exclude_cells.update(matching)\n",
    "\n",
    "tnbc_pathway = breast_cancer_pathway[~breast_cancer_pathway['ModelID'].isin(exclude_cells)].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899498e8",
   "metadata": {},
   "source": [
    "### 2.3 Data Splitting Strategy\n",
    "\n",
    "**Cell-line based splitting:** Ensures no data leakage by splitting at the cell-line level rather than sample level. This prevents the same cell line from appearing in multiple splits.\n",
    "\n",
    "Split ratios: 80% train, 10% validation, 10% test (by cell lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0fad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 1.4: Data Splitting Configuration\n",
    "# Set split_type to 'cell_line' for cell-line based split (no data leakage)\n",
    "# Set split_type to 'random' for random split (standard train/test split)\n",
    "SPLIT_TYPE = 'cell_line'  # Options: 'cell_line' or 'random'\n",
    "\n",
    "def cell_line_split(dataframe, train_ratio=0.8, val_ratio=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    Split by cell lines (GPDRP method).\n",
    "    No cell line appears in multiple splits.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: DataFrame with ModelID column\n",
    "        train_ratio: Proportion of cell lines for training\n",
    "        val_ratio: Proportion of cell lines for validation\n",
    "        random_seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        train_idx, val_idx, test_idx: Index arrays for each split\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    cell_lines = dataframe['ModelID'].unique()\n",
    "    n_cells = len(cell_lines)\n",
    "    \n",
    "    # Shuffle cell lines\n",
    "    shuffled_cells = np.random.permutation(cell_lines)\n",
    "    \n",
    "    # Split cell lines\n",
    "    n_train = int(n_cells * train_ratio)\n",
    "    n_val = int(n_cells * val_ratio)\n",
    "    \n",
    "    train_cells = set(shuffled_cells[:n_train])\n",
    "    val_cells = set(shuffled_cells[n_train:n_train + n_val])\n",
    "    test_cells = set(shuffled_cells[n_train + n_val:])\n",
    "    \n",
    "    # Assign pairs based on cell line\n",
    "    train_idx = dataframe[dataframe['ModelID'].isin(train_cells)].index.values\n",
    "    val_idx = dataframe[dataframe['ModelID'].isin(val_cells)].index.values\n",
    "    test_idx = dataframe[dataframe['ModelID'].isin(test_cells)].index.values\n",
    "    \n",
    "    # Verify no overlap\n",
    "    train_cell_set = set(dataframe.loc[train_idx, 'ModelID'].unique())\n",
    "    val_cell_set = set(dataframe.loc[val_idx, 'ModelID'].unique())\n",
    "    test_cell_set = set(dataframe.loc[test_idx, 'ModelID'].unique())\n",
    "    \n",
    "    assert len(train_cell_set & val_cell_set) == 0, \"Cell line overlap between train and val!\"\n",
    "    assert len(train_cell_set & test_cell_set) == 0, \"Cell line overlap between train and test!\"\n",
    "    assert len(val_cell_set & test_cell_set) == 0, \"Cell line overlap between val and test!\"\n",
    "    \n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "def random_split(dataframe, train_ratio=0.8, val_ratio=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    Random split of samples (standard train/test split).\n",
    "    Note: This allows data leakage as the same cell line can appear in multiple splits.\n",
    "    \n",
    "    Args:\n",
    "        dataframe: DataFrame with samples\n",
    "        train_ratio: Proportion of samples for training\n",
    "        val_ratio: Proportion of samples for validation\n",
    "        random_seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        train_idx, val_idx, test_idx: Index arrays for each split\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    indices = np.arange(len(dataframe))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    n_train = int(len(indices) * train_ratio)\n",
    "    n_val = int(len(indices) * val_ratio)\n",
    "    \n",
    "    train_idx = dataframe.index[indices[:n_train]].values\n",
    "    val_idx = dataframe.index[indices[n_train:n_train + n_val]].values\n",
    "    test_idx = dataframe.index[indices[n_train + n_val:]].values\n",
    "    \n",
    "    return train_idx, val_idx, test_idx\n",
    "\n",
    "if SPLIT_TYPE == 'cell_line':\n",
    "    pan_train_idx, pan_val_idx, pan_test_idx = cell_line_split(pan_cancer_pathway, random_seed=42)\n",
    "elif SPLIT_TYPE == 'random':\n",
    "    pan_train_idx, pan_val_idx, pan_test_idx = random_split(pan_cancer_pathway, random_seed=42)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown SPLIT_TYPE: {SPLIT_TYPE}. Must be 'cell_line' or 'random'\")\n",
    "\n",
    "# Extract cell lines for each global split (for cell-line split)\n",
    "if SPLIT_TYPE == 'cell_line':\n",
    "    pan_train_cells = set(pan_cancer_pathway.loc[pan_train_idx, 'ModelID'].unique())\n",
    "    pan_val_cells = set(pan_cancer_pathway.loc[pan_val_idx, 'ModelID'].unique())\n",
    "    pan_test_cells = set(pan_cancer_pathway.loc[pan_test_idx, 'ModelID'].unique())\n",
    "    \n",
    "    # Filter breast cancer data to only include cell lines from appropriate global split\n",
    "    breast_train_data = breast_cancer_pathway[breast_cancer_pathway['ModelID'].isin(pan_train_cells)].copy()\n",
    "    breast_val_data = breast_cancer_pathway[breast_cancer_pathway['ModelID'].isin(pan_val_cells)].copy()\n",
    "    breast_test_data = breast_cancer_pathway[breast_cancer_pathway['ModelID'].isin(pan_test_cells)].copy()\n",
    "    \n",
    "    # Filter TNBC data similarly\n",
    "    tnbc_train_data = tnbc_pathway[tnbc_pathway['ModelID'].isin(pan_train_cells)].copy()\n",
    "    tnbc_val_data = tnbc_pathway[tnbc_pathway['ModelID'].isin(pan_val_cells)].copy()\n",
    "    tnbc_test_data = tnbc_pathway[tnbc_pathway['ModelID'].isin(pan_test_cells)].copy()\n",
    "    \n",
    "    breast_train_idx = breast_train_data.index.values\n",
    "    breast_val_idx = breast_val_data.index.values\n",
    "    breast_test_idx = breast_test_data.index.values\n",
    "    \n",
    "    tnbc_train_idx = tnbc_train_data.index.values\n",
    "    tnbc_val_idx = tnbc_val_data.index.values\n",
    "    tnbc_test_idx = tnbc_test_data.index.values\n",
    "    \n",
    "    # Verify no data leakage: check that cell lines don't appear in conflicting splits\n",
    "    breast_train_cells = set(breast_train_data['ModelID'].unique())\n",
    "    breast_val_cells = set(breast_val_data['ModelID'].unique())\n",
    "    breast_test_cells = set(breast_test_data['ModelID'].unique())\n",
    "    \n",
    "    tnbc_train_cells = set(tnbc_train_data['ModelID'].unique())\n",
    "    tnbc_val_cells = set(tnbc_val_data['ModelID'].unique())\n",
    "    tnbc_test_cells = set(tnbc_test_data['ModelID'].unique())\n",
    "    \n",
    "    # Verify: breast/TNBC train cells should only be in pan train, not pan val/test\n",
    "    assert len(breast_train_cells & pan_val_cells) == 0, \"Data leakage: breast train cells in pan val!\"\n",
    "    assert len(breast_train_cells & pan_test_cells) == 0, \"Data leakage: breast train cells in pan test!\"\n",
    "    assert len(tnbc_train_cells & pan_val_cells) == 0, \"Data leakage: TNBC train cells in pan val!\"\n",
    "    assert len(tnbc_train_cells & pan_test_cells) == 0, \"Data leakage: TNBC train cells in pan test!\"\n",
    "\n",
    "    # Phase 2 (Breast) test vs Phase 1 (Pan) train/val\n",
    "    assert len(breast_test_cells & pan_train_cells) == 0, \"Data leakage: breast test cells in pan train!\"\n",
    "    assert len(breast_test_cells & pan_val_cells) == 0, \"Data leakage: breast test cells in pan val!\"\n",
    "    assert len(breast_val_cells & pan_test_cells) == 0, \"Data leakage: breast val cells in pan test!\"\n",
    "\n",
    "    # Phase 3 (TNBC) test vs Phase 1 (Pan) train/val\n",
    "    assert len(tnbc_test_cells & pan_train_cells) == 0, \"Data leakage: TNBC test cells in pan train!\"\n",
    "    assert len(tnbc_test_cells & pan_val_cells) == 0, \"Data leakage: TNBC test cells in pan val!\"\n",
    "    assert len(tnbc_val_cells & pan_test_cells) == 0, \"Data leakage: TNBC val cells in pan test!\"\n",
    "\n",
    "    # Phase 3 (TNBC) test vs Phase 2 (Breast) train/val\n",
    "    assert len(tnbc_test_cells & breast_train_cells) == 0, \"Data leakage: TNBC test cells in breast train!\"\n",
    "    assert len(tnbc_test_cells & breast_val_cells) == 0, \"Data leakage: TNBC test cells in breast val!\"\n",
    "\n",
    "    # Phase 2 (Breast) test vs Phase 2 train/val (internal independence)\n",
    "    assert len(breast_test_cells & breast_train_cells) == 0, \"Data leakage: breast test cells in breast train!\"\n",
    "    assert len(breast_test_cells & breast_val_cells) == 0, \"Data leakage: breast test cells in breast val!\"\n",
    "\n",
    "    # Phase 3 (TNBC) test vs Phase 3 train/val (internal independence)\n",
    "    assert len(tnbc_test_cells & tnbc_train_cells) == 0, \"Data leakage: TNBC test cells in TNBC train!\"\n",
    "    assert len(tnbc_test_cells & tnbc_val_cells) == 0, \"Data leakage: TNBC test cells in TNBC val!\"\n",
    "\n",
    "    assert len(tnbc_test_cells & breast_test_cells) == len(tnbc_test_cells), \"TNBC test cells must be subset of breast test cells\"\n",
    "\n",
    "    # Verify test sets are disjoint from each other (should be same cells, but check)\n",
    "    assert len(pan_test_cells & breast_test_cells) == len(breast_test_cells), \"Breast test cells must be subset of pan test cells\"\n",
    "    assert len(pan_test_cells & tnbc_test_cells) == len(tnbc_test_cells), \"TNBC test cells must be subset of pan test cells\"\n",
    "\n",
    "    \n",
    "elif SPLIT_TYPE == 'random':\n",
    "    # For random split, directly split breast cancer and TNBC data\n",
    "    breast_train_idx, breast_val_idx, breast_test_idx = random_split(breast_cancer_pathway, random_seed=42)\n",
    "    tnbc_train_idx, tnbc_val_idx, tnbc_test_idx = random_split(tnbc_pathway, random_seed=42)\n",
    "    \n",
    "\n",
    "# Save split indices to ensure reproducibility during evaluation\n",
    "# splits_dir already defined in Cell 1\n",
    "splits_file = splits_dir / f\"data_splits_{SPLIT_TYPE}.json\"\n",
    "splits_data = {\n",
    "    'split_type': SPLIT_TYPE,\n",
    "    'pan_cancer': {\n",
    "        'train': [int(x) for x in pan_train_idx],\n",
    "        'val': [int(x) for x in pan_val_idx],\n",
    "        'test': [int(x) for x in pan_test_idx]\n",
    "    },\n",
    "    'breast_cancer': {\n",
    "        'train': [int(x) for x in breast_train_idx],\n",
    "        'val': [int(x) for x in breast_val_idx],\n",
    "        'test': [int(x) for x in breast_test_idx]\n",
    "    },\n",
    "    'tnbc': {\n",
    "        'train': [int(x) for x in tnbc_train_idx],\n",
    "        'val': [int(x) for x in tnbc_val_idx],\n",
    "        'test': [int(x) for x in tnbc_test_idx]\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(splits_file, 'w') as f:\n",
    "    json.dump(splits_data, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a58b19",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "\n",
    "### 3.1 Dataset Class\n",
    "\n",
    "Custom PyTorch Dataset class that handles pathway-based features with per-sample z-score normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DrugResponsePathwayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for drug response prediction using GSVA pathway scores.\n",
    "    Performs per-sample z-score normalization of pathway scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, drug_graphs_dict, drug_col='DRUG_NAME', pathway_cols=None):\n",
    "        \"\"\"\n",
    "        Initialize pathway-based dataset.\n",
    "        \n",
    "        Args:\n",
    "            dataframe: DataFrame with pathway scores, ModelID, DRUG_NAME, LN_IC50\n",
    "            drug_graphs_dict: Dictionary mapping drug names to graph data\n",
    "            drug_col: Column name for drug names\n",
    "            pathway_cols: List of pathway column names (if None, will infer)\n",
    "        \"\"\"\n",
    "        self.original_data = dataframe.copy()\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.drug_graphs = drug_graphs_dict\n",
    "        self.drug_col = drug_col\n",
    "        \n",
    "        # Identify pathway columns\n",
    "        if pathway_cols is not None:\n",
    "            self.pathway_cols = [c for c in pathway_cols if c in dataframe.columns]\n",
    "        else:\n",
    "            # Infer pathway columns: exclude metadata columns\n",
    "            exclude_cols = ['ModelID', 'COSMICID', 'StrippedCellLineName', 'OncotreeLineage', \n",
    "                           'OncotreePrimaryDisease', drug_col, 'SMILES', 'LN_IC50', 'CellLineName']\n",
    "            # Only include columns that are numeric and not in exclude list\n",
    "            numeric_cols = dataframe.select_dtypes(include=[np.number]).columns.tolist()\n",
    "            self.pathway_cols = [c for c in numeric_cols if c not in exclude_cols]\n",
    "        \n",
    "        # Verify pathway columns are numeric\n",
    "        for col in self.pathway_cols:\n",
    "            if not pd.api.types.is_numeric_dtype(dataframe[col]):\n",
    "                raise ValueError(f\"Pathway column '{col}' is not numeric\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        drug_name = row[self.drug_col]\n",
    "        \n",
    "        if drug_name not in self.drug_graphs:\n",
    "            raise ValueError(f\"Drug {drug_name} not found in drug_graphs\")\n",
    "        drug_graph = self.drug_graphs[drug_name]['graph_data']\n",
    "        \n",
    "        pathway_scores = row[self.pathway_cols].values.astype(np.float32)\n",
    "        \n",
    "        # Per-sample z-score normalization\n",
    "        mean = pathway_scores.mean()\n",
    "        std = pathway_scores.std()\n",
    "        if std > 1e-8:\n",
    "            pathway_scores = (pathway_scores - mean) / std\n",
    "        else:\n",
    "            pathway_scores = np.zeros_like(pathway_scores)\n",
    "        \n",
    "        pathway_tensor = torch.tensor(pathway_scores, dtype=torch.float32)\n",
    "        ic50 = torch.tensor([row['LN_IC50']], dtype=torch.float32)\n",
    "        \n",
    "        return {\n",
    "            'drug_graph': drug_graph,\n",
    "            'cell_expr': pathway_tensor,  # Keep same key name for compatibility\n",
    "            'ic50': ic50,\n",
    "            'drug_name': drug_name,\n",
    "            'cell_id': row['ModelID']\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdba532",
   "metadata": {},
   "source": [
    "### 3.2 Drug Encoder\n",
    "\n",
    "Graph Neural Network encoder using TransformerConv layers to encode molecular structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd686297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drug Encoder Architecture\n",
    "\n",
    "class DrugEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Neural Network encoder for molecular SMILES structures.\n",
    "    Uses TransformerConv layers to generate 256-dim drug embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, node_feature_dim=9, edge_feature_dim=4, hidden_dim=256, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Initialize DrugEncoder.\n",
    "        \n",
    "        Args:\n",
    "            node_feature_dim: Number of atom features\n",
    "            edge_feature_dim: Number of bond features\n",
    "            hidden_dim: Hidden layer dimension (output is 256)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(DrugEncoder, self).__init__()\n",
    "        \n",
    "        # Device selection\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device('mps')\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        \n",
    "        # TransformerConv layers\n",
    "        self.conv1 = TransformerConv(node_feature_dim, 128, heads=4, dropout=dropout, edge_dim=edge_feature_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(128 * 4)\n",
    "        \n",
    "        self.conv2 = TransformerConv(128 * 4, 256, heads=8, dropout=dropout, edge_dim=edge_feature_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(256 * 8)\n",
    "        \n",
    "        self.conv3 = TransformerConv(256 * 8, 256, heads=8, dropout=dropout, edge_dim=edge_feature_dim)\n",
    "        self.bn3 = nn.BatchNorm1d(256 * 8)\n",
    "        \n",
    "        self.conv4 = TransformerConv(256 * 8, 256, heads=4, concat=False, dropout=dropout, edge_dim=edge_feature_dim)\n",
    "        self.bn4 = nn.BatchNorm1d(256)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Attention weights for pooling\n",
    "        self.attention_weights = nn.Linear(256, 1)\n",
    "        \n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        \"\"\"\n",
    "        Forward pass through drug encoder.\n",
    "        \n",
    "        Args:\n",
    "            data: PyTorch Geometric Data/Batch object\n",
    "            \n",
    "        Returns:\n",
    "            Drug embeddings of shape (batch_size, 256)\n",
    "        \"\"\"\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "        \n",
    "        x = x.to(self.device, non_blocking=True)\n",
    "        edge_index = edge_index.to(self.device, non_blocking=True)\n",
    "        edge_attr = edge_attr.to(self.device, non_blocking=True)\n",
    "        batch = batch.to(self.device, non_blocking=True)\n",
    "        \n",
    "        # Layer 1\n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Layer 4\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        # Attention-weighted pooling\n",
    "        attention_scores = self.attention_weights(x)\n",
    "        attention_scores = torch.softmax(attention_scores, dim=0)\n",
    "        \n",
    "        x_weighted = x * attention_scores\n",
    "        x_mean = global_mean_pool(x_weighted, batch)\n",
    "        x_max = global_max_pool(x, batch)\n",
    "        \n",
    "        embedding = (x_mean + x_max) / 2\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "# Cell Encoder Architecture\n",
    "\n",
    "class CellEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Feedforward neural network to encode cell line pathway scores.\n",
    "    Uses skip connection to preserve direct pathway signal.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=1329, hidden_dim=512, output_dim=256, dropout1=0.4, dropout2=0.3):\n",
    "        \"\"\"\n",
    "        Initialize CellEncoder.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Number of pathway features\n",
    "            hidden_dim: First hidden layer dimension\n",
    "            output_dim: Output embedding dimension\n",
    "            dropout1: Dropout rate for first layer\n",
    "            dropout2: Dropout rate for second layer\n",
    "        \"\"\"\n",
    "        super(CellEncoder, self).__init__()\n",
    "        \n",
    "        # Device selection\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device('mps')\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        \n",
    "        # Main pathway\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(output_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        \n",
    "        self.skip = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through cell encoder.\n",
    "        \n",
    "        Args:\n",
    "            x: Pathway scores tensor of shape (batch_size, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "            Cell embeddings of shape (batch_size, 256)\n",
    "        \"\"\"\n",
    "        x = x.to(self.device, non_blocking=True)\n",
    "        \n",
    "        # Main pathway\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout1(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout2(out)\n",
    "        \n",
    "        skip_out = self.skip(x)\n",
    "        \n",
    "        # Residual connection\n",
    "        embedding = out + skip_out\n",
    "        \n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0fe433",
   "metadata": {},
   "source": [
    "### 3.3 Full Model Architecture\n",
    "\n",
    "**DrugResponsePathwayGNN** combines:\n",
    "- Drug encoder: GNN for molecular structure\n",
    "- Cell encoder: MLP for pathway features\n",
    "- Multi-task learning: IC50 regression + binary classification + reconstruction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6699b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 4: DrugResponsePathwayGNN Model\n",
    "\n",
    "class DrugResponsePathwayGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    GNN model for drug response prediction using pathway activity scores.\n",
    "    Uses 1,329 pathway features instead of raw gene expression.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, drug_node_dim=9, drug_edge_dim=4, cell_input_dim=1329, \n",
    "                 hidden_dim=256, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Initialize DrugResponsePathwayGNN.\n",
    "        \n",
    "        Args:\n",
    "            drug_node_dim: Number of atom features (9)\n",
    "            drug_edge_dim: Number of bond features (4)\n",
    "            cell_input_dim: Number of pathway features (1329)\n",
    "            hidden_dim: Embedding dimension (256)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(DrugResponsePathwayGNN, self).__init__()\n",
    "        \n",
    "        # Device selection\n",
    "        if torch.backends.mps.is_available():\n",
    "            self.device = torch.device('mps')\n",
    "        elif torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        \n",
    "        # Drug encoder (same as before)\n",
    "        self.drug_encoder = DrugEncoder(\n",
    "            node_feature_dim=drug_node_dim,\n",
    "            edge_feature_dim=drug_edge_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Cell encoder for pathway scores (input_dim=1329)\n",
    "        self.cell_encoder = CellEncoder(\n",
    "            input_dim=cell_input_dim,\n",
    "            hidden_dim=512,\n",
    "            output_dim=hidden_dim,\n",
    "            dropout1=0.4,\n",
    "            dropout2=0.3\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for integration\n",
    "        self.attention_query = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.attention_key = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.attention_value = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Combined embedding dimension\n",
    "        combined_dim = hidden_dim * 2  # 512\n",
    "        \n",
    "        # Head A: IC50 Regression (primary task)\n",
    "        self.ic50_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        # Head B: Sensitivity Classification (auxiliary task)\n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        \n",
    "        # Head C: Similarity Reconstruction (auxiliary task)\n",
    "        self.reconstruction_head = nn.Sequential(\n",
    "            nn.Linear(combined_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, combined_dim)\n",
    "        )\n",
    "        \n",
    "        self.to(self.device)\n",
    "    \n",
    "    def integrate_with_attention(self, drug_emb, cell_emb):\n",
    "        \"\"\"\n",
    "        Integrate drug and cell embeddings using attention mechanism.\n",
    "        \n",
    "        Args:\n",
    "            drug_emb: Drug embeddings (batch_size, 256)\n",
    "            cell_emb: Cell embeddings (batch_size, 256)\n",
    "            \n",
    "        Returns:\n",
    "            Combined embedding (batch_size, 512)\n",
    "        \"\"\"\n",
    "        # Drug embedding as query\n",
    "        query = self.attention_query(drug_emb)  # (batch, 256)\n",
    "        key = self.attention_key(cell_emb)      # (batch, 256)\n",
    "        value = self.attention_value(cell_emb)  # (batch, 256)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(query.unsqueeze(1), key.unsqueeze(2))  # (batch, 1, 1)\n",
    "        attention_scores = attention_scores / (drug_emb.size(-1) ** 0.5)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        attended_cell = attention_weights.squeeze(-1) * value  # (batch, 1) * (batch, 256) -> (batch, 256)\n",
    "        \n",
    "        # Concatenate drug and attended cell embeddings\n",
    "        combined = torch.cat([drug_emb, attended_cell], dim=1)  # (batch, 512)\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    def forward(self, drug_batch, cell_batch, return_embeddings=False):\n",
    "        \"\"\"\n",
    "        Forward pass through complete model.\n",
    "        \n",
    "        Args:\n",
    "            drug_batch: PyTorch Geometric batch of molecular graphs\n",
    "            cell_batch: Pathway scores tensor (batch_size, 1329)\n",
    "            return_embeddings: Whether to return intermediate embeddings\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with predictions and optionally embeddings\n",
    "        \"\"\"\n",
    "        # Encode drug and cell\n",
    "        drug_emb = self.drug_encoder(drug_batch)  # (batch_size, 256)\n",
    "        cell_emb = self.cell_encoder(cell_batch)   # (batch_size, 256)\n",
    "        \n",
    "        # Integrate with attention\n",
    "        combined = self.integrate_with_attention(drug_emb, cell_emb)  # (batch_size, 512)\n",
    "        \n",
    "        # Multi-task predictions\n",
    "        ic50_pred = self.ic50_head(combined)\n",
    "        class_pred = self.classification_head(combined)\n",
    "        recon_pred = self.reconstruction_head(combined)\n",
    "        \n",
    "        results = {\n",
    "            'ic50': ic50_pred,\n",
    "            'classification': class_pred,\n",
    "            'reconstruction': recon_pred\n",
    "        }\n",
    "        \n",
    "        if return_embeddings:\n",
    "            results['embeddings'] = {\n",
    "                'drug': drug_emb,\n",
    "                'cell': cell_emb,\n",
    "                'combined': combined\n",
    "            }\n",
    "        \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15243e22",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation Framework\n",
    "\n",
    "### 4.1 Cell-Line Based K-Fold Splitting\n",
    "\n",
    "Implements k-fold cross-validation at the cell-line level to maintain data leakage prevention while enabling robust performance estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b90fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 5.5: Cross-Validation Support\n",
    "\n",
    "def cell_line_kfold_split(dataframe, n_splits=5, random_seed=42):\n",
    "    \"\"\"\n",
    "    Create k-fold splits at the cell-line level (no data leakage).\n",
    "    \n",
    "    Args:\n",
    "        dataframe: DataFrame with ModelID column\n",
    "        n_splits: Number of folds\n",
    "        random_seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        List of (train_idx, val_idx) tuples for each fold\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    cell_lines = dataframe['ModelID'].unique()\n",
    "    n_cells = len(cell_lines)\n",
    "    \n",
    "    # Shuffle cell lines\n",
    "    shuffled_cells = np.random.permutation(cell_lines)\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "    fold_splits = []\n",
    "    \n",
    "    cell_line_indices = np.arange(n_cells)\n",
    "    \n",
    "    for fold_idx, (train_cell_idx, val_cell_idx) in enumerate(kf.split(cell_line_indices)):\n",
    "        train_cells = set(shuffled_cells[train_cell_idx])\n",
    "        val_cells = set(shuffled_cells[val_cell_idx])\n",
    "        \n",
    "        train_idx = dataframe[dataframe['ModelID'].isin(train_cells)].index.values\n",
    "        val_idx = dataframe[dataframe['ModelID'].isin(val_cells)].index.values\n",
    "        \n",
    "        fold_splits.append((train_idx, val_idx))\n",
    "    \n",
    "    return fold_splits\n",
    "\n",
    "def train_phase_pathway_cv(\n",
    "    dataset,\n",
    "    train_idx_all,\n",
    "    val_idx_all,\n",
    "    test_idx,\n",
    "    model_class,\n",
    "    model_kwargs,\n",
    "    device,\n",
    "    phase_name,\n",
    "    n_folds=5,\n",
    "    num_epochs=50,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    patience=10,\n",
    "    scheduler_patience=3,\n",
    "    batch_size=256,\n",
    "    use_prebatched=False,  # CV folds have different splits - use regular batching\n",
    "    random_seed=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a phase with k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: DrugResponsePathwayDataset\n",
    "        train_idx_all: All training indices (will be split into CV folds)\n",
    "        val_idx_all: All validation indices (will be split into CV folds)\n",
    "        test_idx: Test indices (held out completely - NOT used during CV)\n",
    "        model_class: Model class to instantiate\n",
    "        model_kwargs: Keyword arguments for model initialization\n",
    "        device: Device to train on\n",
    "        phase_name: Name of the phase\n",
    "        n_folds: Number of CV folds\n",
    "        num_epochs: Maximum epochs per fold\n",
    "        lr: Learning rate\n",
    "        weight_decay: Weight decay\n",
    "        patience: Early stopping patience\n",
    "        scheduler_patience: LR scheduler patience\n",
    "        batch_size: Batch size\n",
    "        use_prebatched: Whether to use pre-batched data\n",
    "        random_seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with CV results, aggregated metrics, and best model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine train and val for CV splitting\n",
    "    train_val_data = dataset.original_data.loc[np.concatenate([train_idx_all, val_idx_all])]\n",
    "    \n",
    "    cv_splits = cell_line_kfold_split(train_val_data, n_splits=n_folds, random_seed=random_seed)\n",
    "    \n",
    "    cv_results = []\n",
    "    fold_models = []\n",
    "    fold_checkpoints = []\n",
    "    \n",
    "    for fold_idx, (fold_train_idx, fold_val_idx) in enumerate(cv_splits):\n",
    "        \n",
    "        fold_loaders = create_pathway_dataloaders(\n",
    "            dataset, fold_train_idx, fold_val_idx, test_idx,\n",
    "            batch_size=batch_size,\n",
    "            use_prebatched=False,  # Regular batching for CV folds\n",
    "            phase_name=f\"{phase_name.lower().replace(' ', '_')}_fold{fold_idx+1}\",\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        model = model_class(**model_kwargs).to(device)\n",
    "        \n",
    "        fold_checkpoint = models_dir / f\"{phase_name.lower().replace(' ', '_')}_fold{fold_idx+1}_{SPLIT_TYPE}.pt\"\n",
    "        fold_checkpoints.append(fold_checkpoint)\n",
    "        \n",
    "        # Train model\n",
    "        model, history = train_phase_pathway(\n",
    "            model=model,\n",
    "            train_loader=fold_loaders['train'],\n",
    "            val_loader=fold_loaders['val'],\n",
    "            device=device,\n",
    "            phase_name=f\"{phase_name} - Fold {fold_idx+1}\",\n",
    "            num_epochs=num_epochs,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "            patience=patience,\n",
    "            scheduler_patience=scheduler_patience,\n",
    "            checkpoint_path=fold_checkpoint,\n",
    "            prebatched_datasets=fold_loaders.get('datasets', None)\n",
    "        )\n",
    "        \n",
    "        val_results = evaluate_model(model, fold_loaders['val'], device, is_prebatched=False)\n",
    "        val_metrics = val_results['metrics']\n",
    "        \n",
    "        \n",
    "        fold_result = {\n",
    "            'fold': fold_idx + 1,\n",
    "            'val_metrics': val_metrics,\n",
    "            'history': history,\n",
    "            'checkpoint': str(fold_checkpoint)\n",
    "        }\n",
    "        cv_results.append(fold_result)\n",
    "        fold_models.append(model)\n",
    "        \n",
    "    \n",
    "    # Aggregate results across folds\n",
    "    \n",
    "    # Collect metrics across folds\n",
    "    val_metrics_list = [r['val_metrics'] for r in cv_results]\n",
    "    \n",
    "    # Calculate statistics\n",
    "    def calc_stats(metrics_list, metric_name):\n",
    "        values = [m[metric_name] for m in metrics_list]\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        sem = std_val / np.sqrt(len(values))  # Standard error of the mean\n",
    "        # 95% confidence interval (t-distribution)\n",
    "        ci_95 = stats.t.interval(0.95, len(values)-1, loc=mean_val, scale=sem)\n",
    "        return {\n",
    "            'mean': mean_val,\n",
    "            'std': std_val,\n",
    "            'sem': sem,\n",
    "            'ci_95_lower': ci_95[0],\n",
    "            'ci_95_upper': ci_95[1],\n",
    "            'min': np.min(values),\n",
    "            'max': np.max(values),\n",
    "            'values': values\n",
    "        }\n",
    "    \n",
    "    metric_names = ['r2', 'pearson', 'spearman', 'rmse', 'mae']\n",
    "    aggregated = {\n",
    "        'val': {name: calc_stats(val_metrics_list, name) for name in metric_names},\n",
    "    }\n",
    "    \n",
    "    for metric in ['r2', 'pearson', 'rmse']:\n",
    "        stats_dict = aggregated['val'][metric]\n",
    "        print(f\"  {metric.upper():10s}: {stats_dict['mean']:.4f}  {stats_dict['std']:.4f} \"\n",
    "              f\"[95% CI: {stats_dict['ci_95_lower']:.4f}, {stats_dict['ci_95_upper']:.4f}]\")\n",
    "    \n",
    "    # Find best fold (by validation R)\n",
    "    best_fold_idx = np.argmax([r['val_metrics']['r2'] for r in cv_results])\n",
    "    best_model = fold_models[best_fold_idx]\n",
    "    best_checkpoint = fold_checkpoints[best_fold_idx]\n",
    "    \n",
    "    \n",
    "    cv_results_file = output_dir / f\"{phase_name.lower().replace(' ', '_')}_cv_results_{SPLIT_TYPE}.pkl\"\n",
    "    with open(cv_results_file, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'phase_name': phase_name,\n",
    "            'n_folds': n_folds,\n",
    "            'cv_results': cv_results,\n",
    "            'aggregated': aggregated,\n",
    "            'best_fold': best_fold_idx + 1,\n",
    "            'best_checkpoint': str(best_checkpoint)\n",
    "        }, f)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        'best_model': best_model,\n",
    "        'best_checkpoint': best_checkpoint,\n",
    "        'cv_results': cv_results,\n",
    "        'aggregated': aggregated,\n",
    "        'all_models': fold_models\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e4bac",
   "metadata": {},
   "source": [
    "### 4.2 Training and Evaluation Functions\n",
    "\n",
    "Training functions with early stopping, learning rate scheduling, and comprehensive evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9d8ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 5: Training Functions for Pathway-Based Model\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for drug graphs.\"\"\"\n",
    "    drug_graphs = [item['drug_graph'] for item in batch]\n",
    "    cell_exprs = torch.stack([item['cell_expr'] for item in batch])\n",
    "    ic50s = torch.stack([item['ic50'] for item in batch])\n",
    "    drug_batch = Batch.from_data_list(drug_graphs)\n",
    "    return {\n",
    "        'drug_batch': drug_batch,\n",
    "        'cell_batch': cell_exprs,\n",
    "        'ic50': ic50s\n",
    "    }\n",
    "\n",
    "def prebatched_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for pre-batched data.\n",
    "    Since batch_size=1, each item is already a complete batch, just return it.\n",
    "    \"\"\"\n",
    "    return batch[0]  # batch_size=1 means batch is a list with one item\n",
    "\n",
    "class PrebatchedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for pre-batched data with epoch-level batch shuffling.\n",
    "    \"\"\"\n",
    "    def __init__(self, prebatched_batches):\n",
    "        \"\"\"\n",
    "        Initialize pre-batched dataset.\n",
    "        \n",
    "        Args:\n",
    "            prebatched_batches: List of pre-batched dictionaries\n",
    "        \"\"\"\n",
    "        self.batches = prebatched_batches\n",
    "        self.current_order = list(range(len(self.batches)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.batches)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.batches[self.current_order[idx]]\n",
    "    \n",
    "    def shuffle_batches(self, random_seed=None):\n",
    "        \"\"\"\n",
    "        Shuffle batch order for epoch-level randomization.\n",
    "        \n",
    "        Args:\n",
    "            random_seed: Optional random seed for reproducibility\n",
    "        \"\"\"\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "        np.random.shuffle(self.current_order)\n",
    "\n",
    "def create_prebatched_data(dataset, batch_size, split_name, phase_name, output_dir, shuffle_samples=True):\n",
    "    \"\"\"\n",
    "    Pre-compute and save batched data to disk.\n",
    "    \n",
    "    Args:\n",
    "        dataset: DrugResponsePathwayDataset\n",
    "        batch_size: Batch size\n",
    "        split_name: Name of split (train/val/test)\n",
    "        phase_name: Name of training phase (phase1/phase2/phase3)\n",
    "        output_dir: Directory to save pre-batched data\n",
    "        shuffle_samples: Whether to shuffle samples before batching\n",
    "        \n",
    "    Returns:\n",
    "        Path to saved batch file\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    # Include split type in path to avoid overwriting when switching between random and cell_line splits\n",
    "    split_type = globals().get('SPLIT_TYPE', 'unknown')\n",
    "    prebatched_dir = output_dir / \"prebatched_data\" / f\"{phase_name}_{split_type}\"\n",
    "    prebatched_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    batch_file = prebatched_dir / f\"{split_name}_batches.pkl\"\n",
    "    \n",
    "    if batch_file.exists():\n",
    "        return batch_file\n",
    "    \n",
    "    \n",
    "    indices = np.arange(len(dataset))\n",
    "    if shuffle_samples:\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "    \n",
    "    batches = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(indices), batch_size), desc=f\"Pre-batching {split_name}\"):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "        \n",
    "        batch_items = [dataset[idx] for idx in batch_indices]\n",
    "        \n",
    "        # Extract and batch components\n",
    "        drug_graphs = [item['drug_graph'] for item in batch_items]\n",
    "        cell_exprs = torch.stack([item['cell_expr'] for item in batch_items])\n",
    "        ic50s = torch.stack([item['ic50'] for item in batch_items])\n",
    "        \n",
    "        # Batch graphs once\n",
    "        drug_batch = Batch.from_data_list(drug_graphs)\n",
    "        \n",
    "        # Store pre-batched data\n",
    "        batched_data = {\n",
    "            'drug_batch': drug_batch,\n",
    "            'cell_batch': cell_exprs,\n",
    "            'ic50': ic50s\n",
    "        }\n",
    "        batches.append(batched_data)\n",
    "    \n",
    "    with open(batch_file, 'wb') as f:\n",
    "        pickle.dump(batches, f)\n",
    "    \n",
    "    return batch_file\n",
    "\n",
    "def compute_loss(predictions, targets, median_ic50, loss_weights=(1.0, 0.3, 0.3)):\n",
    "    \"\"\"\n",
    "    Compute multi-task loss.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Dictionary with 'ic50', 'classification', 'reconstruction'\n",
    "        targets: Dictionary with 'ic50', 'embeddings'\n",
    "        median_ic50: Threshold for classification\n",
    "        loss_weights: Weights for (IC50, classification, reconstruction)\n",
    "        \n",
    "    Returns:\n",
    "        Total loss and individual losses\n",
    "    \"\"\"\n",
    "    ic50_pred = predictions['ic50'].squeeze()\n",
    "    ic50_target = targets['ic50'].squeeze()\n",
    "    \n",
    "    mse_loss = nn.MSELoss()(ic50_pred, ic50_target)\n",
    "    \n",
    "    class_pred = predictions['classification'].squeeze()\n",
    "    class_target = (ic50_target > median_ic50).float()\n",
    "    bce_loss = nn.BCEWithLogitsLoss()(class_pred, class_target)\n",
    "    \n",
    "    recon_pred = predictions['reconstruction']\n",
    "    recon_target = targets['embeddings']\n",
    "    recon_loss = nn.MSELoss()(recon_pred, recon_target)\n",
    "    \n",
    "    total_loss = loss_weights[0] * mse_loss + loss_weights[1] * bce_loss + loss_weights[2] * recon_loss\n",
    "    \n",
    "    return total_loss, {\n",
    "        'mse': mse_loss.item(),\n",
    "        'bce': bce_loss.item(),\n",
    "        'recon': recon_loss.item()\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, dataloader, device, median_ic50=None, is_prebatched=False):\n",
    "    \"\"\"\n",
    "    Evaluate model on a dataloader.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        dataloader: DataLoader to evaluate on\n",
    "        device: Device to run on\n",
    "        median_ic50: Classification threshold (if None, computed from dataloader)\n",
    "        is_prebatched: Whether using pre-batched data (batch_size=1)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metrics, predictions, and targets\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    if median_ic50 is None:\n",
    "        all_ic50s = []\n",
    "        for batch in dataloader:\n",
    "            # Batch is already unwrapped by prebatched_collate_fn if using pre-batched data\n",
    "            all_ic50s.append(batch['ic50'].cpu().numpy())\n",
    "        if len(all_ic50s) == 0:\n",
    "            raise ValueError(\"Cannot compute median_ic50: dataloader is empty\")\n",
    "        median_ic50 = np.median(np.concatenate(all_ic50s))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Batch is already unwrapped by prebatched_collate_fn if using pre-batched data\n",
    "            drug_batch = batch['drug_batch'].to(device)\n",
    "            cell_batch = batch['cell_batch'].to(device)\n",
    "            ic50_target = batch['ic50'].to(device)\n",
    "            \n",
    "            outputs = model(drug_batch, cell_batch, return_embeddings=True)\n",
    "            combined_emb = outputs['embeddings']['combined']\n",
    "            \n",
    "            targets = {\n",
    "                'ic50': ic50_target,\n",
    "                'embeddings': combined_emb\n",
    "            }\n",
    "            \n",
    "            loss, _ = compute_loss(outputs, targets, median_ic50, loss_weights=(1.0, 0.3, 0.3))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_preds.append(outputs['ic50'].cpu().numpy())\n",
    "            all_targets.append(ic50_target.cpu().numpy())\n",
    "    \n",
    "    # Handle empty dataloader case\n",
    "    if len(all_preds) == 0:\n",
    "        return {\n",
    "            'metrics': {\n",
    "                'loss': 0.0,\n",
    "                'pearson': np.nan,\n",
    "                'spearman': np.nan,\n",
    "                'r2': np.nan,\n",
    "                'rmse': np.nan,\n",
    "                'mae': np.nan\n",
    "            },\n",
    "            'predictions': np.array([]),\n",
    "            'targets': np.array([])\n",
    "        }\n",
    "    \n",
    "    all_preds = np.concatenate(all_preds).flatten()\n",
    "    all_targets = np.concatenate(all_targets).flatten()\n",
    "    \n",
    "    pearson_r, _ = pearsonr(all_targets, all_preds)\n",
    "    spearman_r, _ = spearmanr(all_targets, all_preds)\n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mean_squared_error(all_targets, all_preds))\n",
    "    mae = mean_absolute_error(all_targets, all_preds)\n",
    "    \n",
    "    return {\n",
    "        'metrics': {\n",
    "            'loss': total_loss / len(dataloader) if len(dataloader) > 0 else 0.0,\n",
    "            'pearson': pearson_r,\n",
    "            'spearman': spearman_r,\n",
    "            'r2': r2,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae\n",
    "        },\n",
    "        'predictions': all_preds,\n",
    "        'targets': all_targets\n",
    "    }\n",
    "\n",
    "def create_pathway_dataloaders(dataset, train_idx, val_idx, test_idx, batch_size=128, \n",
    "                                use_prebatched=False, phase_name=None, output_dir=None):\n",
    "    \"\"\"\n",
    "    Create dataloaders using pre-defined cell-line based splits.\n",
    "    \n",
    "    Args:\n",
    "        dataset: DrugResponsePathwayDataset\n",
    "        train_idx: Training indices (from original dataframe)\n",
    "        val_idx: Validation indices (from original dataframe)\n",
    "        test_idx: Test indices (from original dataframe)\n",
    "        batch_size: Batch size\n",
    "        use_prebatched: Whether to use pre-batched data\n",
    "        phase_name: Name of training phase (required if use_prebatched=True)\n",
    "        output_dir: Directory for pre-batched data (required if use_prebatched=True)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with train/val/test loaders and datasets (if pre-batched)\n",
    "    \"\"\"\n",
    "    train_data = dataset.original_data.loc[train_idx].reset_index(drop=True)\n",
    "    val_data = dataset.original_data.loc[val_idx].reset_index(drop=True)\n",
    "    test_data = dataset.original_data.loc[test_idx].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = DrugResponsePathwayDataset(train_data, dataset.drug_graphs, dataset.drug_col, pathway_cols=dataset.pathway_cols)\n",
    "    val_dataset = DrugResponsePathwayDataset(val_data, dataset.drug_graphs, dataset.drug_col, pathway_cols=dataset.pathway_cols)\n",
    "    test_dataset = DrugResponsePathwayDataset(test_data, dataset.drug_graphs, dataset.drug_col, pathway_cols=dataset.pathway_cols)\n",
    "    \n",
    "    if use_prebatched:\n",
    "        if phase_name is None or output_dir is None:\n",
    "            raise ValueError(\"phase_name and output_dir required when use_prebatched=True\")\n",
    "        \n",
    "        train_batch_file = create_prebatched_data(train_dataset, batch_size, 'train', phase_name, output_dir, shuffle_samples=True)\n",
    "        val_batch_file = create_prebatched_data(val_dataset, batch_size, 'val', phase_name, output_dir, shuffle_samples=False)\n",
    "        test_batch_file = create_prebatched_data(test_dataset, batch_size, 'test', phase_name, output_dir, shuffle_samples=False)\n",
    "        \n",
    "        with open(train_batch_file, 'rb') as f:\n",
    "            train_batches = pickle.load(f)\n",
    "        with open(val_batch_file, 'rb') as f:\n",
    "            val_batches = pickle.load(f)\n",
    "        with open(test_batch_file, 'rb') as f:\n",
    "            test_batches = pickle.load(f)\n",
    "        \n",
    "        train_prebatched = PrebatchedDataset(train_batches)\n",
    "        val_prebatched = PrebatchedDataset(val_batches)\n",
    "        test_prebatched = PrebatchedDataset(test_batches)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_prebatched,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            collate_fn=prebatched_collate_fn,\n",
    "            num_workers=4 if torch.cuda.is_available() else 0,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_prebatched,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            collate_fn=prebatched_collate_fn,\n",
    "            num_workers=4 if torch.cuda.is_available() else 0,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_prebatched,\n",
    "            batch_size=1,\n",
    "            shuffle=False,\n",
    "            collate_fn=prebatched_collate_fn,\n",
    "            num_workers=4 if torch.cuda.is_available() else 0,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'train': train_loader,\n",
    "            'val': val_loader,\n",
    "            'test': test_loader,\n",
    "            'datasets': {\n",
    "                'train': train_prebatched,\n",
    "                'val': val_prebatched,\n",
    "                'test': test_prebatched\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        # Standard dataloaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=4 if torch.cuda.is_available() else 0,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=4 if torch.cuda.is_available() else 0,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=collate_fn,\n",
    "            num_workers=4 if torch.cuda.is_available() else 0,\n",
    "            pin_memory=torch.cuda.is_available(),\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'train': train_loader,\n",
    "            'val': val_loader,\n",
    "            'test': test_loader\n",
    "        }\n",
    "\n",
    "def train_phase_pathway(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    phase_name,\n",
    "    num_epochs=50,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4,\n",
    "    patience=10,\n",
    "    scheduler_patience=3,\n",
    "    checkpoint_path=None,\n",
    "    prebatched_datasets=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a phase of the pathway-based model.\n",
    "    \n",
    "    Args:\n",
    "        model: DrugResponsePathwayGNN model\n",
    "        train_loader: Training DataLoader\n",
    "        val_loader: Validation DataLoader\n",
    "        device: Device to train on\n",
    "        phase_name: Name of the phase\n",
    "        num_epochs: Maximum epochs\n",
    "        lr: Learning rate\n",
    "        weight_decay: Weight decay\n",
    "        patience: Early stopping patience\n",
    "        scheduler_patience: LR scheduler patience\n",
    "        checkpoint_path: Path to save best model\n",
    "        prebatched_datasets: Dictionary with 'train' PrebatchedDataset (for epoch-level shuffling)\n",
    "        \n",
    "    Returns:\n",
    "        Trained model and training history\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.7, patience=scheduler_patience)  # Less aggressive: 0.7 instead of 0.5\n",
    "    \n",
    "    is_prebatched = prebatched_datasets is not None and 'train' in prebatched_datasets\n",
    "    \n",
    "    all_ic50s = []\n",
    "    for batch in train_loader:\n",
    "        # Batch is already unwrapped by prebatched_collate_fn if using pre-batched data\n",
    "        all_ic50s.append(batch['ic50'].cpu().numpy())\n",
    "    median_ic50 = np.median(np.concatenate(all_ic50s))\n",
    "    \n",
    "    best_val_r2 = float('-inf')\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_r2': [], 'val_pearson': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle batches at start of each epoch (for pre-batched data)\n",
    "        if is_prebatched:\n",
    "            prebatched_datasets['train'].shuffle_batches(random_seed=epoch)\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            # Batch is already unwrapped by prebatched_collate_fn if using pre-batched data\n",
    "            drug_batch = batch['drug_batch'].to(device)\n",
    "            cell_batch = batch['cell_batch'].to(device)\n",
    "            ic50_target = batch['ic50'].to(device)\n",
    "            \n",
    "            outputs = model(drug_batch, cell_batch, return_embeddings=True)\n",
    "            combined_emb = outputs['embeddings']['combined']\n",
    "            \n",
    "            targets = {\n",
    "                'ic50': ic50_target,\n",
    "                'embeddings': combined_emb.detach()\n",
    "            }\n",
    "            \n",
    "            loss, _ = compute_loss(outputs, targets, median_ic50, loss_weights=(1.0, 0.3, 0.3))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        eval_result = evaluate_model(model, val_loader, device, median_ic50, is_prebatched=is_prebatched)\n",
    "        val_metrics = eval_result['metrics']\n",
    "        val_r2 = val_metrics['r2']\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_r2'].append(val_r2)\n",
    "        history['val_pearson'].append(val_metrics['pearson'])\n",
    "        \n",
    "        scheduler.step(val_r2)\n",
    "        \n",
    "        \n",
    "        if val_r2 > best_val_r2:\n",
    "            best_val_r2 = val_r2\n",
    "            patience_counter = 0\n",
    "            if checkpoint_path:\n",
    "                torch.save({\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'val_metrics': val_metrics,\n",
    "                    'classification_threshold': median_ic50,\n",
    "                    'epoch': epoch + 1\n",
    "                }, checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "    \n",
    "    if checkpoint_path and Path(checkpoint_path).exists():\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Final Training Results:\")\n",
    "        best_val_metrics = checkpoint.get('val_metrics', {})\n",
    "        if best_val_metrics:\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Evaluate classification performance\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "        model.eval()\n",
    "        class_preds_list = []\n",
    "        class_probs_list = []\n",
    "        class_targets_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                drug_batch = batch['drug_batch'].to(device)\n",
    "                cell_batch = batch['cell_batch'].to(device)\n",
    "                ic50_target = batch['ic50'].to(device)\n",
    "                \n",
    "                outputs = model(drug_batch, cell_batch)\n",
    "                class_logits = outputs['classification'].squeeze()\n",
    "                class_probs = torch.sigmoid(class_logits).cpu().numpy()\n",
    "                class_preds = (class_probs > 0.5).astype(int)\n",
    "                class_targets = (ic50_target.cpu().numpy() > median_ic50).astype(int)\n",
    "                \n",
    "                class_probs_list.extend(class_probs)\n",
    "                class_preds_list.extend(class_preds)\n",
    "                class_targets_list.extend(class_targets)\n",
    "        \n",
    "        class_targets = np.array(class_targets_list)\n",
    "        class_preds = np.array(class_preds_list)\n",
    "        class_probs = np.array(class_probs_list)\n",
    "        \n",
    "        try:\n",
    "            roc_auc = roc_auc_score(class_targets, class_probs)\n",
    "        except:\n",
    "            pass\n",
    "        best_val_metrics = checkpoint.get('val_metrics', {})\n",
    "        print(f\"Final Training Results:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        if best_val_metrics:\n",
    "            print(f\"  R:        {best_val_metrics.get('r2', 0):.4f}\")\n",
    "            print(f\"  Pearson:   {best_val_metrics.get('pearson', 0):.4f}\")\n",
    "            print(f\"  Spearman:  {best_val_metrics.get('spearman', 0):.4f}\")\n",
    "            print(f\"  RMSE:      {best_val_metrics.get('rmse', 0):.4f}\")\n",
    "            print(f\"  MAE:       {best_val_metrics.get('mae', 0):.4f}\")\n",
    "        else:\n",
    "            print(f\"  R:        {best_val_r2:.4f}\")\n",
    "        print(f\"\\nTraining Loss: {history['train_loss'][-1]:.4f}\")\n",
    "        print(f\"Validation Loss: {history['val_loss'][-1]:.4f}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181f91fa",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "### 5.1 Data Loader Preparation\n",
    "\n",
    "Create data loaders for all three phases with pre-batched data for efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce64b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 6: 3-Phase Transfer Learning Pipeline\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "models_dir = output_dir / \"models\"\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "split_type = globals().get('SPLIT_TYPE', 'unknown')\n",
    "phase1_checkpoint = models_dir / f\"trial2_phase1_pathway_{split_type}.pt\"\n",
    "phase2_checkpoint = models_dir / f\"trial2_phase2_breast_pathway_{split_type}.pt\"\n",
    "phase3_checkpoint = models_dir / f\"trial2_phase3_tnbc_pathway_{split_type}.pt\"\n",
    "\n",
    "actual_pathway_count = len(pathway_cols)\n",
    "\n",
    "pan_dataset_pathway = DrugResponsePathwayDataset(pan_cancer_pathway, drug_graphs, drug_col=drug_name_col, pathway_cols=pathway_cols)\n",
    "breast_dataset_pathway = DrugResponsePathwayDataset(breast_cancer_pathway, drug_graphs, drug_col=drug_name_col, pathway_cols=pathway_cols)\n",
    "tnbc_dataset_pathway = DrugResponsePathwayDataset(tnbc_pathway, drug_graphs, drug_col=drug_name_col, pathway_cols=pathway_cols)\n",
    "\n",
    "pan_loaders = create_pathway_dataloaders(\n",
    "    pan_dataset_pathway, pan_train_idx, pan_val_idx, pan_test_idx, \n",
    "    batch_size=256, use_prebatched=True, phase_name=\"phase1\", output_dir=output_dir\n",
    ")\n",
    "breast_loaders = create_pathway_dataloaders(\n",
    "    breast_dataset_pathway, breast_train_idx, breast_val_idx, breast_test_idx, \n",
    "    batch_size=256, use_prebatched=True, phase_name=\"phase2\", output_dir=output_dir\n",
    ")\n",
    "tnbc_loaders = create_pathway_dataloaders(\n",
    "    tnbc_dataset_pathway, tnbc_train_idx, tnbc_val_idx, tnbc_test_idx, \n",
    "    batch_size=256, use_prebatched=True, phase_name=\"phase3\", output_dir=output_dir\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3448b0cf",
   "metadata": {},
   "source": [
    "### 5.2 Phase 1: Pan-Cancer Pre-training\n",
    "\n",
    "Train on pan-cancer data using 5-fold cross-validation. Results include mean  standard deviation and 95% confidence intervals across folds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb9e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 1: Pan-Cancer Training (5-Fold Cross-Validation)\n",
    "\n",
    "model_phase1 = DrugResponsePathwayGNN(cell_input_dim=actual_pathway_count).to(device)\n",
    "\n",
    "cv_results_file = output_dir / f\"phase_1_pan-cancer_cv_results_{SPLIT_TYPE}.pkl\"\n",
    "\n",
    "if cv_results_file.exists():\n",
    "    with open(cv_results_file, 'rb') as f:\n",
    "        phase1_cv_data = pickle.load(f)\n",
    "    best_checkpoint_path = Path(phase1_cv_data['best_checkpoint'])\n",
    "\n",
    "    if not best_checkpoint_path.exists():\n",
    "        phase1_cv_results = None\n",
    "    else:\n",
    "        checkpoint = torch.load(best_checkpoint_path, map_location=device, weights_only=False)\n",
    "        model_phase1.load_state_dict(checkpoint['model_state_dict'])\n",
    "        phase1_cv_results = phase1_cv_data\n",
    "elif phase1_checkpoint.exists():\n",
    "    checkpoint = torch.load(phase1_checkpoint, map_location=device, weights_only=False)\n",
    "    model_phase1.load_state_dict(checkpoint['model_state_dict'])\n",
    "else:\n",
    "    # Run 5-fold cross-validation\n",
    "    phase1_cv_results = train_phase_pathway_cv(\n",
    "        dataset=pan_dataset_pathway,\n",
    "        train_idx_all=pan_train_idx,\n",
    "        val_idx_all=pan_val_idx,\n",
    "        test_idx=pan_test_idx,\n",
    "        model_class=DrugResponsePathwayGNN,\n",
    "        model_kwargs={'cell_input_dim': actual_pathway_count},\n",
    "        device=device,\n",
    "        phase_name=\"Phase 1: Pan-Cancer\",\n",
    "        n_folds=5,\n",
    "        num_epochs=50,\n",
    "        lr=1.5e-3,\n",
    "        weight_decay=1e-4,\n",
    "        patience=10,\n",
    "        scheduler_patience=5,\n",
    "        batch_size=256,\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    model_phase1 = phase1_cv_results['best_model']\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': model_phase1.state_dict(),\n",
    "        'val_metrics': phase1_cv_results['cv_results'][phase1_cv_results['best_fold']-1]['val_metrics'],\n",
    "        'epoch': len(phase1_cv_results['cv_results'][phase1_cv_results['best_fold']-1]['history']['train_loss']),\n",
    "        'cv_best_fold': phase1_cv_results['best_fold']\n",
    "    }, phase1_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dde2d2d",
   "metadata": {},
   "source": [
    "### 5.3 Phase 2: Breast Cancer Fine-tuning\n",
    "\n",
    "Fine-tune Phase 1 model on breast cancer data using 5-fold cross-validation. Model weights are transferred from Phase 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a49879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 2: Breast Cancer Training (5-Fold Cross-Validation)\n",
    "\n",
    "model_phase2 = DrugResponsePathwayGNN(cell_input_dim=actual_pathway_count).to(device)\n",
    "# Transfer weights from Phase 1\n",
    "model_phase2.load_state_dict(model_phase1.state_dict(), strict=False)\n",
    "\n",
    "cv_results_file = output_dir / f\"phase_2_breast_cancer_cv_results_{SPLIT_TYPE}.pkl\"\n",
    "\n",
    "if cv_results_file.exists():\n",
    "    with open(cv_results_file, 'rb') as f:\n",
    "        phase2_cv_data = pickle.load(f)\n",
    "    best_checkpoint_path = Path(phase2_cv_data['best_checkpoint'])\n",
    "\n",
    "    if not best_checkpoint_path.exists():\n",
    "        phase2_cv_results = None\n",
    "    else:\n",
    "        checkpoint = torch.load(best_checkpoint_path, map_location=device, weights_only=False)\n",
    "        model_phase2.load_state_dict(checkpoint['model_state_dict'])\n",
    "        phase2_cv_results = phase2_cv_data\n",
    "elif phase2_checkpoint.exists():\n",
    "    checkpoint = torch.load(phase2_checkpoint, map_location=device, weights_only=False)\n",
    "    model_phase2.load_state_dict(checkpoint['model_state_dict'])\n",
    "else:\n",
    "    # Run 5-fold cross-validation\n",
    "    phase2_cv_results = train_phase_pathway_cv(\n",
    "        dataset=breast_dataset_pathway,\n",
    "        train_idx_all=breast_train_idx,\n",
    "        val_idx_all=breast_val_idx,\n",
    "        test_idx=breast_test_idx,\n",
    "        model_class=DrugResponsePathwayGNN,\n",
    "        model_kwargs={'cell_input_dim': actual_pathway_count},\n",
    "        device=device,\n",
    "        phase_name=\"Phase 2: Breast Cancer\",\n",
    "        n_folds=5,\n",
    "        num_epochs=50,\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-5,\n",
    "        patience=15,\n",
    "        scheduler_patience=5,\n",
    "        batch_size=256,\n",
    "        random_seed=42\n",
    "    )\n",
    "    \n",
    "    model_phase2 = phase2_cv_results['best_model']\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': model_phase2.state_dict(),\n",
    "        'val_metrics': phase2_cv_results['cv_results'][phase2_cv_results['best_fold']-1]['val_metrics'],\n",
    "        'epoch': len(phase2_cv_results['cv_results'][phase2_cv_results['best_fold']-1]['history']['train_loss']),\n",
    "        'cv_best_fold': phase2_cv_results['best_fold']\n",
    "    }, phase2_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee01f9",
   "metadata": {},
   "source": [
    "### 5.4 Phase 3: TNBC-Specific Fine-tuning\n",
    "\n",
    "Final fine-tuning on TNBC data. Uses single train/val/test split due to limited sample size (n=24 cell lines).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PHASE 3: TNBC Final Fine-tuning\n",
    "model_phase3 = DrugResponsePathwayGNN(cell_input_dim=actual_pathway_count).to(device)\n",
    "\n",
    "if phase2_checkpoint.exists():\n",
    "    checkpoint = torch.load(phase2_checkpoint, map_location=device, weights_only=False)\n",
    "    model_phase3.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "if phase3_checkpoint.exists():\n",
    "    checkpoint = torch.load(phase3_checkpoint, map_location=device, weights_only=False)\n",
    "    model_phase3.load_state_dict(checkpoint['model_state_dict'])\n",
    "else:\n",
    "    model_phase3, phase3_history = train_phase_pathway(\n",
    "        model=model_phase3,\n",
    "        train_loader=tnbc_loaders['train'],\n",
    "        val_loader=tnbc_loaders['val'],\n",
    "        device=device,\n",
    "        phase_name=\"Phase 3: TNBC\",\n",
    "        num_epochs=50,\n",
    "        lr=5e-5,\n",
    "        weight_decay=1e-5,\n",
    "        patience=15,\n",
    "        scheduler_patience=5,\n",
    "        checkpoint_path=phase3_checkpoint,\n",
    "        prebatched_datasets=tnbc_loaders.get('datasets', None)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea87938b",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "### 6.1 Performance Metrics\n",
    "\n",
    "Evaluate models on test sets and save comprehensive performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_suffix = f\"_{SPLIT_TYPE}_split\" if 'SPLIT_TYPE' in globals() else \"\"\n",
    "performance_csv_path = output_dir / f\"performance_metrics{split_suffix}.csv\"\n",
    "performance_pkl_path = output_dir / f\"performance_metrics{split_suffix}.pkl\"\n",
    "\n",
    "results_df.to_csv(performance_csv_path, index=False)\n",
    "\n",
    "with open(performance_pkl_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'results_df': results_df,\n",
    "        'phase1_results': phase1_results,\n",
    "        'phase2_results': phase2_results,\n",
    "        'phase3_results': phase3_results,\n",
    "        'split_type': SPLIT_TYPE if 'SPLIT_TYPE' in globals() else 'unknown',\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }, f)\n",
    "\n",
    "classification_metrics = {}\n",
    "if 'class_results_phase1' in globals():\n",
    "    classification_metrics['phase1'] = {\n",
    "        'accuracy': accuracy_score(class_results_phase1['ground_truth'], class_results_phase1['predictions']),\n",
    "        'precision': precision_score(class_results_phase1['ground_truth'], class_results_phase1['predictions'], zero_division=0),\n",
    "        'recall': recall_score(class_results_phase1['ground_truth'], class_results_phase1['predictions'], zero_division=0),\n",
    "        'f1_score': f1_score(class_results_phase1['ground_truth'], class_results_phase1['predictions'], zero_division=0),\n",
    "        'roc_auc': roc_auc_score(class_results_phase1['ground_truth'], class_results_phase1['probabilities']),\n",
    "        'n_samples': len(class_results_phase1['ground_truth']),\n",
    "        'n_sensitive': int(np.sum(class_results_phase1['ground_truth'] == 0)),\n",
    "        'n_resistant': int(np.sum(class_results_phase1['ground_truth'] == 1))\n",
    "    }\n",
    "if 'class_results_phase2' in globals():\n",
    "    classification_metrics['phase2'] = {\n",
    "        'accuracy': accuracy_score(class_results_phase2['ground_truth'], class_results_phase2['predictions']),\n",
    "        'precision': precision_score(class_results_phase2['ground_truth'], class_results_phase2['predictions'], zero_division=0),\n",
    "        'recall': recall_score(class_results_phase2['ground_truth'], class_results_phase2['predictions'], zero_division=0),\n",
    "        'f1_score': f1_score(class_results_phase2['ground_truth'], class_results_phase2['predictions'], zero_division=0),\n",
    "        'roc_auc': roc_auc_score(class_results_phase2['ground_truth'], class_results_phase2['probabilities']),\n",
    "        'n_samples': len(class_results_phase2['ground_truth']),\n",
    "        'n_sensitive': int(np.sum(class_results_phase2['ground_truth'] == 0)),\n",
    "        'n_resistant': int(np.sum(class_results_phase2['ground_truth'] == 1))\n",
    "    }\n",
    "if 'class_results_phase3' in globals():\n",
    "    classification_metrics['phase3'] = {\n",
    "        'accuracy': accuracy_score(class_results_phase3['ground_truth'], class_results_phase3['predictions']),\n",
    "        'precision': precision_score(class_results_phase3['ground_truth'], class_results_phase3['predictions'], zero_division=0),\n",
    "        'recall': recall_score(class_results_phase3['ground_truth'], class_results_phase3['predictions'], zero_division=0),\n",
    "        'f1_score': f1_score(class_results_phase3['ground_truth'], class_results_phase3['predictions'], zero_division=0),\n",
    "        'roc_auc': roc_auc_score(class_results_phase3['ground_truth'], class_results_phase3['probabilities']),\n",
    "        'n_samples': len(class_results_phase3['ground_truth']),\n",
    "        'n_sensitive': int(np.sum(class_results_phase3['ground_truth'] == 0)),\n",
    "        'n_resistant': int(np.sum(class_results_phase3['ground_truth'] == 1))\n",
    "    }\n",
    "\n",
    "if classification_metrics:\n",
    "    classification_csv_path = output_dir / f\"classification_metrics{split_suffix}.csv\"\n",
    "    classification_df = pd.DataFrame(classification_metrics).T\n",
    "    classification_df.index.name = 'phase'\n",
    "    classification_df.to_csv(classification_csv_path)\n",
    "    \n",
    "    classification_pkl_path = output_dir / f\"classification_metrics{split_suffix}.pkl\"\n",
    "    with open(classification_pkl_path, 'wb') as f:\n",
    "        pickle.dump(classification_metrics, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e839ed3",
   "metadata": {},
   "source": [
    "### 6.2 Classification Head Evaluation\n",
    "\n",
    "Evaluate binary classification performance (sensitive vs resistant) using median IC50 as threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c874971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Classification Head Function\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, classification_report\n",
    "\n",
    "def evaluate_classification_head(model_path, test_loader, device, phase_name, median_ic50=None):\n",
    "    \"\"\"\n",
    "    Evaluate classification head predictions.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to model checkpoint\n",
    "        test_loader: Test DataLoader\n",
    "        device: Device to run on\n",
    "        phase_name: Name of the phase\n",
    "        median_ic50: Classification threshold (if None, computed from test data)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with predictions, probabilities, and ground truth\n",
    "    \"\"\"\n",
    "    if isinstance(test_loader.dataset, PrebatchedDataset):\n",
    "        pathway_dim = len(pathway_cols)\n",
    "    else:\n",
    "        pathway_dim = len(test_loader.dataset.pathway_cols)\n",
    "    \n",
    "    model = DrugResponsePathwayGNN(cell_input_dim=pathway_dim).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    if median_ic50 is None:\n",
    "        checkpoint_median = checkpoint.get('classification_threshold', None)\n",
    "        if checkpoint_median is not None:\n",
    "            median_ic50 = checkpoint_median\n",
    "        else:\n",
    "            # Compute from test data\n",
    "            all_ic50s = []\n",
    "            for batch in test_loader:\n",
    "                all_ic50s.append(batch['ic50'].cpu().numpy())\n",
    "            median_ic50 = np.median(np.concatenate(all_ic50s))\n",
    "    \n",
    "    is_prebatched = isinstance(test_loader.dataset, PrebatchedDataset) or test_loader.batch_size == 1\n",
    "    \n",
    "    model.eval()\n",
    "    all_class_logits = []\n",
    "    all_ic50_preds = []\n",
    "    all_ic50_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            drug_batch = batch['drug_batch'].to(device)\n",
    "            cell_batch = batch['cell_batch'].to(device)\n",
    "            ic50_target = batch['ic50'].to(device)\n",
    "            \n",
    "            outputs = model(drug_batch, cell_batch, return_embeddings=False)\n",
    "            \n",
    "            all_class_logits.append(outputs['classification'].cpu().numpy())\n",
    "            all_ic50_preds.append(outputs['ic50'].cpu().numpy())\n",
    "            all_ic50_targets.append(ic50_target.cpu().numpy())\n",
    "    \n",
    "    class_logits = np.concatenate(all_class_logits).flatten()\n",
    "    class_probs = torch.sigmoid(torch.tensor(class_logits)).numpy()\n",
    "    ic50_preds = np.concatenate(all_ic50_preds).flatten()\n",
    "    ic50_targets = np.concatenate(all_ic50_targets).flatten()\n",
    "    \n",
    "    # Ground truth labels: 1 if IC50 > median (resistant), 0 if IC50 <= median (sensitive)\n",
    "    y_true = (ic50_targets > median_ic50).astype(int)\n",
    "    \n",
    "    # Binary predictions using 0.5 threshold\n",
    "    y_pred = (class_probs >= 0.5).astype(int)\n",
    "    \n",
    "    return {\n",
    "        'phase_name': phase_name,\n",
    "        'logits': class_logits,\n",
    "        'probabilities': class_probs,\n",
    "        'predictions': y_pred,\n",
    "        'ground_truth': y_true,\n",
    "        'ic50_targets': ic50_targets,\n",
    "        'ic50_preds': ic50_preds,\n",
    "        'median_ic50': median_ic50\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a58ed22",
   "metadata": {},
   "source": [
    "### 6.3 Visualization Helper Functions\n",
    "\n",
    "Load saved splits to ensure reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f58dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_splits(split_type='cell_line'):\n",
    "    \"\"\"Load saved train/val/test split indices.\"\"\"\n",
    "    splits_file = splits_dir / f\"data_splits_{split_type}.json\"\n",
    "    if not splits_file.exists():\n",
    "        raise FileNotFoundError(f\"Saved splits not found: {splits_file}. Please run data splitting cell first.\")\n",
    "    \n",
    "    with open(splits_file, 'r') as f:\n",
    "        splits_data = json.load(f)\n",
    "    \n",
    "    return {\n",
    "        'pan_cancer': {\n",
    "            'train': np.array(splits_data['pan_cancer']['train']),\n",
    "            'val': np.array(splits_data['pan_cancer']['val']),\n",
    "            'test': np.array(splits_data['pan_cancer']['test'])\n",
    "        },\n",
    "        'breast_cancer': {\n",
    "            'train': np.array(splits_data['breast_cancer']['train']),\n",
    "            'val': np.array(splits_data['breast_cancer']['val']),\n",
    "            'test': np.array(splits_data['breast_cancer']['test'])\n",
    "        },\n",
    "        'tnbc': {\n",
    "            'train': np.array(splits_data['tnbc']['train']),\n",
    "            'val': np.array(splits_data['tnbc']['val']),\n",
    "            'test': np.array(splits_data['tnbc']['test'])\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2798a029",
   "metadata": {},
   "source": [
    "## 7. Results Visualization\n",
    "\n",
    "Generate comprehensive visualizations comparing model performance across phases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ce20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "SPLIT_TYPE = 'cell_line'\n",
    "\n",
    "if 'pan_loaders' not in globals() or 'breast_loaders' not in globals() or 'tnbc_loaders' not in globals():\n",
    "    splits = load_saved_splits(SPLIT_TYPE)\n",
    "    pan_train_idx = splits['pan_cancer']['train']\n",
    "    pan_val_idx = splits['pan_cancer']['val']\n",
    "    pan_test_idx = splits['pan_cancer']['test']\n",
    "    breast_train_idx = splits['breast_cancer']['train']\n",
    "    breast_val_idx = splits['breast_cancer']['val']\n",
    "    breast_test_idx = splits['breast_cancer']['test']\n",
    "    tnbc_train_idx = splits['tnbc']['train']\n",
    "    tnbc_val_idx = splits['tnbc']['val']\n",
    "    tnbc_test_idx = splits['tnbc']['test']\n",
    "    \n",
    "    pan_dataset_pathway = DrugResponsePathwayDataset(pan_cancer_pathway, drug_graphs, drug_col=drug_name_col, pathway_cols=pathway_cols)\n",
    "    breast_dataset_pathway = DrugResponsePathwayDataset(breast_cancer_pathway, drug_graphs, drug_col=drug_name_col, pathway_cols=pathway_cols)\n",
    "    tnbc_dataset_pathway = DrugResponsePathwayDataset(tnbc_pathway, drug_graphs, drug_col=drug_name_col, pathway_cols=pathway_cols)\n",
    "    \n",
    "    pan_loaders = create_pathway_dataloaders(\n",
    "        pan_dataset_pathway, pan_train_idx, pan_val_idx, pan_test_idx, \n",
    "        batch_size=256, use_prebatched=True, phase_name=\"phase1\", output_dir=output_dir\n",
    "    )\n",
    "    breast_loaders = create_pathway_dataloaders(\n",
    "        breast_dataset_pathway, breast_train_idx, breast_val_idx, breast_test_idx, \n",
    "        batch_size=256, use_prebatched=True, phase_name=\"phase2\", output_dir=output_dir\n",
    "    )\n",
    "    tnbc_loaders = create_pathway_dataloaders(\n",
    "        tnbc_dataset_pathway, tnbc_train_idx, tnbc_val_idx, tnbc_test_idx, \n",
    "        batch_size=256, use_prebatched=True, phase_name=\"phase3\", output_dir=output_dir\n",
    "    )\n",
    "\n",
    "split_type = SPLIT_TYPE if 'SPLIT_TYPE' in globals() else 'unknown'\n",
    "\n",
    "def find_checkpoint(phase_num, split_type):\n",
    "    \"\"\"Find checkpoint file for cell-line split models.\"\"\"\n",
    "    models_dir = output_dir / \"models\"\n",
    "    \n",
    "    if phase_num == 1:\n",
    "        name = \"trial2_phase1_pathway_cellsplit.pt\"\n",
    "    elif phase_num == 2:\n",
    "        name = \"trial2_phase2_breast_pathway_cellsplit.pt\"\n",
    "    elif phase_num == 3:\n",
    "        name = \"trial2_phase3_tnbc_pathway_cellsplit.pt\"\n",
    "    \n",
    "    return models_dir / name\n",
    "\n",
    "def evaluate_phase_pathway(model_path, test_loader, phase_name, device):\n",
    "    \"\"\"\n",
    "    Evaluate a phase on its test set.\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to model checkpoint\n",
    "        test_loader: Test DataLoader\n",
    "        phase_name: Name of the phase\n",
    "        device: Device to run on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with metrics and predictions\n",
    "    \"\"\"\n",
    "    n_samples = len(test_loader.dataset)\n",
    "    \n",
    "    if n_samples == 0:\n",
    "        print(f\"Warning: {phase_name} test set is empty - returning NaN metrics\")\n",
    "        return {\n",
    "            'phase_name': phase_name,\n",
    "            'metrics': {\n",
    "                'loss': 0.0,\n",
    "                'pearson': np.nan,\n",
    "                'spearman': np.nan,\n",
    "                'r2': np.nan,\n",
    "                'rmse': np.nan,\n",
    "                'mae': np.nan\n",
    "            },\n",
    "            'predictions': np.array([]),\n",
    "            'targets': np.array([]),\n",
    "            'n_samples': 0,\n",
    "            'n_cells': 0\n",
    "        }\n",
    "    \n",
    "    if isinstance(test_loader.dataset, PrebatchedDataset):\n",
    "        pathway_dim = len(pathway_cols)\n",
    "    else:\n",
    "        pathway_dim = len(test_loader.dataset.pathway_cols)\n",
    "    \n",
    "    model = DrugResponsePathwayGNN(cell_input_dim=pathway_dim).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    is_prebatched = isinstance(test_loader.dataset, PrebatchedDataset) or test_loader.batch_size == 1\n",
    "    results = evaluate_model(model, test_loader, device, is_prebatched=is_prebatched)\n",
    "    \n",
    "    if isinstance(test_loader.dataset, PrebatchedDataset):\n",
    "        n_cells = None\n",
    "    else:\n",
    "        n_cells = test_loader.dataset.data['ModelID'].nunique()\n",
    "    \n",
    "    return {\n",
    "        'phase_name': phase_name,\n",
    "        'metrics': results['metrics'],\n",
    "        'predictions': results['predictions'],\n",
    "        'targets': results['targets'],\n",
    "        'n_samples': n_samples,\n",
    "        'n_cells': n_cells\n",
    "    }\n",
    "\n",
    "phase1_checkpoint = find_checkpoint(1, split_type)\n",
    "phase2_checkpoint = find_checkpoint(2, split_type)\n",
    "phase3_checkpoint = find_checkpoint(3, split_type)\n",
    "\n",
    "phase1_results = evaluate_phase_pathway(phase1_checkpoint, pan_loaders['test'], \"Phase 1: Pan-Cancer\", device)\n",
    "phase2_results = evaluate_phase_pathway(phase2_checkpoint, breast_loaders['test'], \"Phase 2: Breast Cancer\", device)\n",
    "phase3_results = evaluate_phase_pathway(phase3_checkpoint, tnbc_loaders['test'], \"Phase 3: TNBC\", device)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Phase': ['Phase 1: Pan-Cancer', 'Phase 2: Breast Cancer', 'Phase 3: TNBC'],\n",
    "    'Pearson': [phase1_results['metrics']['pearson'], phase2_results['metrics']['pearson'], phase3_results['metrics']['pearson']],\n",
    "    'Spearman': [phase1_results['metrics']['spearman'], phase2_results['metrics']['spearman'], phase3_results['metrics']['spearman']],\n",
    "    'R': [phase1_results['metrics']['r2'], phase2_results['metrics']['r2'], phase3_results['metrics']['r2']],\n",
    "    'RMSE': [phase1_results['metrics']['rmse'], phase2_results['metrics']['rmse'], phase3_results['metrics']['rmse']],\n",
    "    'MAE': [phase1_results['metrics']['mae'], phase2_results['metrics']['mae'], phase3_results['metrics']['mae']]\n",
    "})\n",
    "\n",
    "split_suffix = f\"_{SPLIT_TYPE}_split\"\n",
    "performance_csv_path = output_dir / f\"performance_metrics{split_suffix}.csv\"\n",
    "performance_pkl_path = output_dir / f\"performance_metrics{split_suffix}.pkl\"\n",
    "\n",
    "results_df.to_csv(performance_csv_path, index=False)\n",
    "\n",
    "with open(performance_pkl_path, 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'results_df': results_df,\n",
    "        'phase1_results': phase1_results,\n",
    "        'phase2_results': phase2_results,\n",
    "        'phase3_results': phase3_results,\n",
    "        'split_type': SPLIT_TYPE\n",
    "    }, f)\n",
    "\n",
    "class_results_phase1 = evaluate_classification_head(phase1_checkpoint, pan_loaders['test'], device, \"Phase 1: Pan-Cancer\")\n",
    "class_results_phase2 = evaluate_classification_head(phase2_checkpoint, breast_loaders['test'], device, \"Phase 2: Breast Cancer\")\n",
    "class_results_phase3 = evaluate_classification_head(phase3_checkpoint, tnbc_loaders['test'], device, \"Phase 3: TNBC\")\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5.25))\n",
    "\n",
    "results = class_results_phase3\n",
    "y_true = results['ground_truth']\n",
    "y_pred = results['predictions']\n",
    "y_probs = results['probabilities']\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "ax1 = plt.subplot(3, 3, 1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='gray', ax=ax1, cbar_kws={'label': 'Count'})\n",
    "ax1.set_xlabel('Predicted (0=Sensitive, 1=Resistant)')\n",
    "ax1.set_ylabel('True (0=Sensitive, 1=Resistant)')\n",
    "ax1.set_title(f'{results[\"phase_name\"]}\\nConfusion Matrix')\n",
    "\n",
    "# 2. ROC Curve\n",
    "ax2 = plt.subplot(3, 3, 2)\n",
    "fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "ax2.plot(fpr, tpr, color='black', lw=2, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "ax2.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--', label='Random')\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC Curve')\n",
    "ax2.legend(loc=\"lower right\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Precision-Recall Curve\n",
    "ax3 = plt.subplot(3, 3, 3)\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
    "pr_auc = auc(recall, precision)\n",
    "ax3.plot(recall, precision, color='black', lw=2, label=f'PR curve (AUC = {pr_auc:.3f})')\n",
    "baseline = np.sum(y_true) / len(y_true)\n",
    "ax3.axhline(y=baseline, color='black', linestyle='--', label=f'Baseline = {baseline:.3f}')\n",
    "ax3.set_xlabel('Recall')\n",
    "ax3.set_ylabel('Precision')\n",
    "ax3.set_title('Precision-Recall Curve')\n",
    "ax3.legend(loc=\"lower left\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Probability Distribution by Class\n",
    "ax4 = plt.subplot(3, 3, 4)\n",
    "sensitive_probs = y_probs[y_true == 0]\n",
    "resistant_probs = y_probs[y_true == 1]\n",
    "ax4.hist(sensitive_probs, bins=30, alpha=0.7, label='Sensitive (IC50  median)', color='green', density=True)\n",
    "ax4.hist(resistant_probs, bins=30, alpha=0.7, label='Resistant (IC50 > median)', color='red', density=True)\n",
    "ax4.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold (0.5)')\n",
    "ax4.set_xlabel('Predicted Probability')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.set_title('Probability Distribution by True Class')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Classification Report\n",
    "ax5 = plt.subplot(3, 3, 5)\n",
    "ax5.axis('off')\n",
    "report_text = classification_report(y_true, y_pred, target_names=['Sensitive', 'Resistant'])\n",
    "ax5.text(0.1, 0.5, report_text, fontsize=10, family='monospace', verticalalignment='center')\n",
    "ax5.set_title('Classification Report')\n",
    "\n",
    "# 6. IC50 vs Classification Probability\n",
    "ax6 = plt.subplot(3, 3, 6)\n",
    "scatter = ax6.scatter(results['ic50_targets'], y_probs, c=y_true, cmap='gray', alpha=0.7, edgecolors='black', linewidth=0.5)\n",
    "ax6.axhline(y=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "ax6.axvline(x=results['median_ic50'], color='blue', linestyle='--', linewidth=2, label=f'Median IC50 = {results[\"median_ic50\"]:.2f}')\n",
    "ax6.set_xlabel('True IC50 (LN_IC50)')\n",
    "ax6.set_ylabel('Predicted Classification Probability')\n",
    "ax6.set_title('IC50 vs Classification Probability')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "cbar = plt.colorbar(scatter, ax=ax6)\n",
    "cbar.set_label('True Label (0=Sensitive, 1=Resistant)')\n",
    "\n",
    "# 7. ROC Comparison\n",
    "ax7 = plt.subplot(3, 3, 7)\n",
    "fpr_p1, tpr_p1, _ = roc_curve(class_results_phase1['ground_truth'], class_results_phase1['probabilities'])\n",
    "roc_auc_p1 = auc(fpr_p1, tpr_p1)\n",
    "fpr_p2, tpr_p2, _ = roc_curve(class_results_phase2['ground_truth'], class_results_phase2['probabilities'])\n",
    "roc_auc_p2 = auc(fpr_p2, tpr_p2)\n",
    "fpr_p3, tpr_p3, _ = roc_curve(y_true, y_probs)\n",
    "roc_auc_p3 = auc(fpr_p3, tpr_p3)\n",
    "\n",
    "ax7.plot(fpr_p1, tpr_p1, label=f'Phase 1 (AUC={roc_auc_p1:.3f})', lw=2)\n",
    "ax7.plot(fpr_p2, tpr_p2, label=f'Phase 2 (AUC={roc_auc_p2:.3f})', lw=2)\n",
    "ax7.plot(fpr_p3, tpr_p3, label=f'Phase 3 (AUC={roc_auc_p3:.3f})', lw=2)\n",
    "ax7.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "ax7.set_xlabel('False Positive Rate')\n",
    "ax7.set_ylabel('True Positive Rate')\n",
    "ax7.set_title('ROC Curves: All Phases')\n",
    "ax7.legend()\n",
    "ax7.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. Metrics Comparison\n",
    "ax8 = plt.subplot(3, 3, 8)\n",
    "phases = ['Phase 1', 'Phase 2', 'Phase 3']\n",
    "all_results = [class_results_phase1, class_results_phase2, class_results_phase3]\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "for res in all_results:\n",
    "    y_t = res['ground_truth']\n",
    "    y_p = res['predictions']\n",
    "    accuracies.append(accuracy_score(y_t, y_p))\n",
    "    precisions.append(precision_score(y_t, y_p, zero_division=0))\n",
    "    recalls.append(recall_score(y_t, y_p, zero_division=0))\n",
    "    f1_scores.append(f1_score(y_t, y_p, zero_division=0))\n",
    "\n",
    "x = np.arange(len(phases))\n",
    "width = 0.2\n",
    "ax8.bar(x - 1.5*width, accuracies, width, label='Accuracy', alpha=0.8)\n",
    "ax8.bar(x - 0.5*width, precisions, width, label='Precision', alpha=0.8)\n",
    "ax8.bar(x + 0.5*width, recalls, width, label='Recall', alpha=0.8)\n",
    "ax8.bar(x + 1.5*width, f1_scores, width, label='F1-Score', alpha=0.8)\n",
    "ax8.set_xlabel('Phase')\n",
    "ax8.set_ylabel('Score')\n",
    "ax8.set_title('Classification Metrics Comparison')\n",
    "ax8.set_xticks(x)\n",
    "ax8.set_xticklabels(phases)\n",
    "ax8.legend()\n",
    "ax8.set_ylim([0, 1.1])\n",
    "ax8.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 9. Probability Calibration\n",
    "ax9 = plt.subplot(3, 3, 9)\n",
    "n_bins = 10\n",
    "bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "bin_lowers = bin_boundaries[:-1]\n",
    "bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "fraction_of_positives = []\n",
    "mean_predicted_value = []\n",
    "\n",
    "for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "    mask = (y_probs > bin_lower) & (y_probs <= bin_upper)\n",
    "    if mask.sum() > 0:\n",
    "        fraction_of_positives.append(y_true[mask].mean())\n",
    "        mean_predicted_value.append(y_probs[mask].mean())\n",
    "\n",
    "ax9.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "ax9.plot(mean_predicted_value, fraction_of_positives, 's-', label='Model Calibration', markersize=8)\n",
    "ax9.set_xlabel('Mean Predicted Probability')\n",
    "ax9.set_ylabel('Fraction of Positives')\n",
    "ax9.set_title('Calibration Plot')\n",
    "ax9.legend()\n",
    "ax9.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "classification_plot_path = output_dir / 'classification_head_analysis.png'\n",
    "plt.savefig(classification_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "for phase_name, res in zip(['Phase 1: Pan-Cancer', 'Phase 2: Breast Cancer', 'Phase 3: TNBC'], \n",
    "                           [class_results_phase1, class_results_phase2, class_results_phase3]):\n",
    "    y_t = res['ground_truth']\n",
    "    y_p = res['predictions']\n",
    "    y_probs = res['probabilities']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c55da6e",
   "metadata": {},
   "source": [
    "### 7.1 Pan-Cancer vs Fine-tuned Model Comparison\n",
    "\n",
    "Compare Phase 1 (pan-cancer) and Phase 3 (TNBC fine-tuned) models on TNBC test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "splits = load_saved_splits(SPLIT_TYPE)\n",
    "pan_train_idx = splits['pan_cancer']['train']\n",
    "pan_val_idx = splits['pan_cancer']['val']\n",
    "pan_test_idx = splits['pan_cancer']['test']\n",
    "breast_train_idx = splits['breast_cancer']['train']\n",
    "breast_val_idx = splits['breast_cancer']['val']\n",
    "breast_test_idx = splits['breast_cancer']['test']\n",
    "tnbc_train_idx = splits['tnbc']['train']\n",
    "tnbc_val_idx = splits['tnbc']['val']\n",
    "tnbc_test_idx = splits['tnbc']['test']\n",
    "\n",
    "models_dir = output_dir / \"models\"\n",
    "split_type = SPLIT_TYPE if 'SPLIT_TYPE' in globals() else 'cell_line'\n",
    "\n",
    "# Use cellsplit models (cell-line split)\n",
    "phase1_checkpoint = models_dir / \"trial2_phase1_pathway_cellsplit.pt\"\n",
    "phase3_checkpoint = models_dir / \"trial2_phase3_tnbc_pathway_cellsplit.pt\"\n",
    "\n",
    "phase1_on_tnbc = evaluate_phase_pathway(phase1_checkpoint, tnbc_loaders['test'], \"Phase 1 (Pan-Cancer) on TNBC\", device)\n",
    "phase3_on_tnbc = evaluate_phase_pathway(phase3_checkpoint, tnbc_loaders['test'], \"Phase 3 (TNBC Fine-tuned) on TNBC\", device)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Phase 1: Pan-Cancer', 'Phase 3: TNBC Fine-tuned', 'Improvement'],\n",
    "    'Pearson': [\n",
    "        phase1_on_tnbc['metrics']['pearson'],\n",
    "        phase3_on_tnbc['metrics']['pearson'],\n",
    "        phase3_on_tnbc['metrics']['pearson'] - phase1_on_tnbc['metrics']['pearson']\n",
    "    ],\n",
    "    'Spearman': [\n",
    "        phase1_on_tnbc['metrics']['spearman'],\n",
    "        phase3_on_tnbc['metrics']['spearman'],\n",
    "        phase3_on_tnbc['metrics']['spearman'] - phase1_on_tnbc['metrics']['spearman']\n",
    "    ],\n",
    "    'R': [\n",
    "        phase1_on_tnbc['metrics']['r2'],\n",
    "        phase3_on_tnbc['metrics']['r2'],\n",
    "        phase3_on_tnbc['metrics']['r2'] - phase1_on_tnbc['metrics']['r2']\n",
    "    ],\n",
    "    'RMSE': [\n",
    "        phase1_on_tnbc['metrics']['rmse'],\n",
    "        phase3_on_tnbc['metrics']['rmse'],\n",
    "        phase1_on_tnbc['metrics']['rmse'] - phase3_on_tnbc['metrics']['rmse']  # Lower is better\n",
    "    ],\n",
    "    'MAE': [\n",
    "        phase1_on_tnbc['metrics']['mae'],\n",
    "        phase3_on_tnbc['metrics']['mae'],\n",
    "        phase1_on_tnbc['metrics']['mae'] - phase3_on_tnbc['metrics']['mae']  # Lower is better\n",
    "    ]\n",
    "})\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. Metrics Comparison Bar Chart\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "metrics = ['Pearson', 'Spearman', 'R']\n",
    "metric_keys = {'Pearson': 'pearson', 'Spearman': 'spearman', 'R': 'r2'}\n",
    "phase1_vals = [phase1_on_tnbc['metrics'][metric_keys[m]] for m in metrics]\n",
    "phase3_vals = [phase3_on_tnbc['metrics'][metric_keys[m]] for m in metrics]\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, phase1_vals, width, label='Phase 1: Pan-Cancer', alpha=0.8, color='black')\n",
    "ax1.bar(x + width/2, phase3_vals, width, label='Phase 3: TNBC Fine-tuned', alpha=0.8, color='black')\n",
    "ax1.set_xlabel('Metric')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Performance Comparison: Correlation Metrics')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(metrics)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.set_ylim([0, 1.0])\n",
    "\n",
    "# 2. Error Metrics Comparison\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "error_metrics = ['RMSE', 'MAE']\n",
    "phase1_errors = [phase1_on_tnbc['metrics'][m.lower()] for m in error_metrics]\n",
    "phase3_errors = [phase3_on_tnbc['metrics'][m.lower()] for m in error_metrics]\n",
    "x2 = np.arange(len(error_metrics))\n",
    "ax2.bar(x2 - width/2, phase1_errors, width, label='Phase 1: Pan-Cancer', alpha=0.8, color='black')\n",
    "ax2.bar(x2 + width/2, phase3_errors, width, label='Phase 3: TNBC Fine-tuned', alpha=0.8, color='black')\n",
    "ax2.set_xlabel('Metric')\n",
    "ax2.set_ylabel('Error')\n",
    "ax2.set_title('Performance Comparison: Error Metrics')\n",
    "ax2.set_xticks(x2)\n",
    "ax2.set_xticklabels(error_metrics)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Scatter Plot: Phase 1 Predictions vs True\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "preds_p1 = phase1_on_tnbc['predictions']\n",
    "targets = phase1_on_tnbc['targets']\n",
    "ax3.scatter(targets, preds_p1, alpha=0.7, s=20, color='black', edgecolors='black', linewidth=0.3)\n",
    "min_val = min(targets.min(), preds_p1.min())\n",
    "max_val = max(targets.max(), preds_p1.max())\n",
    "ax3.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "ax3.set_xlabel('True IC50 (LN_IC50)')\n",
    "ax3.set_ylabel('Predicted IC50 (LN_IC50)')\n",
    "ax3.set_title(f'Phase 1: Pan-Cancer\\nPearson={phase1_on_tnbc[\"metrics\"][\"pearson\"]:.4f}, R={phase1_on_tnbc[\"metrics\"][\"r2\"]:.4f}')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Scatter Plot: Phase 3 Predictions vs True\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "preds_p3 = phase3_on_tnbc['predictions']\n",
    "ax4.scatter(targets, preds_p3, alpha=0.7, s=20, color='black', edgecolors='black', linewidth=0.3)\n",
    "min_val = min(targets.min(), preds_p3.min())\n",
    "max_val = max(targets.max(), preds_p3.max())\n",
    "ax4.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect Prediction')\n",
    "ax4.set_xlabel('True IC50 (LN_IC50)')\n",
    "ax4.set_ylabel('Predicted IC50 (LN_IC50)')\n",
    "ax4.set_title(f'Phase 3: TNBC Fine-tuned\\nPearson={phase3_on_tnbc[\"metrics\"][\"pearson\"]:.4f}, R={phase3_on_tnbc[\"metrics\"][\"r2\"]:.4f}')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Residual Comparison\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "residuals_p1 = targets - preds_p1\n",
    "residuals_p3 = targets - preds_p3\n",
    "ax5.hist(residuals_p1, bins=50, alpha=0.7, label='Phase 1: Pan-Cancer', color='black', density=True)\n",
    "ax5.hist(residuals_p3, bins=50, alpha=0.7, label='Phase 3: TNBC Fine-tuned', color='black', density=True)\n",
    "ax5.axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "ax5.set_xlabel('Residual (True - Predicted)')\n",
    "ax5.set_ylabel('Density')\n",
    "ax5.set_title('Residual Distribution Comparison')\n",
    "ax5.legend()\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Improvement Summary\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "ax6.axis('off')\n",
    "improvement_text = f\"\"\"\n",
    "Performance Improvement from Fine-tuning:\n",
    "\n",
    "Pearson Correlation: {phase3_on_tnbc['metrics']['pearson'] - phase1_on_tnbc['metrics']['pearson']:+.4f}\n",
    "Spearman Correlation: {phase3_on_tnbc['metrics']['spearman'] - phase1_on_tnbc['metrics']['spearman']:+.4f}\n",
    "R Score: {phase3_on_tnbc['metrics']['r2'] - phase1_on_tnbc['metrics']['r2']:+.4f}\n",
    "RMSE Reduction: {phase1_on_tnbc['metrics']['rmse'] - phase3_on_tnbc['metrics']['rmse']:+.4f}\n",
    "MAE Reduction: {phase1_on_tnbc['metrics']['mae'] - phase3_on_tnbc['metrics']['mae']:+.4f}\n",
    "\n",
    "Test Samples: {len(targets)}\n",
    "\"\"\"\n",
    "ax6.text(0.1, 0.5, improvement_text, fontsize=12, family='monospace', verticalalignment='center',\n",
    "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "ax6.set_title('Fine-tuning Improvement Summary', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_plot_path = output_dir / 'pan_cancer_vs_tnbc_comparison.png'\n",
    "plt.savefig(comparison_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "comparison_csv_path = output_dir / f'pan_cancer_vs_tnbc_comparison_{split_type}.csv'\n",
    "comparison_df.to_csv(comparison_csv_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d6153d",
   "metadata": {},
   "source": [
    "### 7.2 Classification Performance Comparison\n",
    "\n",
    "Compare binary classification performance between pan-cancer and fine-tuned models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8d2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Classification Performance: Pan-Cancer Model vs Fine-tuned TNBC Model on TNBC Test Data\n",
    "\n",
    "\n",
    "\n",
    "splits = load_saved_splits(SPLIT_TYPE)\n",
    "pan_train_idx = splits['pan_cancer']['train']\n",
    "pan_val_idx = splits['pan_cancer']['val']\n",
    "pan_test_idx = splits['pan_cancer']['test']\n",
    "breast_train_idx = splits['breast_cancer']['train']\n",
    "breast_val_idx = splits['breast_cancer']['val']\n",
    "breast_test_idx = splits['breast_cancer']['test']\n",
    "tnbc_train_idx = splits['tnbc']['train']\n",
    "tnbc_val_idx = splits['tnbc']['val']\n",
    "tnbc_test_idx = splits['tnbc']['test']\n",
    "\n",
    "models_dir = output_dir / \"models\"\n",
    "split_type = SPLIT_TYPE if 'SPLIT_TYPE' in globals() else 'cell_line'\n",
    "\n",
    "# Use cellsplit models (cell-line split)\n",
    "phase1_checkpoint = models_dir / \"trial2_phase1_pathway_cellsplit.pt\"\n",
    "phase3_checkpoint = models_dir / \"trial2_phase3_tnbc_pathway_cellsplit.pt\"\n",
    "\n",
    "\n",
    "if 'tnbc_loaders' not in globals():\n",
    "    if 'tnbc_test_idx' not in globals():\n",
    "        pass\n",
    "\n",
    "# Compute median_ic50 from TNBC test data to ensure fair comparison\n",
    "all_tnbc_ic50s = []\n",
    "for batch in tnbc_loaders['test']:\n",
    "    all_tnbc_ic50s.append(batch['ic50'].cpu().numpy())\n",
    "tnbc_test_median_ic50 = np.median(np.concatenate(all_tnbc_ic50s))\n",
    "\n",
    "# Evaluate Phase 1 (Pan-Cancer) classification head on TNBC test data\n",
    "class_phase1_on_tnbc = evaluate_classification_head(phase1_checkpoint, tnbc_loaders['test'], device, \"Phase 1 (Pan-Cancer) on TNBC\", median_ic50=tnbc_test_median_ic50)\n",
    "\n",
    "# Evaluate Phase 3 (TNBC Fine-tuned) classification head on TNBC test data\n",
    "class_phase3_on_tnbc = evaluate_classification_head(phase3_checkpoint, tnbc_loaders['test'], device, \"Phase 3 (TNBC Fine-tuned) on TNBC\", median_ic50=tnbc_test_median_ic50)\n",
    "\n",
    "# Calculate classification metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "y_true = class_phase1_on_tnbc['ground_truth']\n",
    "y_pred_p1 = class_phase1_on_tnbc['predictions']\n",
    "y_probs_p1 = class_phase1_on_tnbc['probabilities']\n",
    "y_pred_p3 = class_phase3_on_tnbc['predictions']\n",
    "y_probs_p3 = class_phase3_on_tnbc['probabilities']\n",
    "\n",
    "metrics_p1 = {\n",
    "    'accuracy': accuracy_score(y_true, y_pred_p1),\n",
    "    'precision': precision_score(y_true, y_pred_p1, zero_division=0),\n",
    "    'recall': recall_score(y_true, y_pred_p1, zero_division=0),\n",
    "    'f1': f1_score(y_true, y_pred_p1, zero_division=0),\n",
    "    'roc_auc': roc_auc_score(y_true, y_probs_p1)\n",
    "}\n",
    "\n",
    "metrics_p3 = {\n",
    "    'accuracy': accuracy_score(y_true, y_pred_p3),\n",
    "    'precision': precision_score(y_true, y_pred_p3, zero_division=0),\n",
    "    'recall': recall_score(y_true, y_pred_p3, zero_division=0),\n",
    "    'f1': f1_score(y_true, y_pred_p3, zero_division=0),\n",
    "    'roc_auc': roc_auc_score(y_true, y_probs_p3)\n",
    "}\n",
    "\n",
    "comparison_class_df = pd.DataFrame({\n",
    "    'Model': ['Phase 1: Pan-Cancer', 'Phase 3: TNBC Fine-tuned', 'Improvement'],\n",
    "    'Accuracy': [\n",
    "        metrics_p1['accuracy'],\n",
    "        metrics_p3['accuracy'],\n",
    "        metrics_p3['accuracy'] - metrics_p1['accuracy']\n",
    "    ],\n",
    "    'Precision': [\n",
    "        metrics_p1['precision'],\n",
    "        metrics_p3['precision'],\n",
    "        metrics_p3['precision'] - metrics_p1['precision']\n",
    "    ],\n",
    "    'Recall': [\n",
    "        metrics_p1['recall'],\n",
    "        metrics_p3['recall'],\n",
    "        metrics_p3['recall'] - metrics_p1['recall']\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        metrics_p1['f1'],\n",
    "        metrics_p3['f1'],\n",
    "        metrics_p3['f1'] - metrics_p1['f1']\n",
    "    ],\n",
    "    'ROC-AUC': [\n",
    "        metrics_p1['roc_auc'],\n",
    "        metrics_p3['roc_auc'],\n",
    "        metrics_p3['roc_auc'] - metrics_p1['roc_auc']\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "# 1. Classification Metrics Comparison Bar Chart\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "class_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "metric_keys = {'Accuracy': 'accuracy', 'Precision': 'precision', 'Recall': 'recall', 'F1-Score': 'f1', 'ROC-AUC': 'roc_auc'}\n",
    "phase1_class_vals = [metrics_p1[metric_keys[m]] for m in class_metrics]\n",
    "phase3_class_vals = [metrics_p3[metric_keys[m]] for m in class_metrics]\n",
    "x = np.arange(len(class_metrics))\n",
    "width = 0.35\n",
    "ax1.bar(x - width/2, phase1_class_vals, width, label='Phase 1: Pan-Cancer', alpha=0.8, color='black')\n",
    "ax1.bar(x + width/2, phase3_class_vals, width, label='Phase 3: TNBC Fine-tuned', alpha=0.8, color='black')\n",
    "ax1.set_xlabel('Metric')\n",
    "ax1.set_ylabel('Score')\n",
    "ax1.set_title('Classification Metrics Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(class_metrics, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.set_ylim([0, 1.1])\n",
    "\n",
    "# 2. ROC Curve Comparison\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "fpr_p1, tpr_p1, _ = roc_curve(y_true, y_probs_p1)\n",
    "roc_auc_p1 = auc(fpr_p1, tpr_p1)\n",
    "fpr_p3, tpr_p3, _ = roc_curve(y_true, y_probs_p3)\n",
    "roc_auc_p3 = auc(fpr_p3, tpr_p3)\n",
    "\n",
    "ax2.plot(fpr_p1, tpr_p1, label=f'Phase 1 (AUC={roc_auc_p1:.3f})', lw=2, color='black')\n",
    "ax2.plot(fpr_p3, tpr_p3, label=f'Phase 3 (AUC={roc_auc_p3:.3f})', lw=2, color='black')\n",
    "ax2.plot([0, 1], [0, 1], 'k--', label='Random', lw=1)\n",
    "ax2.set_xlabel('False Positive Rate')\n",
    "ax2.set_ylabel('True Positive Rate')\n",
    "ax2.set_title('ROC Curves Comparison')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Precision-Recall Curve Comparison\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "precision_p1, recall_p1, _ = precision_recall_curve(y_true, y_probs_p1)\n",
    "pr_auc_p1 = auc(recall_p1, precision_p1)\n",
    "precision_p3, recall_p3, _ = precision_recall_curve(y_true, y_probs_p3)\n",
    "pr_auc_p3 = auc(recall_p3, precision_p3)\n",
    "\n",
    "baseline = np.sum(y_true) / len(y_true)\n",
    "ax3.plot(recall_p1, precision_p1, label=f'Phase 1 (AUC={pr_auc_p1:.3f})', lw=2, color='black')\n",
    "ax3.plot(recall_p3, precision_p3, label=f'Phase 3 (AUC={pr_auc_p3:.3f})', lw=2, color='black')\n",
    "ax3.axhline(y=baseline, color='black', linestyle='--', label=f'Baseline={baseline:.3f}')\n",
    "ax3.set_xlabel('Recall')\n",
    "ax3.set_ylabel('Precision')\n",
    "ax3.set_title('Precision-Recall Curves Comparison')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Confusion Matrix: Phase 1\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "cm_p1 = confusion_matrix(y_true, y_pred_p1)\n",
    "sns.heatmap(cm_p1, annot=True, fmt='d', cmap='gray', ax=ax4, cbar_kws={'label': 'Count'})\n",
    "ax4.set_xlabel('Predicted (0=Sensitive, 1=Resistant)')\n",
    "ax4.set_ylabel('True (0=Sensitive, 1=Resistant)')\n",
    "ax4.set_title(f'Phase 1: Pan-Cancer\\nAccuracy={metrics_p1[\"accuracy\"]:.4f}')\n",
    "\n",
    "# 5. Confusion Matrix: Phase 3\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "cm_p3 = confusion_matrix(y_true, y_pred_p3)\n",
    "sns.heatmap(cm_p3, annot=True, fmt='d', cmap='Greens', ax=ax5, cbar_kws={'label': 'Count'})\n",
    "ax5.set_xlabel('Predicted (0=Sensitive, 1=Resistant)')\n",
    "ax5.set_ylabel('True (0=Sensitive, 1=Resistant)')\n",
    "ax5.set_title(f'Phase 3: TNBC Fine-tuned\\nAccuracy={metrics_p3[\"accuracy\"]:.4f}')\n",
    "\n",
    "# 6. Probability Distribution Comparison\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "sensitive_probs_p1 = y_probs_p1[y_true == 0]\n",
    "resistant_probs_p1 = y_probs_p1[y_true == 1]\n",
    "sensitive_probs_p3 = y_probs_p3[y_true == 0]\n",
    "resistant_probs_p3 = y_probs_p3[y_true == 1]\n",
    "\n",
    "ax6.hist(sensitive_probs_p1, bins=30, alpha=0.4, label='Phase 1 Sensitive', color='lightblue', density=True)\n",
    "ax6.hist(resistant_probs_p1, bins=30, alpha=0.4, label='Phase 1 Resistant', color='lightcoral', density=True)\n",
    "ax6.hist(sensitive_probs_p3, bins=30, alpha=0.4, label='Phase 3 Sensitive', color='lightgreen', density=True, histtype='step', linewidth=2)\n",
    "ax6.hist(resistant_probs_p3, bins=30, alpha=0.4, label='Phase 3 Resistant', color='black', density=True, histtype='step', linewidth=2)\n",
    "ax6.axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "ax6.set_xlabel('Predicted Probability')\n",
    "ax6.set_ylabel('Density')\n",
    "ax6.set_title('Probability Distribution Comparison')\n",
    "ax6.legend(fontsize=8)\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "comparison_class_plot_path = output_dir / 'classification_pan_cancer_vs_tnbc_comparison.png'\n",
    "plt.savefig(comparison_class_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig2 = plt.figure(figsize=(7, 4.67))\n",
    "\n",
    "# Improvement metrics\n",
    "ax_imp = plt.subplot(2, 2, 1)\n",
    "improvements = {\n",
    "    'Accuracy': metrics_p3['accuracy'] - metrics_p1['accuracy'],\n",
    "    'Precision': metrics_p3['precision'] - metrics_p1['precision'],\n",
    "    'Recall': metrics_p3['recall'] - metrics_p1['recall'],\n",
    "    'F1-Score': metrics_p3['f1'] - metrics_p1['f1'],\n",
    "    'ROC-AUC': metrics_p3['roc_auc'] - metrics_p1['roc_auc']\n",
    "}\n",
    "colors = ['green' if v > 0 else 'red' for v in improvements.values()]\n",
    "ax_imp.barh(list(improvements.keys()), list(improvements.values()), color=colors, alpha=0.7)\n",
    "ax_imp.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "ax_imp.set_xlabel('Improvement')\n",
    "ax_imp.set_title('Classification Performance Improvement\\nfrom Fine-tuning')\n",
    "ax_imp.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Percentage improvement\n",
    "ax_pct = plt.subplot(2, 2, 2)\n",
    "pct_improvements = {\n",
    "    'Accuracy': (metrics_p3['accuracy'] - metrics_p1['accuracy']) / metrics_p1['accuracy'] * 100 if metrics_p1['accuracy'] > 0 else 0,\n",
    "    'Precision': (metrics_p3['precision'] - metrics_p1['precision']) / metrics_p1['precision'] * 100 if metrics_p1['precision'] > 0 else 0,\n",
    "    'Recall': (metrics_p3['recall'] - metrics_p1['recall']) / metrics_p1['recall'] * 100 if metrics_p1['recall'] > 0 else 0,\n",
    "    'F1-Score': (metrics_p3['f1'] - metrics_p1['f1']) / metrics_p1['f1'] * 100 if metrics_p1['f1'] > 0 else 0,\n",
    "    'ROC-AUC': (metrics_p3['roc_auc'] - metrics_p1['roc_auc']) / metrics_p1['roc_auc'] * 100 if metrics_p1['roc_auc'] > 0 else 0\n",
    "}\n",
    "colors_pct = ['green' if v > 0 else 'red' for v in pct_improvements.values()]\n",
    "ax_pct.barh(list(pct_improvements.keys()), list(pct_improvements.values()), color=colors_pct, alpha=0.7)\n",
    "ax_pct.axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "ax_pct.set_xlabel('Percentage Improvement (%)')\n",
    "ax_pct.set_title('Percentage Improvement from Fine-tuning')\n",
    "ax_pct.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Summary text\n",
    "ax_summary = plt.subplot(2, 2, (3, 4))\n",
    "ax_summary.axis('off')\n",
    "summary_text = f\"\"\"\n",
    "Classification Performance Improvement Summary:\n",
    "\n",
    "Absolute Improvements:\n",
    "  Accuracy:  {improvements['Accuracy']:+.4f} ({pct_improvements['Accuracy']:+.2f}%)\n",
    "  Precision: {improvements['Precision']:+.4f} ({pct_improvements['Precision']:+.2f}%)\n",
    "  Recall:    {improvements['Recall']:+.4f} ({pct_improvements['Recall']:+.2f}%)\n",
    "  F1-Score:  {improvements['F1-Score']:+.4f} ({pct_improvements['F1-Score']:+.2f}%)\n",
    "  ROC-AUC:   {improvements['ROC-AUC']:+.4f} ({pct_improvements['ROC-AUC']:+.2f}%)\n",
    "\n",
    "Test Samples: {len(y_true)}\n",
    "  Sensitive (IC50  median): {np.sum(y_true == 0)}\n",
    "  Resistant (IC50 > median): {np.sum(y_true == 1)}\n",
    "\n",
    "Phase 1 Performance:\n",
    "  Accuracy:  {metrics_p1['accuracy']:.4f}\n",
    "  Precision: {metrics_p1['precision']:.4f}\n",
    "  Recall:    {metrics_p1['recall']:.4f}\n",
    "  F1-Score:  {metrics_p1['f1']:.4f}\n",
    "  ROC-AUC:   {metrics_p1['roc_auc']:.4f}\n",
    "\n",
    "Phase 3 Performance:\n",
    "  Accuracy:  {metrics_p3['accuracy']:.4f}\n",
    "  Precision: {metrics_p3['precision']:.4f}\n",
    "  Recall:    {metrics_p3['recall']:.4f}\n",
    "  F1-Score:  {metrics_p3['f1']:.4f}\n",
    "  ROC-AUC:   {metrics_p3['roc_auc']:.4f}\n",
    "\"\"\"\n",
    "ax_summary.text(0.05, 0.5, summary_text, fontsize=11, family='monospace', verticalalignment='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "improvement_plot_path = output_dir / 'classification_improvement_summary.png'\n",
    "plt.savefig(improvement_plot_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "comparison_class_csv_path = output_dir / f'classification_pan_cancer_vs_tnbc_comparison_{split_type}.csv'\n",
    "comparison_class_df.to_csv(comparison_class_csv_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d557b47",
   "metadata": {},
   "source": [
    "## 8. Interpretability Analysis\n",
    "\n",
    "### 8.1 SHAP Analysis: Pathway Importance\n",
    "\n",
    "Use SHAP (SHapley Additive exPlanations) to identify which pathways are most important for drug response prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ee62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Analysis: Pathway Importance per Drug\n",
    "\n",
    "from collections import defaultdict\n",
    "xs = True\n",
    "\n",
    "if xs == True:\n",
    "    \n",
    "    def create_pathway_predictor(model, drug_graph, drug_name, pathway_cols, normalization_stats=None):\n",
    "        \"\"\"\n",
    "        Create a wrapper function for SHAP that fixes the drug and varies pathways.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained DrugResponsePathwayGNN model\n",
    "            drug_graph: Fixed drug graph (PyTorch Geometric Data object)\n",
    "            drug_name: Name of the drug (for reference)\n",
    "            pathway_cols: List of pathway column names\n",
    "            normalization_stats: Dict with 'means' and 'stds' for normalization (optional)\n",
    "            \n",
    "        Returns:\n",
    "            Function that takes pathway scores array and returns IC50 predictions\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        drug_graph = drug_graph.to(device)\n",
    "        \n",
    "        drug_batch = Batch.from_data_list([drug_graph]).to(device)\n",
    "        \n",
    "        def pathway_predictor(pathway_scores_array):\n",
    "            \"\"\"\n",
    "            Predict IC50 given pathway scores.\n",
    "            \n",
    "            Args:\n",
    "                pathway_scores_array: numpy array of shape (n_samples, n_pathways)\n",
    "                \n",
    "            Returns:\n",
    "                numpy array of IC50 predictions (n_samples,)\n",
    "            \"\"\"\n",
    "            # Convert to tensor\n",
    "            if isinstance(pathway_scores_array, np.ndarray):\n",
    "                pathway_tensor = torch.FloatTensor(pathway_scores_array).to(device)\n",
    "            else:\n",
    "                pathway_tensor = pathway_scores_array.to(device)\n",
    "            \n",
    "            if normalization_stats is not None:\n",
    "                means = torch.FloatTensor(normalization_stats['means']).to(device)\n",
    "                stds = torch.FloatTensor(normalization_stats['stds']).to(device)\n",
    "                pathway_tensor = (pathway_tensor - means) / stds\n",
    "            \n",
    "            # Expand drug batch to match batch size\n",
    "            batch_size = pathway_tensor.shape[0]\n",
    "            expanded_drug_batch = Batch.from_data_list([drug_graph] * batch_size).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(expanded_drug_batch, pathway_tensor)\n",
    "                predictions = outputs['ic50'].cpu().numpy().flatten()\n",
    "            \n",
    "            return predictions\n",
    "        \n",
    "        return pathway_predictor\n",
    "    \n",
    "    def compute_shap_for_drug(model, drug_name, drug_graph, test_data, pathway_cols, \n",
    "                              normalization_stats=None, n_samples=100, n_background=50):\n",
    "        \"\"\"\n",
    "        Compute SHAP values for a specific drug.\n",
    "        \n",
    "        Args:\n",
    "            model: Trained model\n",
    "            drug_name: Name of the drug\n",
    "            drug_graph: Drug graph (PyTorch Geometric Data)\n",
    "            test_data: DataFrame with test samples (filtered for this drug)\n",
    "            pathway_cols: List of pathway column names\n",
    "            normalization_stats: Normalization statistics\n",
    "            n_samples: Number of samples to explain\n",
    "            n_background: Number of background samples for SHAP\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with SHAP values and pathway names\n",
    "        \"\"\"\n",
    "        if len(test_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Sample test data\n",
    "        if len(test_data) > n_samples:\n",
    "            test_sample = test_data.sample(n=min(n_samples, len(test_data)), random_state=42)\n",
    "        else:\n",
    "            test_sample = test_data.copy()\n",
    "        \n",
    "        X_test = test_sample[pathway_cols].values.astype(np.float32)\n",
    "        \n",
    "        if SPLIT_TYPE == 'cell_line':\n",
    "            background_data = pan_cancer_pathway.loc[pan_train_idx]\n",
    "        else:\n",
    "            background_data = pan_cancer_pathway.loc[pan_train_idx] if len(pan_train_idx) > 0 else pan_cancer_pathway\n",
    "        \n",
    "        drug_background = background_data[background_data['DrugName'] == drug_name]\n",
    "        if len(drug_background) == 0:\n",
    "            drug_background = background_data.sample(n=min(n_background, len(background_data)), random_state=42)\n",
    "        else:\n",
    "            drug_background = drug_background.sample(n=min(n_background, len(drug_background)), random_state=42)\n",
    "        \n",
    "        X_background = drug_background[pathway_cols].values.astype(np.float32)\n",
    "        \n",
    "        predictor = create_pathway_predictor(model, drug_graph, drug_name, pathway_cols, normalization_stats)\n",
    "        \n",
    "        # Compute SHAP values using KernelExplainer\n",
    "        explainer = shap.KernelExplainer(predictor, X_background)\n",
    "        shap_values = explainer.shap_values(X_test, nsamples=min(100, len(X_test)))\n",
    "        \n",
    "        mean_shap = np.abs(shap_values).mean(axis=0)\n",
    "        \n",
    "        results = {\n",
    "            'drug_name': drug_name,\n",
    "            'shap_values': shap_values,\n",
    "            'mean_abs_shap': mean_shap,\n",
    "            'pathway_names': pathway_cols,\n",
    "            'test_samples': len(test_sample),\n",
    "            'background_samples': len(X_background)\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "\n",
    "    # Define checkpoint paths - use cellsplit models for cell_line split\n",
    "    models_dir = output_dir / \"models\"\n",
    "    split_type = SPLIT_TYPE if 'SPLIT_TYPE' in globals() else 'cell_line'\n",
    "    \n",
    "    # For cell_line split, use cellsplit models (old naming convention)\n",
    "    if split_type == 'cell_line':\n",
    "        phase2_checkpoint = models_dir / \"trial2_phase2_breast_pathway_cellsplit.pt\"\n",
    "        phase3_checkpoint = models_dir / \"trial2_phase3_tnbc_pathway_cellsplit.pt\"\n",
    "    else:\n",
    "        phase2_checkpoint = models_dir / f\"trial2_phase2_breast_pathway_{split_type}.pt\"\n",
    "        phase3_checkpoint = models_dir / f\"trial2_phase3_tnbc_pathway_{split_type}.pt\"\n",
    "    \n",
    "    \n",
    "    if not phase3_checkpoint.exists():\n",
    "        # Try cellsplit as fallback\n",
    "        fallback_checkpoint = models_dir / \"trial2_phase3_tnbc_pathway_cellsplit.pt\"\n",
    "        if fallback_checkpoint.exists():\n",
    "            phase3_checkpoint = fallback_checkpoint\n",
    "    \n",
    "    shap_pathway_count = actual_pathway_count if 'actual_pathway_count' in globals() else len(pathway_cols)\n",
    "    model_shap = DrugResponsePathwayGNN(cell_input_dim=shap_pathway_count).to(device)\n",
    "    if phase3_checkpoint.exists():\n",
    "        checkpoint = torch.load(phase3_checkpoint, map_location=device, weights_only=False)\n",
    "        model_shap.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        if phase2_checkpoint.exists():\n",
    "            checkpoint = torch.load(phase2_checkpoint, map_location=device, weights_only=False)\n",
    "            model_shap.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Compute normalization stats from training data\n",
    "    if SPLIT_TYPE == 'cell_line':\n",
    "        train_data = pan_cancer_pathway.loc[pan_train_idx]\n",
    "    else:\n",
    "        train_data = pan_cancer_pathway.loc[pan_train_idx] if len(pan_train_idx) > 0 else pan_cancer_pathway\n",
    "    \n",
    "    normalization_stats = {\n",
    "        'means': train_data[pathway_cols].values.mean(axis=0).astype(np.float32),\n",
    "        'stds': train_data[pathway_cols].values.std(axis=0).astype(np.float32) + 1e-8\n",
    "    }\n",
    "    \n",
    "    if phase3_results['n_samples'] > 0:\n",
    "        if SPLIT_TYPE == 'cell_line':\n",
    "            test_data_full = tnbc_pathway.loc[tnbc_test_idx] if len(tnbc_test_idx) > 0 else tnbc_pathway\n",
    "        else:\n",
    "            test_data_full = tnbc_pathway.loc[tnbc_test_idx] if len(tnbc_test_idx) > 0 else tnbc_pathway\n",
    "    else:\n",
    "        if SPLIT_TYPE == 'cell_line':\n",
    "            test_data_full = breast_cancer_pathway.loc[breast_test_idx] if len(breast_test_idx) > 0 else breast_cancer_pathway\n",
    "        else:\n",
    "            test_data_full = breast_cancer_pathway.loc[breast_test_idx] if len(breast_test_idx) > 0 else breast_cancer_pathway\n",
    "    \n",
    "    unique_drugs = test_data_full['DrugName'].unique()\n",
    "    \n",
    "    # Define high-priority drugs for analysis\n",
    "    high_priority_drugs = [\n",
    "        'Paclitaxel',     # Microtubule - should show mitotic pathways\n",
    "        'Cisplatin',      # DNA crosslinker - should show DNA repair\n",
    "        'Olaparib',       # PARP inhibitor - should show DNA repair\n",
    "        'Trametinib',     # MEK inhibitor - should show MAPK/ERK\n",
    "        'Lapatinib',      # EGFR/HER2 - should show EGFR signaling\n",
    "    ]\n",
    "    \n",
    "    # Prioritize high-priority drugs, then analyze all remaining drugs\n",
    "    priority_drugs_in_set = [drug for drug in high_priority_drugs if drug in unique_drugs]\n",
    "    other_drugs = [drug for drug in unique_drugs if drug not in high_priority_drugs]\n",
    "    \n",
    "    # Combine: high-priority first, then all others\n",
    "    unique_drugs = priority_drugs_in_set + other_drugs\n",
    "    \n",
    "    if priority_drugs_in_set:\n",
    "        pass\n",
    "    \n",
    "    # Compute SHAP values for each drug\n",
    "    \n",
    "    drug_shap_results = {}\n",
    "    \n",
    "    for drug_name in tqdm(unique_drugs, desc=\"Processing drugs\"):\n",
    "        # Filter test data for this drug\n",
    "        drug_test_data = test_data_full[test_data_full['DrugName'] == drug_name].copy()\n",
    "        \n",
    "        if len(drug_test_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        if drug_name not in drug_graphs:\n",
    "            continue\n",
    "        \n",
    "        drug_graph_data = drug_graphs[drug_name]['graph_data']\n",
    "        \n",
    "        # Compute SHAP values\n",
    "        try:\n",
    "            shap_result = compute_shap_for_drug(\n",
    "                model_shap, drug_name, drug_graph_data, drug_test_data,\n",
    "                pathway_cols, normalization_stats, n_samples=50, n_background=50\n",
    "            )\n",
    "            \n",
    "            if shap_result is not None:\n",
    "                drug_shap_results[drug_name] = shap_result\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    # Analyze and visualize results\n",
    "    pathway_importance_aggregate = defaultdict(list)\n",
    "    \n",
    "    for drug_name, result in drug_shap_results.items():\n",
    "        mean_shap = result['mean_abs_shap']\n",
    "        pathway_names = result['pathway_names']\n",
    "        \n",
    "        for pathway, importance in zip(pathway_names, mean_shap):\n",
    "            pathway_importance_aggregate[pathway].append(importance)\n",
    "    \n",
    "    # Compute mean importance per pathway\n",
    "    pathway_mean_importance = {\n",
    "        pathway: np.mean(importances) \n",
    "        for pathway, importances in pathway_importance_aggregate.items()\n",
    "    }\n",
    "    \n",
    "    # Sort by mean importance\n",
    "    sorted_pathways = sorted(pathway_mean_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i, (pathway, importance) in enumerate(sorted_pathways[:20], 1):\n",
    "        n_drugs = len(pathway_importance_aggregate[pathway])\n",
    "    \n",
    "    shap_results_path = output_dir / \"shap_analysis_results.pkl\"\n",
    "    with open(shap_results_path, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            'drug_shap_results': drug_shap_results,\n",
    "            'pathway_mean_importance': pathway_mean_importance,\n",
    "            'normalization_stats': normalization_stats\n",
    "        }, f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Select top pathways and drugs for visualization\n",
    "    top_pathways_viz = [p[0] for p in sorted_pathways[:15]]\n",
    "    top_drugs_viz = list(drug_shap_results.keys())[:15]\n",
    "    \n",
    "    heatmap_data = []\n",
    "    for drug_name in top_drugs_viz:\n",
    "        if drug_name in drug_shap_results:\n",
    "            result = drug_shap_results[drug_name]\n",
    "            pathway_names = result['pathway_names']\n",
    "            mean_shap = result['mean_abs_shap']\n",
    "            \n",
    "            row = []\n",
    "            for pathway in top_pathways_viz:\n",
    "                if pathway in pathway_names:\n",
    "                    idx = pathway_names.index(pathway)\n",
    "                    row.append(mean_shap[idx])\n",
    "                else:\n",
    "                    row.append(0.0)\n",
    "            heatmap_data.append(row)\n",
    "    \n",
    "    if len(heatmap_data) > 0:\n",
    "        heatmap_array = np.array(heatmap_data)\n",
    "        \n",
    "        plt.figure(figsize=(7, 4.375))\n",
    "        plt.imshow(heatmap_array, aspect='auto', cmap='gray', interpolation='nearest')\n",
    "        plt.colorbar(label='Mean |SHAP| Value')\n",
    "        plt.xlabel('Pathway', fontsize=10)\n",
    "        plt.ylabel('Drug', fontsize=10)\n",
    "        plt.title('SHAP Analysis: Pathway Importance per Drug\\n(Top 15 Pathways  Top 15 Drugs)', fontsize=14, fontweight='bold')\n",
    "        plt.xticks(range(len(top_pathways_viz)), top_pathways_viz, rotation=45, ha='right', fontsize=8)\n",
    "        plt.yticks(range(len(top_drugs_viz)), top_drugs_viz, fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        heatmap_path = output_dir / \"shap_heatmap.png\"\n",
    "        plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5ed1d",
   "metadata": {},
   "source": [
    "## 9. Results Summary\n",
    "\n",
    "Save all results, metrics, splits, and metadata for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e7998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numpy(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: convert_numpy(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "splits_summary = {\n",
    "    'split_type': SPLIT_TYPE,\n",
    "    'pan_cancer': {\n",
    "        'train_samples': len(pan_train_idx),\n",
    "        'val_samples': len(pan_val_idx),\n",
    "        'test_samples': len(pan_test_idx),\n",
    "        'train_cell_lines': len(set(pan_cancer_pathway.loc[pan_train_idx]['ModelID'].unique())),\n",
    "        'val_cell_lines': len(set(pan_cancer_pathway.loc[pan_val_idx]['ModelID'].unique())),\n",
    "        'test_cell_lines': len(set(pan_cancer_pathway.loc[pan_test_idx]['ModelID'].unique()))\n",
    "    },\n",
    "    'breast_cancer': {\n",
    "        'train_samples': len(breast_train_idx),\n",
    "        'val_samples': len(breast_val_idx),\n",
    "        'test_samples': len(breast_test_idx),\n",
    "        'train_cell_lines': len(set(breast_cancer_pathway.loc[breast_train_idx]['ModelID'].unique())),\n",
    "        'val_cell_lines': len(set(breast_cancer_pathway.loc[breast_val_idx]['ModelID'].unique())),\n",
    "        'test_cell_lines': len(set(breast_cancer_pathway.loc[breast_test_idx]['ModelID'].unique()))\n",
    "    },\n",
    "    'tnbc': {\n",
    "        'train_samples': len(tnbc_train_idx),\n",
    "        'val_samples': len(tnbc_val_idx),\n",
    "        'test_samples': len(tnbc_test_idx),\n",
    "        'train_cell_lines': len(set(tnbc_pathway.loc[tnbc_train_idx]['ModelID'].unique())),\n",
    "        'val_cell_lines': len(set(tnbc_pathway.loc[tnbc_val_idx]['ModelID'].unique())),\n",
    "        'test_cell_lines': len(set(tnbc_pathway.loc[tnbc_test_idx]['ModelID'].unique()))\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "splits_summary_path = output_dir / f\"data_splits_summary_{SPLIT_TYPE}.json\"\n",
    "with open(splits_summary_path, 'w') as f:\n",
    "    json.dump(splits_summary, f, indent=2)\n",
    "\n",
    "cv_summary = {}\n",
    "if 'phase1_cv_results' in globals() and phase1_cv_results is not None:\n",
    "    cv_summary['phase1'] = {\n",
    "        'n_folds': len(phase1_cv_results.get('cv_results', [])),\n",
    "        'best_fold': phase1_cv_results.get('best_fold', 1),\n",
    "        'aggregated_metrics': phase1_cv_results.get('aggregated', {}),\n",
    "        'fold_results': [\n",
    "            {\n",
    "                'fold': i+1,\n",
    "                'val_r2': float(r['val_metrics']['r2']),\n",
    "                'val_pearson': float(r['val_metrics']['pearson']),\n",
    "                'val_rmse': float(r['val_metrics']['rmse'])\n",
    "            }\n",
    "            for i, r in enumerate(phase1_cv_results.get('cv_results', []))\n",
    "        ]\n",
    "    }\n",
    "\n",
    "if 'phase2_cv_results' in globals() and phase2_cv_results is not None:\n",
    "    cv_summary['phase2'] = {\n",
    "        'n_folds': len(phase2_cv_results.get('cv_results', [])),\n",
    "        'best_fold': phase2_cv_results.get('best_fold', 1),\n",
    "        'aggregated_metrics': phase2_cv_results.get('aggregated', {}),\n",
    "        'fold_results': [\n",
    "            {\n",
    "                'fold': i+1,\n",
    "                'val_r2': float(r['val_metrics']['r2']),\n",
    "                'val_pearson': float(r['val_metrics']['pearson']),\n",
    "                'val_rmse': float(r['val_metrics']['rmse'])\n",
    "            }\n",
    "            for i, r in enumerate(phase2_cv_results.get('cv_results', []))\n",
    "        ]\n",
    "    }\n",
    "\n",
    "if cv_summary:\n",
    "    cv_summary_path = output_dir / f\"cv_results_summary_{SPLIT_TYPE}.json\"\n",
    "    cv_summary_serializable = convert_numpy(cv_summary)\n",
    "    with open(cv_summary_path, 'w') as f:\n",
    "        json.dump(cv_summary_serializable, f, indent=2)\n",
    "\n",
    "checkpoints_info = {\n",
    "    'phase1': {\n",
    "        'checkpoint_path': str(phase1_checkpoint),\n",
    "        'exists': phase1_checkpoint.exists(),\n",
    "        'cv_best_checkpoint': str(phase1_cv_results['best_checkpoint']) if 'phase1_cv_results' in globals() and phase1_cv_results is not None else None\n",
    "    },\n",
    "    'phase2': {\n",
    "        'checkpoint_path': str(phase2_checkpoint),\n",
    "        'exists': phase2_checkpoint.exists(),\n",
    "        'cv_best_checkpoint': str(phase2_cv_results['best_checkpoint']) if 'phase2_cv_results' in globals() and phase2_cv_results is not None else None\n",
    "    },\n",
    "    'phase3': {\n",
    "        'checkpoint_path': str(phase3_checkpoint),\n",
    "        'exists': phase3_checkpoint.exists()\n",
    "    },\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "checkpoints_info_path = output_dir / f\"model_checkpoints_info_{SPLIT_TYPE}.json\"\n",
    "with open(checkpoints_info_path, 'w') as f:\n",
    "    json.dump(checkpoints_info, f, indent=2)\n",
    "\n",
    "comprehensive_summary = {\n",
    "    'experiment_info': {\n",
    "        'split_type': SPLIT_TYPE,\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'device': str(device),\n",
    "        'pathway_count': int(actual_pathway_count)\n",
    "    },\n",
    "    'data_splits': splits_summary,\n",
    "    'regression_metrics': {\n",
    "        'phase1': {\n",
    "            'r2': float(phase1_results['metrics']['r2']),\n",
    "            'pearson': float(phase1_results['metrics']['pearson']),\n",
    "            'spearman': float(phase1_results['metrics']['spearman']),\n",
    "            'rmse': float(phase1_results['metrics']['rmse']),\n",
    "            'mae': float(phase1_results['metrics']['mae'])\n",
    "        },\n",
    "        'phase2': {\n",
    "            'r2': float(phase2_results['metrics']['r2']),\n",
    "            'pearson': float(phase2_results['metrics']['pearson']),\n",
    "            'spearman': float(phase2_results['metrics']['spearman']),\n",
    "            'rmse': float(phase2_results['metrics']['rmse']),\n",
    "            'mae': float(phase2_results['metrics']['mae'])\n",
    "        },\n",
    "        'phase3': {\n",
    "            'r2': float(phase3_results['metrics']['r2']),\n",
    "            'pearson': float(phase3_results['metrics']['pearson']),\n",
    "            'spearman': float(phase3_results['metrics']['spearman']),\n",
    "            'rmse': float(phase3_results['metrics']['rmse']),\n",
    "            'mae': float(phase3_results['metrics']['mae'])\n",
    "        }\n",
    "    },\n",
    "    'classification_metrics': classification_metrics if 'classification_metrics' in globals() else {},\n",
    "    'cv_summary': cv_summary,\n",
    "    'files_saved': {\n",
    "        'splits': str(splits_file),\n",
    "        'performance_csv': str(performance_csv_path),\n",
    "        'performance_pkl': str(performance_pkl_path),\n",
    "        'classification_csv': str(classification_csv_path) if 'classification_csv_path' in globals() else None,\n",
    "        'classification_pkl': str(classification_pkl_path) if 'classification_pkl_path' in globals() else None,\n",
    "        'figures': [\n",
    "            'classification_head_analysis.png',\n",
    "            'pan_cancer_vs_tnbc_comparison.png',\n",
    "            'classification_pan_cancer_vs_tnbc_comparison.png',\n",
    "            'classification_improvement_summary.png'\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "if 'comparison_df' in globals():\n",
    "    comprehensive_summary['comparison_regression'] = comparison_df.to_dict('records')\n",
    "if 'comparison_class_df' in globals():\n",
    "    comprehensive_summary['comparison_classification'] = comparison_class_df.to_dict('records')\n",
    "\n",
    "shap_data = None\n",
    "if 'drug_shap_results' in globals():\n",
    "    shap_data = drug_shap_results\n",
    "elif 'shap_results_dict' in globals():\n",
    "    shap_data = shap_results_dict\n",
    "\n",
    "if shap_data is not None:\n",
    "    shap_summary = {}\n",
    "    for drug_name, result in shap_data.items():\n",
    "        if isinstance(result, dict) and 'mean_abs_shap' in result:\n",
    "            top_indices = np.argsort(result['mean_abs_shap'])[-10:][::-1]\n",
    "            shap_summary[drug_name] = {\n",
    "                'top_pathways': [result['pathway_names'][i] for i in top_indices],\n",
    "                'top_shap_values': [float(result['mean_abs_shap'][i]) for i in top_indices],\n",
    "                'n_test_samples': int(result.get('test_samples', 0)),\n",
    "                'n_background_samples': int(result.get('background_samples', 0))\n",
    "            }\n",
    "    \n",
    "    if shap_summary:\n",
    "        comprehensive_summary['shap_analysis'] = shap_summary\n",
    "        comprehensive_summary['files_saved']['shap_results'] = 'shap_analysis_results.pkl'\n",
    "\n",
    "comprehensive_summary_path = output_dir / f\"comprehensive_results_summary_{SPLIT_TYPE}.json\"\n",
    "comprehensive_summary_serializable = convert_numpy(comprehensive_summary)\n",
    "with open(comprehensive_summary_path, 'w') as f:\n",
    "    json.dump(comprehensive_summary_serializable, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
