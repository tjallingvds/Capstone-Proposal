{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a1b2c3d4",
      "metadata": {},
      "source": [
        "# TNBC Drug Cytotoxicity Prediction Model\n",
        "## Comprehensive Data Preparation Pipeline\n",
        "\n",
        "This notebook creates hierarchical datasets for GNN-based drug response prediction:\n",
        "\n",
        "**Pipeline Steps:**\n",
        "1. Load cancer cell line data (DepMap, GDSC2, gene expression)\n",
        "2. Identify BL1 TNBC cell lines using molecular criteria\n",
        "3. Load/fetch SMILES structures for all drugs\n",
        "4. Filter GDSC2 data for quality (RMSE < 0.3)\n",
        "5. Merge drug response with SMILES and gene expression\n",
        "6. Apply quality control filters\n",
        "7. Create tissue-specific subsets:\n",
        "   - Pan-cancer (all cell lines) → `pan_cancer_data.pkl`\n",
        "   - Breast cancer only → `breast_cancer_data.pkl`\n",
        "   - TNBC subset → `tnbc_data.pkl`\n",
        "   - BL1 TNBC subset → `bl1_tnbc_data.pkl`\n",
        "8. Generate summary report\n",
        "\n",
        "**Training Strategy:**\n",
        "Train on pan-cancer → Fine-tune on breast → Fine-tune on TNBC → Fine-tune on BL1 TNBC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "fd7f5f66",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import time\n",
        "import pubchempy as pcp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.data import Data, Batch\n",
        "from torch_geometric.nn import TransformerConv, global_mean_pool, global_max_pool\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data directory\n",
        "data_dir = Path(\"/Users/tjalling/Desktop/Dev./Capstone/Datasets/Cytotoxicity Model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "setup_cell",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "\n",
        "model_df = pd.read_csv(data_dir / \"DepMap Model Data.csv\")\n",
        "expression_df = pd.read_csv(data_dir / \"Omics Expression TPM Logp1 Human Protein Coding Genes.csv\")\n",
        "mutations_df = pd.read_csv(data_dir / \"Omics Somatic Mutations.csv\")\n",
        "cn_df = pd.read_csv(data_dir / \"Omics CN Gene WGS Data.csv\")\n",
        "compounds_df = pd.read_csv(data_dir / \"Screened Compounds v8.5.csv\")\n",
        "gdsc2_df = pd.read_excel(data_dir / \"GDSC2 Fitted Dose Response Oct 27 2023.xlsx\")\n",
        "\n",
        "print(f\"Loaded {len(model_df)} cell lines, {len(compounds_df)} compounds, {len(gdsc2_df)} drug responses\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bl1_cell",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BL1 TNBC Cell Line Identification\n",
            "================================================================================\n",
            "Breast cancer lines: 96\n",
            "TP53 mutated lines: 1187\n",
            "High proliferation lines: 439\n",
            "MYC amplified lines: 49\n",
            "\n",
            "Final BL1 lines: 23 (Criteria: 19, Reference: 8)\n",
            "================================================================================\n",
            "  ACH-000017           SKBR3                [criteria]\n",
            "  ACH-000111           HCC1187              [criteria]\n",
            "  ACH-000117           EFM192A              [criteria]\n",
            "  ACH-000196           HCC1599              [criteria]\n",
            "  ACH-000223           HCC1937              [criteria, reference]\n",
            "  ACH-000248           AU565                [criteria]\n",
            "  ACH-000277           HCC1419              [criteria]\n",
            "  ACH-000288           BT549                [criteria, reference]\n",
            "  ACH-000374           HCC1143              [reference]\n",
            "  ACH-000536           BT20                 [criteria]\n",
            "  ACH-000621           MDAMB157             [criteria]\n",
            "  ACH-000624           HCC1806              [reference]\n",
            "  ACH-000668           HCC70                [criteria, reference]\n",
            "  ACH-000699           HCC1395              [criteria]\n",
            "  ACH-000711           JIMT1                [criteria]\n",
            "  ACH-000768           MDAMB231             [criteria]\n",
            "  ACH-000849           MDAMB468             [criteria, reference]\n",
            "  ACH-000856           CAL51                [reference]\n",
            "  ACH-000859           HCC1954              [criteria]\n",
            "  ACH-000902           CAL148               [criteria]\n",
            "  ACH-000930           HCC1569              [criteria]\n",
            "  ACH-001390           SUM149PT             [reference]\n",
            "  ACH-001820           COLO824              [criteria]\n"
          ]
        }
      ],
      "source": [
        "# BL1 Cell Line Identification Functions\n",
        "\n",
        "def find_gene_columns(df, gene_symbols):\n",
        "    \"\"\"Find columns matching gene symbols (handles 'SYMBOL (EntrezID)' format).\"\"\"\n",
        "    found = {}\n",
        "    for symbol in gene_symbols:\n",
        "        matches = [col for col in df.columns if col.startswith(f\"{symbol} (\")]\n",
        "        if matches:\n",
        "            found[symbol] = matches[0]\n",
        "    return found\n",
        "\n",
        "def normalize_cell_line_name(name):\n",
        "    \"\"\"Normalize cell line name for matching.\"\"\"\n",
        "    return str(name).replace('-', '').replace('_', '').upper()\n",
        "\n",
        "def identify_bl1_cell_lines(model_df, mutations_df, expression_df, cn_df):\n",
        "    \"\"\"\n",
        "    Identify BL1 TNBC cell lines using molecular criteria.\n",
        "    \n",
        "    Criteria: Breast + TP53 mutation + (High proliferation OR MYC amplification)\n",
        "    Plus known reference lines: HCC1143, HCC1937, MDAMB468, HCC70, HCC1806, CAL51, BT549, SUM149PT\n",
        "    \"\"\"\n",
        "    print(\"BL1 TNBC Cell Line Identification\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    known_bl1_names = ['HCC1143', 'HCC1937', 'MDAMB468', 'HCC70', 'HCC1806', 'CAL51', 'BT549', 'SUM149PT']\n",
        "    \n",
        "    # Filter for breast cancer lines\n",
        "    breast_lines = model_df[model_df['OncotreeLineage'] == 'Breast']['ModelID'].values\n",
        "    print(f\"Breast cancer lines: {len(breast_lines)}\")\n",
        "    \n",
        "    # TP53 mutations (non-silent)\n",
        "    tp53_mutations = mutations_df[\n",
        "        (mutations_df['HugoSymbol'] == 'TP53') & \n",
        "        (mutations_df['VariantType'] != 'Silent')\n",
        "    ]['ModelID'].unique()\n",
        "    print(f\"TP53 mutated lines: {len(tp53_mutations)}\")\n",
        "    \n",
        "    # High proliferation (top 25%)\n",
        "    prolif_genes = ['MKI67', 'CCNB1', 'CCNE1']\n",
        "    prolif_cols = find_gene_columns(expression_df, prolif_genes)\n",
        "    \n",
        "    if len(prolif_cols) > 0:\n",
        "        gene_col_names = list(prolif_cols.values())\n",
        "        prolif_data = expression_df[['ModelID'] + gene_col_names].copy()\n",
        "        prolif_data['prolif_score'] = prolif_data[gene_col_names].mean(axis=1)\n",
        "        threshold = prolif_data['prolif_score'].quantile(0.75)\n",
        "        high_prolif_lines = prolif_data[prolif_data['prolif_score'] >= threshold]['ModelID'].values\n",
        "        print(f\"High proliferation lines: {len(high_prolif_lines)}\")\n",
        "    else:\n",
        "        high_prolif_lines = []\n",
        "    \n",
        "    # MYC amplification (CN > 6)\n",
        "    myc_cols = find_gene_columns(cn_df, ['MYC', 'MYCN'])\n",
        "    if myc_cols:\n",
        "        myc_col = myc_cols.get('MYC') or myc_cols.get('MYCN')\n",
        "        myc_amp_lines = cn_df[cn_df[myc_col] > 6]['ModelID'].values\n",
        "        print(f\"MYC amplified lines: {len(myc_amp_lines)}\")\n",
        "    else:\n",
        "        myc_amp_lines = []\n",
        "    \n",
        "    # Combine criteria\n",
        "    criteria_bl1 = set(breast_lines) & set(tp53_mutations) & (set(high_prolif_lines) | set(myc_amp_lines))\n",
        "    \n",
        "    # Find known reference lines\n",
        "    known_bl1_present = set()\n",
        "    cell_name_col = 'StrippedCellLineName' if 'StrippedCellLineName' in model_df.columns else 'ModelID'\n",
        "    \n",
        "    for ref_name in known_bl1_names:\n",
        "        normalized_ref = normalize_cell_line_name(ref_name)\n",
        "        matching = model_df[\n",
        "            model_df[cell_name_col].apply(lambda x: normalize_cell_line_name(x) == normalized_ref)\n",
        "        ]['ModelID'].values\n",
        "        if len(matching) > 0:\n",
        "            known_bl1_present.update(matching)\n",
        "    \n",
        "    # Final BL1 list\n",
        "    final_bl1 = criteria_bl1 | known_bl1_present\n",
        "    \n",
        "    print(f\"\\nFinal BL1 lines: {len(final_bl1)} (Criteria: {len(criteria_bl1)}, Reference: {len(known_bl1_present)})\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    # Display\n",
        "    for model_id in sorted(final_bl1):\n",
        "        cell_name = model_df[model_df['ModelID'] == model_id][cell_name_col].values\n",
        "        name = cell_name[0] if len(cell_name) > 0 else 'Unknown'\n",
        "        source = []\n",
        "        if model_id in criteria_bl1:\n",
        "            source.append(\"criteria\")\n",
        "        if model_id in known_bl1_present:\n",
        "            source.append(\"reference\")\n",
        "        print(f\"  {model_id:20s} {name:20s} [{', '.join(source)}]\")\n",
        "    \n",
        "    return sorted(list(final_bl1))\n",
        "\n",
        "# Run identification\n",
        "bl1_model_ids = identify_bl1_cell_lines(model_df, mutations_df, expression_df, cn_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "training_cell",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Dataset: 5311 pairs | 286 drugs | 20 cell lines\n"
          ]
        }
      ],
      "source": [
        "# Create Training Dataset\n",
        "\n",
        "# Map BL1 ModelIDs to COSMIC IDs\n",
        "bl1_cosmic_map = model_df[model_df['ModelID'].isin(bl1_model_ids)][\n",
        "    ['ModelID', 'COSMICID', 'StrippedCellLineName']\n",
        "].copy()\n",
        "\n",
        "# Filter GDSC2 for BL1 cell lines\n",
        "cell_id_col = 'COSMIC_ID' if 'COSMIC_ID' in gdsc2_df.columns else 'COSMICID'\n",
        "bl1_cosmic_ids = bl1_cosmic_map['COSMICID'].values\n",
        "gdsc2_bl1 = gdsc2_df[gdsc2_df[cell_id_col].isin(bl1_cosmic_ids)].copy()\n",
        "\n",
        "# Remove missing IC50\n",
        "ln_ic50_col = [col for col in gdsc2_df.columns if 'LN_IC50' in col.upper()][0]\n",
        "gdsc2_bl1 = gdsc2_bl1[gdsc2_bl1[ln_ic50_col].notna()]\n",
        "\n",
        "# Merge with cell line info\n",
        "training_data = gdsc2_bl1.merge(\n",
        "    bl1_cosmic_map,\n",
        "    left_on=cell_id_col,\n",
        "    right_on='COSMICID',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "drug_col = [col for col in training_data.columns if 'DRUG' in col.upper() and 'NAME' in col.upper()][0]\n",
        "\n",
        "print(f\"Training Dataset: {len(training_data)} pairs | {training_data[drug_col].nunique()} drugs | {training_data['ModelID'].nunique()} cell lines\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "smiles_cell",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drugs with valid SMILES: 228\n"
          ]
        }
      ],
      "source": [
        "# Load SMILES structures (use existing file if available)\n",
        "\n",
        "smiles_file = data_dir / 'drugs_with_smiles.csv'\n",
        "\n",
        "if smiles_file.exists():\n",
        "    drugs_with_smiles = pd.read_csv(smiles_file)\n",
        "else:\n",
        "    def get_smiles_pubchempy(drug_name):\n",
        "        \"\"\"\n",
        "        Get SMILES string for a drug using PubChemPy.\n",
        "        \n",
        "        Args:\n",
        "            drug_name: Drug name to search\n",
        "            \n",
        "        Returns:\n",
        "            SMILES string or None\n",
        "        \"\"\"\n",
        "        try:\n",
        "            compounds = pcp.get_compounds(drug_name, 'name')\n",
        "            if compounds:\n",
        "                return compounds[0].isomeric_smiles\n",
        "            return None\n",
        "        except Exception:\n",
        "            return None\n",
        "    \n",
        "    unique_drugs = gdsc2_df[drug_col].unique()\n",
        "    drugs_df = pd.DataFrame({drug_col: unique_drugs})\n",
        "    \n",
        "    smiles_list = []\n",
        "    for drug_name in drugs_df[drug_col]:\n",
        "        smiles_list.append(get_smiles_pubchempy(drug_name))\n",
        "        time.sleep(0.2)\n",
        "    \n",
        "    drugs_df['SMILES'] = smiles_list\n",
        "    drugs_df.to_csv(smiles_file, index=False)\n",
        "    drugs_with_smiles = drugs_df\n",
        "\n",
        "drugs_with_smiles = drugs_with_smiles[drugs_with_smiles['SMILES'].notna()].copy()\n",
        "print(f\"Drugs with valid SMILES: {len(drugs_with_smiles)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "failed_cell",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GDSC2 quality filter: 242036 → 242036 measurements (RMSE < 0.3)\n"
          ]
        }
      ],
      "source": [
        "# Filter GDSC2 data for quality (RMSE < 0.3)\n",
        "\n",
        "def filter_gdsc2_quality(gdsc2_df, rmse_threshold=0.3):\n",
        "    \"\"\"\n",
        "    Filter GDSC2 data for good curve fits.\n",
        "    \n",
        "    Args:\n",
        "        gdsc2_df: GDSC2 dataframe\n",
        "        rmse_threshold: Maximum RMSE to keep\n",
        "        \n",
        "    Returns:\n",
        "        Filtered dataframe\n",
        "    \"\"\"\n",
        "    rmse_col = [col for col in gdsc2_df.columns if 'RMSE' in col.upper()]\n",
        "    if rmse_col:\n",
        "        initial_count = len(gdsc2_df)\n",
        "        gdsc2_filtered = gdsc2_df[gdsc2_df[rmse_col[0]] < rmse_threshold].copy()\n",
        "        print(f\"GDSC2 quality filter: {initial_count} → {len(gdsc2_filtered)} measurements (RMSE < {rmse_threshold})\")\n",
        "        return gdsc2_filtered\n",
        "    return gdsc2_df.copy()\n",
        "\n",
        "gdsc2_filtered = filter_gdsc2_quality(gdsc2_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea61aee4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drug-cell pairs with SMILES: 199501\n",
            "Unique drugs: 228\n",
            "Unique cell lines: 969\n"
          ]
        }
      ],
      "source": [
        "# Merge GDSC2 drug response with SMILES structures\n",
        "\n",
        "# Identify column names\n",
        "ln_ic50_col = [col for col in gdsc2_filtered.columns if 'LN_IC50' in col.upper()][0]\n",
        "auc_col = [col for col in gdsc2_filtered.columns if 'AUC' in col.upper()][0]\n",
        "drug_name_col = [col for col in gdsc2_filtered.columns if 'DRUG' in col.upper() and 'NAME' in col.upper()][0]\n",
        "cosmic_id_col = [col for col in gdsc2_filtered.columns if 'COSMIC' in col.upper()][0]\n",
        "\n",
        "# Merge on drug name\n",
        "response_with_smiles = gdsc2_filtered.merge(\n",
        "    drugs_with_smiles,\n",
        "    left_on=drug_name_col,\n",
        "    right_on=drugs_with_smiles.columns[0],\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# Remove rows with missing IC50\n",
        "response_with_smiles = response_with_smiles[response_with_smiles[ln_ic50_col].notna()].copy()\n",
        "\n",
        "print(f\"Drug-cell pairs with SMILES: {len(response_with_smiles)}\")\n",
        "print(f\"Unique drugs: {response_with_smiles[drug_name_col].nunique()}\")\n",
        "print(f\"Unique cell lines: {response_with_smiles[cosmic_id_col].nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9affe11d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cell lines with expression data and COSMIC ID: 715\n"
          ]
        }
      ],
      "source": [
        "# Create mapping between COSMIC_ID and ModelID\n",
        "\n",
        "def create_id_mapping(model_df, expression_df):\n",
        "    \"\"\"\n",
        "    Create mapping between COSMIC_ID and ModelID for cell lines with expression data.\n",
        "    \n",
        "    Args:\n",
        "        model_df: Model metadata\n",
        "        expression_df: Expression data\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with COSMICID and ModelID mapping\n",
        "    \"\"\"\n",
        "    # Get cell lines that have expression data\n",
        "    expression_model_ids = set(expression_df['ModelID'])\n",
        "    \n",
        "    # Create mapping\n",
        "    id_mapping = model_df[model_df['ModelID'].isin(expression_model_ids)][\n",
        "        ['ModelID', 'COSMICID', 'StrippedCellLineName', 'OncotreeLineage', 'OncotreePrimaryDisease']\n",
        "    ].copy()\n",
        "    \n",
        "    # Remove rows without COSMICID\n",
        "    id_mapping = id_mapping[id_mapping['COSMICID'].notna()].copy()\n",
        "    \n",
        "    return id_mapping\n",
        "\n",
        "id_mapping = create_id_mapping(model_df, expression_df)\n",
        "print(f\"Cell lines with expression data and COSMIC ID: {len(id_mapping)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b92a32ea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After ID mapping: 144921 pairs | 698 cell lines | 228 drugs\n"
          ]
        }
      ],
      "source": [
        "# Merge response with cell line IDs (without expression data yet)\n",
        "\n",
        "# Step 1: Merge response with ID mapping to get ModelIDs\n",
        "response_with_ids = response_with_smiles.merge(\n",
        "    id_mapping,\n",
        "    left_on=cosmic_id_col,\n",
        "    right_on='COSMICID',\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "print(f\"After ID mapping: {len(response_with_ids)} pairs | {response_with_ids['ModelID'].nunique()} cell lines | {response_with_ids[drug_name_col].nunique()} drugs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47cf793c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quality filter: 144921 → 144921 pairs, 228 drugs\n"
          ]
        }
      ],
      "source": [
        "# Apply quality filters before merging\n",
        "\n",
        "def apply_quality_filters_pre_merge(response_data, drug_col, min_tests_per_drug=10):\n",
        "    \"\"\"\n",
        "    Filter drugs tested on too few cell lines.\n",
        "    \n",
        "    Args:\n",
        "        response_data: Response data with ModelIDs\n",
        "        drug_col: Drug name column\n",
        "        min_tests_per_drug: Minimum tests required per drug\n",
        "        \n",
        "    Returns:\n",
        "        Filtered dataset\n",
        "    \"\"\"\n",
        "    drug_counts = response_data.groupby(drug_col).size()\n",
        "    valid_drugs = drug_counts[drug_counts >= min_tests_per_drug].index\n",
        "    data_clean = response_data[response_data[drug_col].isin(valid_drugs)].copy()\n",
        "    \n",
        "    print(f\"Quality filter: {len(response_data)} → {len(data_clean)} pairs, {data_clean[drug_col].nunique()} drugs\")\n",
        "    \n",
        "    return data_clean\n",
        "\n",
        "response_clean = apply_quality_filters_pre_merge(response_with_ids, drug_name_col)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4def890d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing expression data...\n",
            "Starting: 718 cell lines × 19216 genes\n",
            "After removing genes with >1.0% missing: 19216 genes\n",
            "Selected top 3000 highest variance genes\n",
            "Final: 718 cell lines × 3000 genes (NaNs imputed)\n",
            "\n",
            "Merging response with expression...\n",
            "Pan-cancer dataset: 149093 pairs | 698 cell lines | 228 drugs\n"
          ]
        }
      ],
      "source": [
        "# Preprocess expression data (quality + dimensionality reduction)\n",
        "\n",
        "def preprocess_expression_data(expression_df, response_clean, max_missing_pct=0.01, top_n_genes=3000):\n",
        "    \"\"\"\n",
        "    Clean and reduce expression data before merging.\n",
        "    \n",
        "    Args:\n",
        "        expression_df: Full gene expression data\n",
        "        response_clean: Response data (to filter cell lines)\n",
        "        max_missing_pct: Maximum missing % per gene (default 1%)\n",
        "        top_n_genes: Number of top variance genes to keep\n",
        "        \n",
        "    Returns:\n",
        "        Preprocessed expression dataframe\n",
        "    \"\"\"\n",
        "    # Filter to only needed cell lines\n",
        "    needed_model_ids = response_clean['ModelID'].unique()\n",
        "    expr_subset = expression_df[expression_df['ModelID'].isin(needed_model_ids)].copy()\n",
        "    \n",
        "    # Get gene columns (numeric columns only, excluding ModelID)\n",
        "    gene_cols = expr_subset.select_dtypes(include=['number']).columns.tolist()\n",
        "    \n",
        "    print(f\"Starting: {len(expr_subset)} cell lines × {len(gene_cols)} genes\")\n",
        "    \n",
        "    # Remove genes with >1% missing values\n",
        "    missing_pct = expr_subset[gene_cols].isna().mean()\n",
        "    valid_genes = missing_pct[missing_pct <= max_missing_pct].index.tolist()\n",
        "    print(f\"After removing genes with >{max_missing_pct*100}% missing: {len(valid_genes)} genes\")\n",
        "    \n",
        "    # Select top variance genes\n",
        "    gene_variances = expr_subset[valid_genes].var()\n",
        "    top_genes = gene_variances.nlargest(min(top_n_genes, len(valid_genes))).index.tolist()\n",
        "    print(f\"Selected top {len(top_genes)} highest variance genes\")\n",
        "    \n",
        "    # Create filtered dataset\n",
        "    expr_filtered = expr_subset[['ModelID'] + top_genes].copy()\n",
        "    \n",
        "    # Impute remaining NaNs with gene median\n",
        "    for gene in top_genes:\n",
        "        if expr_filtered[gene].isna().any():\n",
        "            expr_filtered[gene].fillna(expr_filtered[gene].median(), inplace=True)\n",
        "    \n",
        "    print(f\"Final: {len(expr_filtered)} cell lines × {len(top_genes)} genes (NaNs imputed)\")\n",
        "    \n",
        "    return expr_filtered\n",
        "\n",
        "print(\"Preprocessing expression data...\")\n",
        "expression_clean = preprocess_expression_data(expression_df, response_clean)\n",
        "\n",
        "# Merge with preprocessed expression\n",
        "print(\"\\nMerging response with expression...\")\n",
        "pan_cancer_data = response_clean.merge(\n",
        "    expression_clean,\n",
        "    on='ModelID',\n",
        "    how='inner'\n",
        ")\n",
        "\n",
        "# Organize columns\n",
        "meta_cols = ['COSMICID', 'ModelID', 'StrippedCellLineName', 'OncotreeLineage', \n",
        "             'OncotreePrimaryDisease', drug_name_col, 'SMILES', ln_ic50_col, auc_col]\n",
        "gene_cols = [col for col in expression_clean.columns if col != 'ModelID']\n",
        "pan_cancer_data = pan_cancer_data[meta_cols + gene_cols].copy()\n",
        "\n",
        "print(f\"Pan-cancer dataset: {len(pan_cancer_data)} pairs | {pan_cancer_data['ModelID'].nunique()} cell lines | {pan_cancer_data[drug_name_col].nunique()} drugs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71be37af",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Breast: 9601 pairs, 45 cell lines\n",
            "TNBC: 9601 pairs, 45 cell lines\n",
            "BL1 TNBC: 4303 pairs, 20 cell lines\n"
          ]
        }
      ],
      "source": [
        "# Create tissue-specific subsets\n",
        "\n",
        "def create_tissue_subset(data, tissue_type=None, primary_disease=None):\n",
        "    \"\"\"\n",
        "    Filter data by tissue type and/or primary disease.\n",
        "    \n",
        "    Args:\n",
        "        data: Full dataset\n",
        "        tissue_type: OncotreeLineage value\n",
        "        primary_disease: OncotreePrimaryDisease substring\n",
        "        \n",
        "    Returns:\n",
        "        Filtered subset\n",
        "    \"\"\"\n",
        "    subset = data.copy()\n",
        "    \n",
        "    if tissue_type:\n",
        "        subset = subset[subset['OncotreeLineage'] == tissue_type]\n",
        "    \n",
        "    if primary_disease:\n",
        "        subset = subset[subset['OncotreePrimaryDisease'].str.contains(primary_disease, na=False, case=False)]\n",
        "    \n",
        "    return subset\n",
        "\n",
        "breast_cancer_data = create_tissue_subset(pan_cancer_data, tissue_type='Breast')\n",
        "tnbc_data = create_tissue_subset(breast_cancer_data, primary_disease='Breast')\n",
        "bl1_tnbc_data = tnbc_data[tnbc_data['ModelID'].isin(bl1_model_ids)].copy()\n",
        "\n",
        "print(f\"Breast: {len(breast_cancer_data)} pairs, {breast_cancer_data['ModelID'].nunique()} cell lines\")\n",
        "print(f\"TNBC: {len(tnbc_data)} pairs, {tnbc_data['ModelID'].nunique()} cell lines\")\n",
        "print(f\"BL1 TNBC: {len(bl1_tnbc_data)} pairs, {bl1_tnbc_data['ModelID'].nunique()} cell lines\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23663386",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved pan_cancer_data.pkl: 149093 pairs, 698 cell lines\n",
            "Saved breast_cancer_data.pkl: 9601 pairs, 45 cell lines\n",
            "Saved tnbc_data.pkl: 9601 pairs, 45 cell lines\n",
            "Saved bl1_tnbc_data.pkl: 4303 pairs, 20 cell lines\n"
          ]
        }
      ],
      "source": [
        "# Save processed datasets\n",
        "\n",
        "output_dir = Path(\"/Users/tjalling/Desktop/Dev./Capstone/Model_Notebooks\")\n",
        "\n",
        "datasets = {\n",
        "    'pan_cancer_data.pkl': pan_cancer_data,\n",
        "    'breast_cancer_data.pkl': breast_cancer_data,\n",
        "    'tnbc_data.pkl': tnbc_data,\n",
        "    'bl1_tnbc_data.pkl': bl1_tnbc_data\n",
        "}\n",
        "\n",
        "for filename, dataset in datasets.items():\n",
        "    output_path = output_dir / filename\n",
        "    dataset.to_pickle(output_path)\n",
        "    print(f\"Saved {filename}: {len(dataset)} pairs, {dataset['ModelID'].nunique()} cell lines\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d8f963b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DATA PROCESSING SUMMARY\n",
            "\n",
            "OVERALL STATISTICS:\n",
            "  Total drugs: 228\n",
            "  Total cell lines: 698\n",
            "  Gene features: 2999\n",
            "\n",
            "DATASET STATISTICS:\n",
            "\n",
            "  pan_cancer_data.pkl:\n",
            "    Pairs: 149093\n",
            "    Cell lines: 698\n",
            "    Drugs: 228\n",
            "\n",
            "  breast_cancer_data.pkl:\n",
            "    Pairs: 9601\n",
            "    Cell lines: 45\n",
            "    Drugs: 228\n",
            "\n",
            "  tnbc_data.pkl:\n",
            "    Pairs: 9601\n",
            "    Cell lines: 45\n",
            "    Drugs: 228\n",
            "\n",
            "  bl1_tnbc_data.pkl:\n",
            "    Pairs: 4303\n",
            "    Cell lines: 20\n",
            "    Drugs: 228\n",
            "\n",
            "BL1 TNBC CELL LINES:\n",
            "  ACH-000111: HCC1187 (224 drugs)\n",
            "  ACH-000117: EFM192A (223 drugs)\n",
            "  ACH-000196: HCC1599 (182 drugs)\n",
            "  ACH-000223: HCC1937 (225 drugs)\n",
            "  ACH-000248: AU565 (224 drugs)\n",
            "  ACH-000277: HCC1419 (220 drugs)\n",
            "  ACH-000288: BT549 (226 drugs)\n",
            "  ACH-000374: HCC1143 (224 drugs)\n",
            "  ACH-000536: BT20 (224 drugs)\n",
            "  ACH-000621: MDAMB157 (226 drugs)\n",
            "  ACH-000624: HCC1806 (224 drugs)\n",
            "  ACH-000668: HCC70 (226 drugs)\n",
            "  ACH-000699: HCC1395 (158 drugs)\n",
            "  ACH-000711: JIMT1 (225 drugs)\n",
            "  ACH-000768: MDAMB231 (226 drugs)\n",
            "  ACH-000849: MDAMB468 (225 drugs)\n",
            "  ACH-000856: CAL51 (225 drugs)\n",
            "  ACH-000859: HCC1954 (225 drugs)\n",
            "  ACH-000902: CAL148 (155 drugs)\n",
            "  ACH-000930: HCC1569 (216 drugs)\n"
          ]
        }
      ],
      "source": [
        "# Generate summary report\n",
        "\n",
        "def generate_summary_report(datasets_dict, drug_col, output_file):\n",
        "    \"\"\"\n",
        "    Generate summary report of processed datasets.\n",
        "    \n",
        "    Args:\n",
        "        datasets_dict: Dictionary of dataset names and dataframes\n",
        "        drug_col: Drug name column\n",
        "        output_file: Path to save summary\n",
        "    \"\"\"\n",
        "    summary_lines = []\n",
        "    summary_lines.append(\"DATA PROCESSING SUMMARY\")\n",
        "    summary_lines.append(\"\")\n",
        "    \n",
        "    pan_cancer = datasets_dict['pan_cancer_data.pkl']\n",
        "    summary_lines.append(\"OVERALL STATISTICS:\")\n",
        "    summary_lines.append(f\"  Total drugs: {pan_cancer[drug_col].nunique()}\")\n",
        "    summary_lines.append(f\"  Total cell lines: {pan_cancer['ModelID'].nunique()}\")\n",
        "    summary_lines.append(f\"  Gene features: {len([c for c in pan_cancer.columns if '(' in c and ')' in c])}\")\n",
        "    summary_lines.append(\"\")\n",
        "    \n",
        "    summary_lines.append(\"DATASET STATISTICS:\")\n",
        "    for name, data in datasets_dict.items():\n",
        "        summary_lines.append(f\"\\n  {name}:\")\n",
        "        summary_lines.append(f\"    Pairs: {len(data)}\")\n",
        "        summary_lines.append(f\"    Cell lines: {data['ModelID'].nunique()}\")\n",
        "        summary_lines.append(f\"    Drugs: {data[drug_col].nunique()}\")\n",
        "    \n",
        "    summary_lines.append(\"\")\n",
        "    \n",
        "    bl1_data = datasets_dict['bl1_tnbc_data.pkl']\n",
        "    if len(bl1_data) > 0:\n",
        "        summary_lines.append(\"BL1 TNBC CELL LINES:\")\n",
        "        for model_id in sorted(bl1_data['ModelID'].unique()):\n",
        "            cell_name = bl1_data[bl1_data['ModelID'] == model_id]['StrippedCellLineName'].iloc[0]\n",
        "            n_drugs = len(bl1_data[bl1_data['ModelID'] == model_id])\n",
        "            summary_lines.append(f\"  {model_id}: {cell_name} ({n_drugs} drugs)\")\n",
        "    \n",
        "    with open(output_file, 'w') as f:\n",
        "        f.write('\\n'.join(summary_lines))\n",
        "    \n",
        "    print('\\n'.join(summary_lines))\n",
        "\n",
        "summary_file = output_dir / 'data_summary.txt'\n",
        "generate_summary_report(datasets, drug_name_col, summary_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4ad8997",
      "metadata": {},
      "outputs": [],
      "source": [
        "# SMILES to Graph Conversion\n",
        "\n",
        "def get_atom_features(atom):\n",
        "    \"\"\"\n",
        "    Extract atom features for GNN.\n",
        "    \n",
        "    Args:\n",
        "        atom: RDKit atom object\n",
        "        \n",
        "    Returns:\n",
        "        List of atom features\n",
        "    \"\"\"\n",
        "    features = [\n",
        "        atom.GetAtomicNum(),\n",
        "        atom.GetDegree(),\n",
        "        atom.GetFormalCharge(),\n",
        "        atom.GetHybridization().real,\n",
        "        atom.GetIsAromatic(),\n",
        "        atom.GetTotalNumHs(),\n",
        "        atom.GetNumRadicalElectrons(),\n",
        "        atom.IsInRing(),\n",
        "        atom.GetChiralTag().real,\n",
        "    ]\n",
        "    return features\n",
        "\n",
        "def get_bond_features(bond):\n",
        "    \"\"\"\n",
        "    Extract bond features for GNN.\n",
        "    \n",
        "    Args:\n",
        "        bond: RDKit bond object\n",
        "        \n",
        "    Returns:\n",
        "        List of bond features\n",
        "    \"\"\"\n",
        "    features = [\n",
        "        bond.GetBondTypeAsDouble(),\n",
        "        bond.GetIsConjugated(),\n",
        "        bond.IsInRing(),\n",
        "        bond.GetStereo().real,\n",
        "    ]\n",
        "    return features\n",
        "\n",
        "def smiles_to_graph(smiles_string):\n",
        "    \"\"\"\n",
        "    Convert SMILES string to PyTorch Geometric graph.\n",
        "    \n",
        "    Args:\n",
        "        smiles_string: SMILES representation of molecule\n",
        "        \n",
        "    Returns:\n",
        "        torch_geometric.data.Data object or None if invalid\n",
        "    \"\"\"\n",
        "    try:\n",
        "        mol = Chem.MolFromSmiles(smiles_string)\n",
        "        if mol is None:\n",
        "            return None\n",
        "        \n",
        "        mol = Chem.AddHs(mol)\n",
        "        \n",
        "        # Extract node features\n",
        "        node_features = []\n",
        "        for atom in mol.GetAtoms():\n",
        "            node_features.append(get_atom_features(atom))\n",
        "        \n",
        "        if len(node_features) == 0:\n",
        "            return None\n",
        "            \n",
        "        x = torch.tensor(node_features, dtype=torch.float)\n",
        "        \n",
        "        # Extract edge indices and features\n",
        "        edge_indices = []\n",
        "        edge_features = []\n",
        "        \n",
        "        for bond in mol.GetBonds():\n",
        "            i = bond.GetBeginAtomIdx()\n",
        "            j = bond.GetEndAtomIdx()\n",
        "            \n",
        "            edge_indices.append([i, j])\n",
        "            edge_indices.append([j, i])\n",
        "            \n",
        "            bond_feats = get_bond_features(bond)\n",
        "            edge_features.append(bond_feats)\n",
        "            edge_features.append(bond_feats)\n",
        "        \n",
        "        if len(edge_indices) == 0:\n",
        "            edge_indices = [[0, 0]]\n",
        "            edge_features = [[0.0] * 4]\n",
        "        \n",
        "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
        "        edge_attr = torch.tensor(edge_features, dtype=torch.float)\n",
        "        \n",
        "        return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
        "        \n",
        "    except Exception:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c5c6903",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drug Encoder Architecture\n",
        "\n",
        "class DrugEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Neural Network encoder for molecular SMILES structures.\n",
        "    Uses TransformerConv layers to generate 256-dim drug embeddings.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, node_feature_dim=9, edge_feature_dim=4, hidden_dim=256, dropout=0.3):\n",
        "        \"\"\"\n",
        "        Initialize DrugEncoder.\n",
        "        \n",
        "        Args:\n",
        "            node_feature_dim: Number of atom features\n",
        "            edge_feature_dim: Number of bond features\n",
        "            hidden_dim: Hidden layer dimension (output is 256)\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super(DrugEncoder, self).__init__()\n",
        "        \n",
        "        # Device selection\n",
        "        if torch.backends.mps.is_available():\n",
        "            self.device = torch.device('mps')\n",
        "        elif torch.cuda.is_available():\n",
        "            self.device = torch.device('cuda')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "        \n",
        "        # TransformerConv layers\n",
        "        self.conv1 = TransformerConv(node_feature_dim, 128, heads=4, dropout=dropout, edge_dim=edge_feature_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(128 * 4)\n",
        "        \n",
        "        self.conv2 = TransformerConv(128 * 4, 256, heads=8, dropout=dropout, edge_dim=edge_feature_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(256 * 8)\n",
        "        \n",
        "        self.conv3 = TransformerConv(256 * 8, 256, heads=8, dropout=dropout, edge_dim=edge_feature_dim)\n",
        "        self.bn3 = nn.BatchNorm1d(256 * 8)\n",
        "        \n",
        "        self.conv4 = TransformerConv(256 * 8, 256, heads=4, concat=False, dropout=dropout, edge_dim=edge_feature_dim)\n",
        "        self.bn4 = nn.BatchNorm1d(256)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        # Attention weights for pooling\n",
        "        self.attention_weights = nn.Linear(256, 1)\n",
        "        \n",
        "        self.to(self.device)\n",
        "    \n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "        Forward pass through drug encoder.\n",
        "        \n",
        "        Args:\n",
        "            data: PyTorch Geometric Data/Batch object\n",
        "            \n",
        "        Returns:\n",
        "            Drug embeddings of shape (batch_size, 256)\n",
        "        \"\"\"\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
        "        \n",
        "        x = x.to(self.device, non_blocking=True)\n",
        "        edge_index = edge_index.to(self.device, non_blocking=True)\n",
        "        edge_attr = edge_attr.to(self.device, non_blocking=True)\n",
        "        batch = batch.to(self.device, non_blocking=True)\n",
        "        \n",
        "        # Layer 1\n",
        "        x = self.conv1(x, edge_index, edge_attr)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Layer 2\n",
        "        x = self.conv2(x, edge_index, edge_attr)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Layer 3\n",
        "        x = self.conv3(x, edge_index, edge_attr)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Layer 4\n",
        "        x = self.conv4(x, edge_index, edge_attr)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        # Attention-weighted pooling\n",
        "        attention_scores = self.attention_weights(x)\n",
        "        attention_scores = torch.softmax(attention_scores, dim=0)\n",
        "        \n",
        "        x_weighted = x * attention_scores\n",
        "        x_mean = global_mean_pool(x_weighted, batch)\n",
        "        x_max = global_max_pool(x, batch)\n",
        "        \n",
        "        embedding = (x_mean + x_max) / 2\n",
        "        \n",
        "        return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb2f7eac",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting SMILES:   0%|          | 0/228 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting SMILES: 100%|██████████| 228/228 [00:00<00:00, 363.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted 228/228 drugs to graphs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Preprocess and Cache Drug Graphs\n",
        "\n",
        "def preprocess_drugs(drugs_df, drug_name_col, smiles_col, save_path):\n",
        "    \"\"\"\n",
        "    Convert all drugs to graphs and cache.\n",
        "    \n",
        "    Args:\n",
        "        drugs_df: DataFrame with drug names and SMILES\n",
        "        drug_name_col: Column name for drug names\n",
        "        smiles_col: Column name for SMILES strings\n",
        "        save_path: Path to save cached graphs\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary mapping drug names to graph data\n",
        "    \"\"\"\n",
        "    drug_graphs = {}\n",
        "    failed_drugs = []\n",
        "    \n",
        "    for idx, row in tqdm(drugs_df.iterrows(), total=len(drugs_df), desc=\"Converting SMILES\"):\n",
        "        drug_name = row[drug_name_col]\n",
        "        smiles = row[smiles_col]\n",
        "        \n",
        "        graph = smiles_to_graph(smiles)\n",
        "        \n",
        "        if graph is not None:\n",
        "            drug_graphs[drug_name] = {\n",
        "                'drug_name': drug_name,\n",
        "                'smiles': smiles,\n",
        "                'graph_data': graph,\n",
        "                'node_dim': graph.x.shape[1],\n",
        "                'edge_dim': graph.edge_attr.shape[1] if graph.edge_attr is not None else 0,\n",
        "                'num_atoms': graph.x.shape[0],\n",
        "                'num_bonds': graph.edge_index.shape[1]\n",
        "            }\n",
        "        else:\n",
        "            failed_drugs.append(drug_name)\n",
        "    \n",
        "    with open(save_path, 'wb') as f:\n",
        "        pickle.dump(drug_graphs, f)\n",
        "    \n",
        "    print(f\"Converted {len(drug_graphs)}/{len(drugs_df)} drugs to graphs\")\n",
        "    \n",
        "    return drug_graphs\n",
        "\n",
        "# Preprocess all drugs\n",
        "output_dir = Path(\"/Users/tjalling/Desktop/Dev./Capstone/Model_Notebooks\")\n",
        "drug_graphs_path = output_dir / \"drug_graphs.pkl\"\n",
        "\n",
        "drug_graphs = preprocess_drugs(\n",
        "    drugs_with_smiles,\n",
        "    drugs_with_smiles.columns[0],\n",
        "    'SMILES',\n",
        "    drug_graphs_path\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fe71ca9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DrugEncoder test passed: output shape torch.Size([1, 256])\n"
          ]
        }
      ],
      "source": [
        "# Test DrugEncoder\n",
        "\n",
        "aspirin_smiles = \"CC(=O)OC1=CC=CC=C1C(=O)O\"\n",
        "aspirin_graph = smiles_to_graph(aspirin_smiles)\n",
        "\n",
        "if aspirin_graph is not None:\n",
        "    encoder = DrugEncoder()\n",
        "    encoder.eval()\n",
        "    \n",
        "    aspirin_graph.batch = torch.zeros(aspirin_graph.x.shape[0], dtype=torch.long)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        embedding = encoder(aspirin_graph)\n",
        "    \n",
        "    assert embedding.shape == (1, 256), f\"Expected shape (1, 256), got {embedding.shape}\"\n",
        "    assert not torch.isnan(embedding).any(), \"Output contains NaN values\"\n",
        "    \n",
        "    print(f\"DrugEncoder test passed: output shape {embedding.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8d168a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell Encoder Architecture\n",
        "\n",
        "class CellEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Feedforward neural network to encode cell line gene expression.\n",
        "    Uses skip connection to preserve direct gene signal.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim=3000, hidden_dim=512, output_dim=256, dropout1=0.4, dropout2=0.3):\n",
        "        \"\"\"\n",
        "        Initialize CellEncoder.\n",
        "        \n",
        "        Args:\n",
        "            input_dim: Number of gene expression features\n",
        "            hidden_dim: First hidden layer dimension\n",
        "            output_dim: Output embedding dimension\n",
        "            dropout1: Dropout rate for first layer\n",
        "            dropout2: Dropout rate for second layer\n",
        "        \"\"\"\n",
        "        super(CellEncoder, self).__init__()\n",
        "        \n",
        "        # Device selection\n",
        "        if torch.backends.mps.is_available():\n",
        "            self.device = torch.device('mps')\n",
        "        elif torch.cuda.is_available():\n",
        "            self.device = torch.device('cuda')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "        \n",
        "        # Main pathway\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout1)\n",
        "        \n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.bn2 = nn.BatchNorm1d(output_dim)\n",
        "        self.dropout2 = nn.Dropout(dropout2)\n",
        "        \n",
        "        # Skip connection\n",
        "        self.skip = nn.Linear(input_dim, output_dim)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "        self.to(self.device)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through cell encoder.\n",
        "        \n",
        "        Args:\n",
        "            x: Gene expression tensor of shape (batch_size, 3000)\n",
        "            \n",
        "        Returns:\n",
        "            Cell embeddings of shape (batch_size, 256)\n",
        "        \"\"\"\n",
        "        x = x.to(self.device, non_blocking=True)\n",
        "        \n",
        "        # Main pathway\n",
        "        out = self.fc1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout1(out)\n",
        "        \n",
        "        out = self.fc2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout2(out)\n",
        "        \n",
        "        # Skip connection\n",
        "        skip_out = self.skip(x)\n",
        "        \n",
        "        # Residual connection\n",
        "        embedding = out + skip_out\n",
        "        \n",
        "        return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d901ad4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input Normalization Utilities\n",
        "\n",
        "def normalize_expression(expression_data, method='check'):\n",
        "    \"\"\"\n",
        "    Check and normalize gene expression data if needed.\n",
        "    \n",
        "    Args:\n",
        "        expression_data: Gene expression tensor or array (samples × genes)\n",
        "        method: 'check' to analyze, 'zscore' to force normalization\n",
        "        \n",
        "    Returns:\n",
        "        Normalized expression data\n",
        "    \"\"\"\n",
        "    if isinstance(expression_data, pd.DataFrame):\n",
        "        expression_data = expression_data.values\n",
        "    \n",
        "    if isinstance(expression_data, np.ndarray):\n",
        "        expression_data = torch.tensor(expression_data, dtype=torch.float32)\n",
        "    \n",
        "    mean = expression_data.mean().item()\n",
        "    std = expression_data.std().item()\n",
        "    \n",
        "    if method == 'check':\n",
        "        if abs(mean) > 10 or std > 5:\n",
        "            expression_data = (expression_data - expression_data.mean(dim=1, keepdim=True)) / (expression_data.std(dim=1, keepdim=True) + 1e-8)\n",
        "            print(f\"Applied row-wise normalization (mean={mean:.2f}, std={std:.2f} -> normalized)\")\n",
        "    \n",
        "    elif method == 'zscore':\n",
        "        expression_data = (expression_data - expression_data.mean(dim=1, keepdim=True)) / (expression_data.std(dim=1, keepdim=True) + 1e-8)\n",
        "        print(f\"Applied z-score normalization\")\n",
        "    \n",
        "    return expression_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00642bd0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CellEncoder test passed: output shape torch.Size([10, 256])\n"
          ]
        }
      ],
      "source": [
        "# Test CellEncoder\n",
        "\n",
        "np.random.seed(42)\n",
        "sample_expression = np.random.normal(loc=2.5, scale=1.5, size=(10, 3000))\n",
        "sample_expression = torch.tensor(sample_expression, dtype=torch.float32)\n",
        "\n",
        "sample_expression = normalize_expression(sample_expression, method='check')\n",
        "\n",
        "cell_encoder = CellEncoder(input_dim=3000, output_dim=256)\n",
        "cell_encoder.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    embeddings = cell_encoder(sample_expression)\n",
        "\n",
        "assert embeddings.shape == (10, 256), f\"Expected shape (10, 256), got {embeddings.shape}\"\n",
        "assert not torch.isnan(embeddings).any(), \"Output contains NaN values\"\n",
        "\n",
        "print(f\"CellEncoder test passed: output shape {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b2385cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drug Response Prediction Model\n",
        "\n",
        "class DrugResponseGNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete GNN model for drug response prediction with multi-task learning.\n",
        "    Combines DrugEncoder and CellEncoder with three prediction heads.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, drug_node_dim=9, drug_edge_dim=4, cell_input_dim=3000, \n",
        "                 hidden_dim=256, dropout=0.3):\n",
        "        \"\"\"\n",
        "        Initialize DrugResponseGNN.\n",
        "        \n",
        "        Args:\n",
        "            drug_node_dim: Number of atom features\n",
        "            drug_edge_dim: Number of bond features\n",
        "            cell_input_dim: Number of gene expression features\n",
        "            hidden_dim: Embedding dimension (256)\n",
        "            dropout: Dropout rate\n",
        "        \"\"\"\n",
        "        super(DrugResponseGNN, self).__init__()\n",
        "        \n",
        "        # Device selection\n",
        "        if torch.backends.mps.is_available():\n",
        "            self.device = torch.device('mps')\n",
        "        elif torch.cuda.is_available():\n",
        "            self.device = torch.device('cuda')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "        \n",
        "        # Encoders\n",
        "        self.drug_encoder = DrugEncoder(\n",
        "            node_feature_dim=drug_node_dim,\n",
        "            edge_feature_dim=drug_edge_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        self.cell_encoder = CellEncoder(\n",
        "            input_dim=cell_input_dim,\n",
        "            hidden_dim=512,\n",
        "            output_dim=hidden_dim,\n",
        "            dropout1=0.4,\n",
        "            dropout2=0.3\n",
        "        )\n",
        "        \n",
        "        # Attention mechanism for integration\n",
        "        self.attention_query = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.attention_key = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.attention_value = nn.Linear(hidden_dim, hidden_dim)\n",
        "        \n",
        "        # Combined embedding dimension\n",
        "        combined_dim = hidden_dim * 2  # 512\n",
        "        \n",
        "        # Head A: IC50 Regression (primary task)\n",
        "        self.ic50_head = nn.Sequential(\n",
        "            nn.Linear(combined_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        \n",
        "        # Head B: Sensitivity Classification (auxiliary task)\n",
        "        self.classification_head = nn.Sequential(\n",
        "            nn.Linear(combined_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 1)\n",
        "        )\n",
        "        \n",
        "        # Head C: Similarity Reconstruction (auxiliary task)\n",
        "        self.reconstruction_head = nn.Sequential(\n",
        "            nn.Linear(combined_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, combined_dim)\n",
        "        )\n",
        "        \n",
        "        self.to(self.device)\n",
        "    \n",
        "    def integrate_with_attention(self, drug_emb, cell_emb):\n",
        "        \"\"\"\n",
        "        Integrate drug and cell embeddings using attention mechanism.\n",
        "        \n",
        "        Args:\n",
        "            drug_emb: Drug embeddings (batch_size, 256)\n",
        "            cell_emb: Cell embeddings (batch_size, 256)\n",
        "            \n",
        "        Returns:\n",
        "            Combined embedding (batch_size, 512)\n",
        "        \"\"\"\n",
        "        # Drug embedding as query\n",
        "        query = self.attention_query(drug_emb)  # (batch, 256)\n",
        "        key = self.attention_key(cell_emb)      # (batch, 256)\n",
        "        value = self.attention_value(cell_emb)  # (batch, 256)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        attention_scores = torch.matmul(query.unsqueeze(1), key.unsqueeze(2))  # (batch, 1, 1)\n",
        "        attention_scores = attention_scores / (drug_emb.size(-1) ** 0.5)\n",
        "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
        "        \n",
        "        # Apply attention to cell embedding\n",
        "        attended_cell = attention_weights.squeeze(-1) * value  # (batch, 1) * (batch, 256) -> (batch, 256)\n",
        "        \n",
        "        # Concatenate drug and attended cell embeddings\n",
        "        combined = torch.cat([drug_emb, attended_cell], dim=1)  # (batch, 512)\n",
        "        \n",
        "        return combined\n",
        "    \n",
        "    def forward(self, drug_batch, cell_batch, return_embeddings=False):\n",
        "        \"\"\"\n",
        "        Forward pass through complete model.\n",
        "        \n",
        "        Args:\n",
        "            drug_batch: PyTorch Geometric batch of molecular graphs\n",
        "            cell_batch: Gene expression tensor (batch_size, 3000)\n",
        "            return_embeddings: Whether to return intermediate embeddings\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary with predictions and optionally embeddings\n",
        "        \"\"\"\n",
        "        # Encode drug and cell\n",
        "        drug_emb = self.drug_encoder(drug_batch)  # (batch_size, 256)\n",
        "        cell_emb = self.cell_encoder(cell_batch)   # (batch_size, 256)\n",
        "        \n",
        "        # Integrate with attention\n",
        "        combined = self.integrate_with_attention(drug_emb, cell_emb)  # (batch_size, 512)\n",
        "        \n",
        "        # Multi-task predictions\n",
        "        ic50_pred = self.ic50_head(combined)\n",
        "        class_pred = self.classification_head(combined)\n",
        "        recon_pred = self.reconstruction_head(combined)\n",
        "        \n",
        "        results = {\n",
        "            'ic50': ic50_pred,\n",
        "            'classification': class_pred,\n",
        "            'reconstruction': recon_pred\n",
        "        }\n",
        "        \n",
        "        if return_embeddings:\n",
        "            results['embeddings'] = {\n",
        "                'drug': drug_emb,\n",
        "                'cell': cell_emb,\n",
        "                'combined': combined\n",
        "            }\n",
        "        \n",
        "        return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f01276",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Utilities\n",
        "\n",
        "def compute_loss(predictions, targets, median_ic50, loss_weights=(1.0, 0.5, 0.3)):\n",
        "    \"\"\"\n",
        "    Compute multi-task loss.\n",
        "    \n",
        "    Args:\n",
        "        predictions: Dictionary with 'ic50', 'classification', 'reconstruction'\n",
        "        targets: Dictionary with 'ic50', 'embeddings'\n",
        "        median_ic50: Median IC50 value for classification\n",
        "        loss_weights: Tuple of (ic50_weight, class_weight, recon_weight)\n",
        "        \n",
        "    Returns:\n",
        "        total_loss: Combined loss\n",
        "        loss_dict: Individual losses\n",
        "    \"\"\"\n",
        "    # IC50 regression loss (MSE)\n",
        "    ic50_loss = nn.functional.mse_loss(predictions['ic50'], targets['ic50'])\n",
        "    \n",
        "    # Classification loss (BCE)\n",
        "    class_labels = (targets['ic50'] < median_ic50).float()\n",
        "    class_loss = nn.functional.binary_cross_entropy_with_logits(\n",
        "        predictions['classification'], \n",
        "        class_labels\n",
        "    )\n",
        "    \n",
        "    # Reconstruction loss (MSE)\n",
        "    target_embeddings = targets['embeddings']\n",
        "    recon_loss = nn.functional.mse_loss(predictions['reconstruction'], target_embeddings)\n",
        "    \n",
        "    # Weighted combination\n",
        "    total_loss = (loss_weights[0] * ic50_loss + \n",
        "                  loss_weights[1] * class_loss + \n",
        "                  loss_weights[2] * recon_loss)\n",
        "    \n",
        "    loss_dict = {\n",
        "        'total': total_loss.item(),\n",
        "        'ic50': ic50_loss.item(),\n",
        "        'classification': class_loss.item(),\n",
        "        'reconstruction': recon_loss.item()\n",
        "    }\n",
        "    \n",
        "    return total_loss, loss_dict\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute evaluation metrics.\n",
        "    \n",
        "    Args:\n",
        "        y_true: True IC50 values (numpy array or tensor)\n",
        "        y_pred: Predicted IC50 values (numpy array or tensor)\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary of metrics\n",
        "    \"\"\"\n",
        "    # Handle both tensor and numpy inputs\n",
        "    if hasattr(y_true, 'cpu'):\n",
        "        y_true = y_true.cpu().numpy()\n",
        "    if hasattr(y_pred, 'cpu'):\n",
        "        y_pred = y_pred.cpu().numpy()\n",
        "    \n",
        "    y_true = np.asarray(y_true).flatten()\n",
        "    y_pred = np.asarray(y_pred).flatten()\n",
        "    \n",
        "    pearson_corr, _ = pearsonr(y_true, y_pred)\n",
        "    spearman_corr, _ = spearmanr(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    \n",
        "    return {\n",
        "        'pearson': pearson_corr,\n",
        "        'spearman': spearman_corr,\n",
        "        'r2': r2,\n",
        "        'rmse': rmse,\n",
        "        'mae': mae\n",
        "    }\n",
        "\n",
        "def print_model_summary(model):\n",
        "    \"\"\"\n",
        "    Print model architecture summary.\n",
        "    \n",
        "    Args:\n",
        "        model: DrugResponseGNN instance\n",
        "    \"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    print(\"MODEL SUMMARY\")\n",
        "    print(f\"Device: {model.device}\")\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Memory footprint: ~{total_params * 4 / 1024**2:.2f} MB\")\n",
        "    print(\"\\nArchitecture:\")\n",
        "    print(\"  DrugEncoder: SMILES → 256-dim\")\n",
        "    print(\"  CellEncoder: 3000 genes → 256-dim\")\n",
        "    print(\"  Attention Integration: 256 + 256 → 512-dim\")\n",
        "    print(\"  IC50 Head: 512 → 256 → 128 → 1\")\n",
        "    print(\"  Classification Head: 512 → 256 → 1\")\n",
        "    print(\"  Reconstruction Head: 512 → 256 → 512\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36f79920",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized on mps\n",
            "\n",
            "Output shapes:\n",
            "  IC50: torch.Size([4, 1])\n",
            "  Classification: torch.Size([4, 1])\n",
            "  Reconstruction: torch.Size([4, 512])\n",
            "  Drug embedding: torch.Size([4, 256])\n",
            "  Cell embedding: torch.Size([4, 256])\n",
            "  Combined embedding: torch.Size([4, 512])\n",
            "\n",
            "All tests passed!\n",
            "IC50 predictions: [0.04182592034339905, 0.007889561355113983, 0.05483219027519226, 0.030792269855737686]\n",
            "\n",
            "==================================================\n",
            "MODEL SUMMARY\n",
            "Device: mps\n",
            "Total parameters: 31,053,827\n",
            "Trainable parameters: 31,053,827\n",
            "Memory footprint: ~118.46 MB\n",
            "\n",
            "Architecture:\n",
            "  DrugEncoder: SMILES → 256-dim\n",
            "  CellEncoder: 3000 genes → 256-dim\n",
            "  Attention Integration: 256 + 256 → 512-dim\n",
            "  IC50 Head: 512 → 256 → 128 → 1\n",
            "  Classification Head: 512 → 256 → 1\n",
            "  Reconstruction Head: 512 → 256 → 512\n"
          ]
        }
      ],
      "source": [
        "# Test Complete Pipeline\n",
        "\n",
        "# Create model\n",
        "model = DrugResponseGNN()\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model initialized on {model.device}\\n\")\n",
        "\n",
        "# Test data\n",
        "batch_size = 4\n",
        "\n",
        "# Drug data: Create batch of aspirin graphs\n",
        "aspirin_smiles = \"CC(=O)OC1=CC=CC=C1C(=O)O\"\n",
        "test_graphs = []\n",
        "for _ in range(batch_size):\n",
        "    graph = smiles_to_graph(aspirin_smiles)\n",
        "    if graph is not None:\n",
        "        test_graphs.append(graph)\n",
        "\n",
        "drug_batch = Batch.from_data_list(test_graphs)\n",
        "\n",
        "# Cell data: Random expression\n",
        "np.random.seed(42)\n",
        "cell_data = np.random.normal(loc=2.5, scale=1.5, size=(batch_size, 3000))\n",
        "cell_batch = torch.tensor(cell_data, dtype=torch.float32)\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(drug_batch, cell_batch, return_embeddings=True)\n",
        "\n",
        "# Verify outputs\n",
        "print(\"Output shapes:\")\n",
        "print(f\"  IC50: {outputs['ic50'].shape}\")\n",
        "print(f\"  Classification: {outputs['classification'].shape}\")\n",
        "print(f\"  Reconstruction: {outputs['reconstruction'].shape}\")\n",
        "print(f\"  Drug embedding: {outputs['embeddings']['drug'].shape}\")\n",
        "print(f\"  Cell embedding: {outputs['embeddings']['cell'].shape}\")\n",
        "print(f\"  Combined embedding: {outputs['embeddings']['combined'].shape}\")\n",
        "\n",
        "# Check for NaNs\n",
        "has_nan = any(torch.isnan(outputs[key]).any() for key in ['ic50', 'classification', 'reconstruction'])\n",
        "assert not has_nan, \"Outputs contain NaN values\"\n",
        "\n",
        "# Verify shapes\n",
        "assert outputs['ic50'].shape == (batch_size, 1), f\"IC50 shape mismatch\"\n",
        "assert outputs['classification'].shape == (batch_size, 1), f\"Classification shape mismatch\"\n",
        "assert outputs['reconstruction'].shape == (batch_size, 512), f\"Reconstruction shape mismatch\"\n",
        "\n",
        "print(\"\\nAll tests passed!\")\n",
        "print(f\"IC50 predictions: {outputs['ic50'].squeeze().tolist()}\")\n",
        "\n",
        "# Model summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print_model_summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc6de78b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model initialized on mps\n",
            "\n",
            "Output shapes:\n",
            "  IC50: torch.Size([4, 1])\n",
            "  Classification: torch.Size([4, 1])\n",
            "  Reconstruction: torch.Size([4, 512])\n",
            "  Drug embedding: torch.Size([4, 256])\n",
            "  Cell embedding: torch.Size([4, 256])\n",
            "  Combined embedding: torch.Size([4, 512])\n",
            "\n",
            "All tests passed!\n",
            "IC50 predictions: [-0.0026915408670902252, -0.057257648557424545, -0.028083961457014084, 0.014925692230463028]\n",
            "\n",
            "==================================================\n",
            "MODEL SUMMARY\n",
            "Device: mps\n",
            "Total parameters: 31,053,827\n",
            "Trainable parameters: 31,053,827\n",
            "Memory footprint: ~118.46 MB\n",
            "\n",
            "Architecture:\n",
            "  DrugEncoder: SMILES → 256-dim\n",
            "  CellEncoder: 3000 genes → 256-dim\n",
            "  Attention Integration: 256 + 256 → 512-dim\n",
            "  IC50 Head: 512 → 256 → 128 → 1\n",
            "  Classification Head: 512 → 256 → 1\n",
            "  Reconstruction Head: 512 → 256 → 512\n"
          ]
        }
      ],
      "source": [
        "# Test Complete Pipeline\n",
        "\n",
        "# Create model\n",
        "model = DrugResponseGNN()\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model initialized on {model.device}\\n\")\n",
        "\n",
        "# Test data\n",
        "batch_size = 4\n",
        "\n",
        "# Drug data: Create batch of aspirin graphs\n",
        "aspirin_smiles = \"CC(=O)OC1=CC=CC=C1C(=O)O\"\n",
        "test_graphs = []\n",
        "for _ in range(batch_size):\n",
        "    graph = smiles_to_graph(aspirin_smiles)\n",
        "    if graph is not None:\n",
        "        test_graphs.append(graph)\n",
        "\n",
        "drug_batch = Batch.from_data_list(test_graphs)\n",
        "\n",
        "# Cell data: Random expression\n",
        "np.random.seed(42)\n",
        "cell_data = np.random.normal(loc=2.5, scale=1.5, size=(batch_size, 3000))\n",
        "cell_batch = torch.tensor(cell_data, dtype=torch.float32)\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(drug_batch, cell_batch, return_embeddings=True)\n",
        "\n",
        "# Verify outputs\n",
        "print(\"Output shapes:\")\n",
        "print(f\"  IC50: {outputs['ic50'].shape}\")\n",
        "print(f\"  Classification: {outputs['classification'].shape}\")\n",
        "print(f\"  Reconstruction: {outputs['reconstruction'].shape}\")\n",
        "print(f\"  Drug embedding: {outputs['embeddings']['drug'].shape}\")\n",
        "print(f\"  Cell embedding: {outputs['embeddings']['cell'].shape}\")\n",
        "print(f\"  Combined embedding: {outputs['embeddings']['combined'].shape}\")\n",
        "\n",
        "# Check for NaNs\n",
        "has_nan = any(torch.isnan(outputs[key]).any() for key in ['ic50', 'classification', 'reconstruction'])\n",
        "assert not has_nan, \"Outputs contain NaN values\"\n",
        "\n",
        "# Verify shapes\n",
        "assert outputs['ic50'].shape == (batch_size, 1), f\"IC50 shape mismatch\"\n",
        "assert outputs['classification'].shape == (batch_size, 1), f\"Classification shape mismatch\"\n",
        "assert outputs['reconstruction'].shape == (batch_size, 512), f\"Reconstruction shape mismatch\"\n",
        "\n",
        "print(\"\\nAll tests passed!\")\n",
        "print(f\"IC50 predictions: {outputs['ic50'].squeeze().tolist()}\")\n",
        "\n",
        "# Model summary\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print_model_summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac2174ba",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training pipeline functions defined\n"
          ]
        }
      ],
      "source": [
        "# Three-Phase Transfer Learning Pipeline\n",
        "\n",
        "# Setup directories\n",
        "output_dir = Path(\"/Users/tjalling/Desktop/Dev./Capstone/Model_Notebooks\")\n",
        "models_dir = output_dir / \"models\"\n",
        "results_dir = output_dir / \"results\"\n",
        "prebatched_dir = output_dir / \"prebatched_data\"\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "results_dir.mkdir(exist_ok=True)\n",
        "prebatched_dir.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "class DrugResponseDataset(Dataset):\n",
        "    \"\"\"Dataset for drug-cell response prediction.\"\"\"\n",
        "    \n",
        "    def __init__(self, dataframe, drug_graphs_dict, drug_col='DRUG_NAME'):\n",
        "        \"\"\"\n",
        "        Initialize dataset.\n",
        "        \n",
        "        Args:\n",
        "            dataframe: DataFrame with drug-cell pairs\n",
        "            drug_graphs_dict: Dictionary mapping drug names to graph objects\n",
        "            drug_col: Column name for drug identifier\n",
        "        \"\"\"\n",
        "        self.data = dataframe.reset_index(drop=True)\n",
        "        self.drug_graphs = drug_graphs_dict\n",
        "        self.drug_col = drug_col\n",
        "        \n",
        "        # Extract gene expression columns\n",
        "        self.gene_cols = [c for c in self.data.columns if '(' in c and ')' in c]\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        \n",
        "        # Get drug graph from dictionary\n",
        "        drug_name = row[self.drug_col]\n",
        "        drug_entry = self.drug_graphs[drug_name]\n",
        "        \n",
        "        # Extract actual graph Data object\n",
        "        if isinstance(drug_entry, dict) and 'graph_data' in drug_entry:\n",
        "            drug_graph = drug_entry['graph_data']\n",
        "        elif isinstance(drug_entry, Data):\n",
        "            drug_graph = drug_entry\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown drug graph format for {drug_name}\")\n",
        "        \n",
        "        # Get cell expression\n",
        "        cell_expr = torch.tensor(\n",
        "            row[self.gene_cols].values.astype(np.float32),\n",
        "            dtype=torch.float32\n",
        "        )\n",
        "        \n",
        "        # Get target\n",
        "        ic50 = torch.tensor([row['LN_IC50']], dtype=torch.float32)\n",
        "        \n",
        "        return {\n",
        "            'drug_graph': drug_graph,\n",
        "            'cell_expr': cell_expr,\n",
        "            'ic50': ic50,\n",
        "            'drug_name': drug_name,\n",
        "            'cell_id': row['ModelID']\n",
        "        }\n",
        "\n",
        "\n",
        "class PreBatchedDataset(Dataset):\n",
        "    \"\"\"Dataset that loads pre-computed batches directly.\"\"\"\n",
        "    \n",
        "    def __init__(self, batch_file):\n",
        "        \"\"\"\n",
        "        Initialize pre-batched dataset.\n",
        "        \n",
        "        Args:\n",
        "            batch_file: Path to file containing pre-batched data\n",
        "        \"\"\"\n",
        "        self.batches = torch.load(batch_file, weights_only=False)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.batches)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.batches[idx]\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function for drug graphs.\"\"\"\n",
        "    # Extract components from batch\n",
        "    drug_graphs = [item['drug_graph'] for item in batch]\n",
        "    cell_exprs = torch.stack([item['cell_expr'] for item in batch])\n",
        "    ic50s = torch.stack([item['ic50'] for item in batch])\n",
        "    \n",
        "    # Batch drug graphs properly\n",
        "    drug_batch = Batch.from_data_list(drug_graphs)\n",
        "    \n",
        "    return {\n",
        "        'drug_batch': drug_batch,\n",
        "        'cell_batch': cell_exprs,\n",
        "        'ic50': ic50s\n",
        "    }\n",
        "\n",
        "\n",
        "def prebatched_collate_fn(batch):\n",
        "    \"\"\"Collate function for pre-batched data - just returns the batch as-is.\"\"\"\n",
        "    # batch is a list with a single pre-computed batch dictionary\n",
        "    return batch[0]\n",
        "\n",
        "\n",
        "def create_prebatched_data(dataset, batch_size, split_name, phase_name, shuffle=False):\n",
        "    \"\"\"\n",
        "    Pre-compute and save batched data to disk.\n",
        "    \n",
        "    Args:\n",
        "        dataset: DrugResponseDataset\n",
        "        batch_size: Batch size\n",
        "        split_name: Name of split (train/val/test)\n",
        "        phase_name: Name of training phase (phase1/phase2/phase3)\n",
        "        shuffle: Whether to shuffle before batching\n",
        "        \n",
        "    Returns:\n",
        "        Path to saved batch file\n",
        "    \"\"\"\n",
        "    indices = np.arange(len(dataset))\n",
        "    if shuffle:\n",
        "        np.random.seed(42)\n",
        "        np.random.shuffle(indices)\n",
        "    \n",
        "    batches = []\n",
        "    \n",
        "    # Create batches\n",
        "    for i in tqdm(range(0, len(indices), batch_size), desc=f\"Pre-batching {split_name}\"):\n",
        "        batch_indices = indices[i:i+batch_size]\n",
        "        \n",
        "        # Get items for this batch\n",
        "        batch_items = [dataset[idx] for idx in batch_indices]\n",
        "        \n",
        "        # Extract and batch components\n",
        "        drug_graphs = [item['drug_graph'] for item in batch_items]\n",
        "        cell_exprs = torch.stack([item['cell_expr'] for item in batch_items])\n",
        "        ic50s = torch.stack([item['ic50'] for item in batch_items])\n",
        "        \n",
        "        # Batch drug graphs (the slow operation we're pre-computing)\n",
        "        drug_batch = Batch.from_data_list(drug_graphs)\n",
        "        \n",
        "        # Store pre-computed batch\n",
        "        batches.append({\n",
        "            'drug_batch': drug_batch,\n",
        "            'cell_batch': cell_exprs,\n",
        "            'ic50': ic50s\n",
        "        })\n",
        "    \n",
        "    # Save to disk\n",
        "    batch_file = prebatched_dir / f\"{phase_name}_{split_name}_batches.pt\"\n",
        "    torch.save(batches, batch_file)\n",
        "    print(f\"Saved {len(batches)} batches to {batch_file}\")\n",
        "    \n",
        "    return batch_file\n",
        "\n",
        "\n",
        "def create_dataloaders(dataset, batch_size, stratify_col=None, use_prebatched=False, phase_name=None):\n",
        "    \"\"\"\n",
        "    Create train/val/test splits and dataloaders.\n",
        "    \n",
        "    Args:\n",
        "        dataset: DrugResponseDataset\n",
        "        batch_size: Batch size for training\n",
        "        stratify_col: Column name for stratification\n",
        "        use_prebatched: Whether to use pre-batched data\n",
        "        phase_name: Name of phase for pre-batched data (phase1/phase2/phase3)\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary with train/val/test loaders (and datasets if not using prebatched)\n",
        "    \"\"\"\n",
        "    if use_prebatched:\n",
        "        # Load pre-batched data directly\n",
        "        train_file = prebatched_dir / f\"{phase_name}_train_batches.pt\"\n",
        "        val_file = prebatched_dir / f\"{phase_name}_val_batches.pt\"\n",
        "        test_file = prebatched_dir / f\"{phase_name}_test_batches.pt\"\n",
        "        \n",
        "        train_dataset = PreBatchedDataset(train_file)\n",
        "        val_dataset = PreBatchedDataset(val_file)\n",
        "        test_dataset = PreBatchedDataset(test_file)\n",
        "        \n",
        "        # DataLoader with prebatched data (batch_size=1 since each item is already a batch)\n",
        "        # num_workers=0 because data is already in memory and workers can't pickle notebook classes\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=1,\n",
        "            shuffle=True,\n",
        "            collate_fn=prebatched_collate_fn,\n",
        "            num_workers=0,\n",
        "            pin_memory=False\n",
        "        )\n",
        "        \n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=1,\n",
        "            shuffle=False,\n",
        "            collate_fn=prebatched_collate_fn,\n",
        "            num_workers=0,\n",
        "            pin_memory=False\n",
        "        )\n",
        "        \n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=1,\n",
        "            shuffle=False,\n",
        "            collate_fn=prebatched_collate_fn,\n",
        "            num_workers=0,\n",
        "            pin_memory=False\n",
        "        )\n",
        "        \n",
        "        print(f\"Loaded pre-batched data: Train: {len(train_dataset)} batches | Val: {len(val_dataset)} batches | Test: {len(test_dataset)} batches\")\n",
        "        \n",
        "        return {\n",
        "            'train': train_loader,\n",
        "            'val': val_loader,\n",
        "            'test': test_loader\n",
        "        }\n",
        "    \n",
        "    # Original approach: create datasets on-the-fly\n",
        "    data = dataset.data\n",
        "    \n",
        "    # Try stratification, fall back to random if it fails\n",
        "    stratify = None\n",
        "    if stratify_col and stratify_col in data.columns:\n",
        "        # Check if stratification is feasible (need at least 2 samples per class)\n",
        "        value_counts = data[stratify_col].value_counts()\n",
        "        if value_counts.min() >= 2:\n",
        "            stratify = data[stratify_col]\n",
        "        else:\n",
        "            print(f\"Warning: Cannot stratify by {stratify_col}, falling back to random split\")\n",
        "    \n",
        "    # Split: 70% train, 15% val, 15% test\n",
        "    try:\n",
        "        train_idx, temp_idx = train_test_split(\n",
        "            np.arange(len(data)),\n",
        "            test_size=0.3,\n",
        "            stratify=stratify,\n",
        "            random_state=42\n",
        "        )\n",
        "        \n",
        "        val_idx, test_idx = train_test_split(\n",
        "            temp_idx,\n",
        "            test_size=0.5,\n",
        "            stratify=stratify.iloc[temp_idx] if stratify is not None else None,\n",
        "            random_state=42\n",
        "        )\n",
        "    except ValueError:\n",
        "        # Stratification failed, use random split\n",
        "        print(\"Stratification failed, using random split\")\n",
        "        train_idx, temp_idx = train_test_split(\n",
        "            np.arange(len(data)),\n",
        "            test_size=0.3,\n",
        "            random_state=42\n",
        "        )\n",
        "        val_idx, test_idx = train_test_split(\n",
        "            temp_idx,\n",
        "            test_size=0.5,\n",
        "            random_state=42\n",
        "        )\n",
        "    \n",
        "    # Create subset datasets\n",
        "    train_data = data.iloc[train_idx]\n",
        "    val_data = data.iloc[val_idx]\n",
        "    test_data = data.iloc[test_idx]\n",
        "    \n",
        "    train_dataset = DrugResponseDataset(train_data, dataset.drug_graphs, dataset.drug_col)\n",
        "    val_dataset = DrugResponseDataset(val_data, dataset.drug_graphs, dataset.drug_col)\n",
        "    test_dataset = DrugResponseDataset(test_data, dataset.drug_graphs, dataset.drug_col)\n",
        "    \n",
        "    # If phase_name is provided, create pre-batched data\n",
        "    if phase_name:\n",
        "        print(f\"\\nCreating pre-batched data for {phase_name}...\")\n",
        "        create_prebatched_data(train_dataset, batch_size, 'train', phase_name, shuffle=False)\n",
        "        create_prebatched_data(val_dataset, batch_size, 'val', phase_name, shuffle=False)\n",
        "        create_prebatched_data(test_dataset, batch_size, 'test', phase_name, shuffle=False)\n",
        "        print(f\"Pre-batching complete! Rerun with use_prebatched=True to use cached data.\\n\")\n",
        "    \n",
        "    # Create loaders - use standard DataLoader with custom collate\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=8,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=8,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn,\n",
        "        num_workers=8,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    \n",
        "    print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n",
        "    \n",
        "    return {\n",
        "        'train': train_loader,\n",
        "        'val': val_loader,\n",
        "        'test': test_loader,\n",
        "        'datasets': {\n",
        "            'train': train_dataset,\n",
        "            'val': val_dataset,\n",
        "            'test': test_dataset\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, device, median_ic50=None):\n",
        "    \"\"\"\n",
        "    Evaluate model on a dataset.\n",
        "    \n",
        "    Args:\n",
        "        model: DrugResponseGNN model\n",
        "        dataloader: DataLoader\n",
        "        device: torch device\n",
        "        median_ic50: Median IC50 for classification threshold\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary of metrics and predictions\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    \n",
        "    all_preds = []\n",
        "    all_targets = []\n",
        "    total_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Move to device\n",
        "            drug_batch = batch['drug_batch'].to(device)\n",
        "            cell_batch = batch['cell_batch'].to(device)\n",
        "            ic50_target = batch['ic50'].to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(drug_batch, cell_batch)\n",
        "            \n",
        "            # Compute validation loss\n",
        "            if median_ic50 is not None:\n",
        "                outputs_with_emb = model(drug_batch, cell_batch, return_embeddings=True)\n",
        "                targets = {\n",
        "                    'ic50': ic50_target,\n",
        "                    'embeddings': outputs_with_emb['embeddings']['combined'].detach()\n",
        "                }\n",
        "                loss, _ = compute_loss(outputs_with_emb, targets, median_ic50)\n",
        "                total_loss += loss.item()\n",
        "            \n",
        "            # Collect predictions\n",
        "            all_preds.append(outputs['ic50'].cpu())\n",
        "            all_targets.append(ic50_target.cpu())\n",
        "    \n",
        "    all_preds = torch.cat(all_preds).numpy().flatten()\n",
        "    all_targets = torch.cat(all_targets).numpy().flatten()\n",
        "    \n",
        "    # Compute metrics\n",
        "    metrics = compute_metrics(all_targets, all_preds)\n",
        "    metrics['loss'] = total_loss / len(dataloader)\n",
        "    \n",
        "    return metrics, all_preds, all_targets\n",
        "\n",
        "\n",
        "def train_epoch(model, train_loader, optimizer, device, median_ic50, max_grad_norm=1.0):\n",
        "    \"\"\"\n",
        "    Train for one epoch with multi-task learning.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "    for batch in pbar:\n",
        "        # Move to device\n",
        "        drug_batch = batch['drug_batch'].to(device)\n",
        "        cell_batch = batch['cell_batch'].to(device)\n",
        "        ic50_target = batch['ic50'].to(device)\n",
        "        \n",
        "        # Forward pass with embeddings (compute once, not twice)\n",
        "        outputs = model(drug_batch, cell_batch, return_embeddings=True)\n",
        "        \n",
        "        # Get embeddings for reconstruction target\n",
        "        combined_emb = outputs['embeddings']['combined']\n",
        "        \n",
        "        targets = {\n",
        "            'ic50': ic50_target,\n",
        "            'embeddings': combined_emb.detach()\n",
        "        }\n",
        "        \n",
        "        loss, loss_dict = compute_loss(outputs, targets, median_ic50)\n",
        "        \n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def train_phase(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    device,\n",
        "    phase_name,\n",
        "    num_epochs,\n",
        "    lr,\n",
        "    weight_decay,\n",
        "    patience,\n",
        "    scheduler_patience,\n",
        "    checkpoint_path\n",
        "):\n",
        "    \"\"\"\n",
        "    Train one phase of the pipeline.\n",
        "    \n",
        "    Args:\n",
        "        model: DrugResponseGNN\n",
        "        train_loader: Training DataLoader\n",
        "        val_loader: Validation DataLoader\n",
        "        device: torch device\n",
        "        phase_name: Name of phase for logging\n",
        "        num_epochs: Number of epochs to train\n",
        "        lr: Learning rate\n",
        "        weight_decay: Weight decay\n",
        "        patience: Early stopping patience\n",
        "        scheduler_patience: LR scheduler patience\n",
        "        checkpoint_path: Path to save best model\n",
        "        \n",
        "    Returns:\n",
        "        Trained model and training history\n",
        "    \"\"\"\n",
        "    # Setup optimizer and scheduler\n",
        "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay, betas=(0.9, 0.999))\n",
        "    scheduler = ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',\n",
        "        factor=0.5,\n",
        "        patience=scheduler_patience,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "    \n",
        "    # Compute median IC50 from training data\n",
        "    all_ic50 = []\n",
        "    for batch in train_loader:\n",
        "        all_ic50.append(batch['ic50'])\n",
        "    median_ic50 = torch.cat(all_ic50).median().item()\n",
        "    \n",
        "    # Training history\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'val_r2': [],\n",
        "        'val_pearson': [],\n",
        "        'val_spearman': []\n",
        "    }\n",
        "    \n",
        "    best_val_r2 = -np.inf\n",
        "    epochs_without_improvement = 0\n",
        "    \n",
        "    print(f\"\\nStarting {phase_name}\")\n",
        "    print(f\"Learning rate: {lr}, Weight decay: {weight_decay}\")\n",
        "    print(f\"Median IC50: {median_ic50:.3f}\\n\")\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        \n",
        "        # Train\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, device, median_ic50)\n",
        "        \n",
        "        # Validate\n",
        "        val_metrics, _, _ = evaluate_model(model, val_loader, device, median_ic50)\n",
        "        \n",
        "        # Update history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_metrics['loss'])\n",
        "        history['val_r2'].append(val_metrics['r2'])\n",
        "        history['val_pearson'].append(val_metrics['pearson'])\n",
        "        history['val_spearman'].append(val_metrics['spearman'])\n",
        "        \n",
        "        # Print metrics\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_metrics['loss']:.4f}\")\n",
        "        print(f\"Val R²: {val_metrics['r2']:.4f} | Pearson: {val_metrics['pearson']:.4f} | \"\n",
        "              f\"Spearman: {val_metrics['spearman']:.4f} | RMSE: {val_metrics['rmse']:.4f}\")\n",
        "        \n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(val_metrics['r2'])\n",
        "        \n",
        "        # Check for improvement\n",
        "        if val_metrics['r2'] > best_val_r2:\n",
        "            best_val_r2 = val_metrics['r2']\n",
        "            epochs_without_improvement = 0\n",
        "            \n",
        "            # Save checkpoint\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_r2': best_val_r2,\n",
        "                'val_metrics': val_metrics,\n",
        "                'history': history\n",
        "            }, checkpoint_path)\n",
        "            \n",
        "            print(f\"✓ Saved best model (R² = {best_val_r2:.4f})\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "            print(f\"No improvement for {epochs_without_improvement} epoch(s)\")\n",
        "        \n",
        "        # Early stopping\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"\\nEarly stopping after {epoch+1} epochs\")\n",
        "            break\n",
        "        \n",
        "        print()\n",
        "    \n",
        "    # Load best model\n",
        "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    \n",
        "    print(f\"Completed {phase_name}\")\n",
        "    print(f\"Best validation R²: {checkpoint['val_r2']:.4f}\\n\")\n",
        "    \n",
        "    return model, history\n",
        "\n",
        "\n",
        "def plot_training_history(history, phase_name, save_path):\n",
        "    \"\"\"Plot training history.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    \n",
        "    # Loss\n",
        "    axes[0].plot(history['train_loss'], label='Train')\n",
        "    axes[0].plot(history['val_loss'], label='Val')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].set_title(f'{phase_name} - Loss')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # R²\n",
        "    axes[1].plot(history['val_r2'])\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('R²')\n",
        "    axes[1].set_title(f'{phase_name} - Validation R²')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def save_predictions(preds, targets, phase_name, save_path):\n",
        "    \"\"\"Save predictions and create scatter plot.\"\"\"\n",
        "    # Save CSV\n",
        "    df = pd.DataFrame({\n",
        "        'predicted': preds,\n",
        "        'actual': targets\n",
        "    })\n",
        "    csv_path = str(save_path).replace('.png', '.csv')\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    \n",
        "    # Create scatter plot\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.scatter(targets, preds, alpha=0.3, s=10)\n",
        "    \n",
        "    # Perfect prediction line\n",
        "    min_val = min(targets.min(), preds.min())\n",
        "    max_val = max(targets.max(), preds.max())\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect prediction')\n",
        "    \n",
        "    # Calculate metrics\n",
        "    from scipy.stats import pearsonr\n",
        "    r, _ = pearsonr(targets, preds)\n",
        "    r2 = r2_score(targets, preds)\n",
        "    \n",
        "    plt.xlabel('Actual LN_IC50')\n",
        "    plt.ylabel('Predicted LN_IC50')\n",
        "    plt.title(f'{phase_name} - Test Set\\nR² = {r2:.3f}, Pearson r = {r:.3f}')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "print(\"Training pipeline functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b84640",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Caffeinate wrapper ready - your Mac will stay awake during training!\n"
          ]
        }
      ],
      "source": [
        "# Keep Mac Awake During Training\n",
        "\n",
        "def keep_awake_wrapper(func):\n",
        "    \"\"\"\n",
        "    Wrapper to keep Mac awake during long-running operations.\n",
        "    Uses caffeinate to prevent system sleep.\n",
        "    \"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        print(\"Starting caffeinate to prevent system sleep...\")\n",
        "        print(\"Your Mac will stay awake until training completes.\\n\")\n",
        "        \n",
        "        # Start caffeinate process\n",
        "        # -i: Prevent idle sleep\n",
        "        # -d: Prevent display sleep\n",
        "        # -m: Prevent disk sleep\n",
        "        caffeinate_process = subprocess.Popen(\n",
        "            ['caffeinate', '-dims'],\n",
        "            stdout=subprocess.DEVNULL,\n",
        "            stderr=subprocess.DEVNULL\n",
        "        )\n",
        "        \n",
        "        try:\n",
        "            # Run the actual training\n",
        "            result = func(*args, **kwargs)\n",
        "            return result\n",
        "        finally:\n",
        "            # Stop caffeinate when done\n",
        "            caffeinate_process.terminate()\n",
        "            caffeinate_process.wait()\n",
        "            print(\"\\n✓ Caffeinate stopped. System can now sleep normally.\")\n",
        "    \n",
        "    return wrapper\n",
        "\n",
        "print(\"Caffeinate wrapper ready - your Mac will stay awake during training!\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# PRE-BATCHING SETUP\n",
        "# =============================================================================\n",
        "\n",
        "def create_all_prebatched_data():\n",
        "    \"\"\"\n",
        "    Create pre-batched data for all phases.\n",
        "    Run this once, then use use_prebatched=True in training.\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"CREATING PRE-BATCHED DATA FOR ALL PHASES\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"This will take a few minutes but only needs to be done once.\\n\")\n",
        "    \n",
        "    drug_col = drug_name_col\n",
        "    \n",
        "    # Phase 1: Pan-cancer\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PHASE 1: PAN-CANCER\")\n",
        "    print(\"=\"*70)\n",
        "    pan_dataset = DrugResponseDataset(pan_cancer_data, drug_graphs, drug_col=drug_col)\n",
        "    phase1_loaders = create_dataloaders(\n",
        "        pan_dataset, \n",
        "        batch_size=256, \n",
        "        stratify_col='OncotreePrimaryDisease' if 'OncotreePrimaryDisease' in pan_cancer_data.columns else None,\n",
        "        use_prebatched=False,\n",
        "        phase_name='phase1'\n",
        "    )\n",
        "    \n",
        "    # Phase 2: Breast/TNBC\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PHASE 2: BREAST/TNBC\")\n",
        "    print(\"=\"*70)\n",
        "    breast_dataset = DrugResponseDataset(breast_cancer_data, drug_graphs, drug_col=drug_col)\n",
        "    phase2_loaders = create_dataloaders(\n",
        "        breast_dataset,\n",
        "        batch_size=128,\n",
        "        stratify_col='ModelID',\n",
        "        use_prebatched=False,\n",
        "        phase_name='phase2'\n",
        "    )\n",
        "    \n",
        "    # Phase 3: BL1 TNBC\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"PHASE 3: BL1 TNBC\")\n",
        "    print(\"=\"*70)\n",
        "    bl1_dataset = DrugResponseDataset(bl1_tnbc_data, drug_graphs, drug_col=drug_col)\n",
        "    phase3_loaders = create_dataloaders(\n",
        "        bl1_dataset,\n",
        "        batch_size=128,\n",
        "        stratify_col='ModelID',\n",
        "        use_prebatched=False,\n",
        "        phase_name='phase3'\n",
        "    )\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"✓ PRE-BATCHING COMPLETE!\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"Pre-batched data saved to:\", prebatched_dir)\n",
        "    print(\"\\nNow run the training pipeline with use_prebatched=True for much faster training!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4caece46",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting caffeinate to prevent system sleep...\n",
            "Your Mac will stay awake until training completes.\n",
            "\n",
            "Loading datasets...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pan-cancer: 149093 pairs\n",
            "Breast/TNBC: 9601 pairs\n",
            "BL1 TNBC: 4303 pairs\n",
            "Drug graphs: 228 drugs\n",
            "\n",
            "Using device: mps\n",
            "\n",
            "======================================================================\n",
            "PHASE 1: PAN-CANCER PRE-TRAINING\n",
            "======================================================================\n",
            "Train: 104365 | Val: 22364 | Test: 22364\n",
            "\n",
            "Model parameters: 31,053,827\n"
          ]
        }
      ],
      "source": [
        "# Execute Three-Phase Training Pipeline (with caffeinate to prevent sleep)\n",
        "\n",
        "@keep_awake_wrapper\n",
        "def run_full_training_pipeline(use_prebatched=False):\n",
        "    \"\"\"\n",
        "    Run complete 3-phase training pipeline.\n",
        "    Mac will stay awake during entire process.\n",
        "    \n",
        "    Args:\n",
        "        use_prebatched: If True, use pre-batched data (much faster). \n",
        "                       If False, batch data on-the-fly (slower).\n",
        "    \"\"\"\n",
        "    print(f\"Loading datasets... (use_prebatched={use_prebatched})\")\n",
        "    \n",
        "    # Get the drug column name from data\n",
        "    drug_col = drug_name_col  # Already defined in earlier cells\n",
        "    \n",
        "    print(f\"Pan-cancer: {len(pan_cancer_data)} pairs\")\n",
        "    print(f\"Breast/TNBC: {len(breast_cancer_data)} pairs\")\n",
        "    print(f\"BL1 TNBC: {len(bl1_tnbc_data)} pairs\")\n",
        "    print(f\"Drug graphs: {len(drug_graphs)} drugs\\n\")\n",
        "    \n",
        "    # Device\n",
        "    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\\n\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # PHASE 1: PAN-CANCER PRE-TRAINING\n",
        "    # =============================================================================\n",
        "    print(\"=\"*70)\n",
        "    print(\"PHASE 1: PAN-CANCER PRE-TRAINING\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    if use_prebatched:\n",
        "        # Use pre-batched data\n",
        "        phase1_loaders = create_dataloaders(\n",
        "            None, \n",
        "            batch_size=256, \n",
        "            use_prebatched=True, \n",
        "            phase_name='phase1'\n",
        "        )\n",
        "        # Get n_genes from first batch\n",
        "        first_batch = next(iter(phase1_loaders['train']))\n",
        "        n_genes = first_batch['cell_batch'].shape[1]\n",
        "    else:\n",
        "        # Create dataset and loaders on-the-fly\n",
        "        pan_dataset = DrugResponseDataset(pan_cancer_data, drug_graphs, drug_col=drug_col)\n",
        "        n_genes = len(pan_dataset.gene_cols)\n",
        "        \n",
        "        stratify_col = 'OncotreePrimaryDisease' if 'OncotreePrimaryDisease' in pan_cancer_data.columns else 'ModelID'\n",
        "        phase1_loaders = create_dataloaders(\n",
        "            pan_dataset, \n",
        "            batch_size=256, \n",
        "            stratify_col=stratify_col,\n",
        "            use_prebatched=False\n",
        "        )\n",
        "    \n",
        "    print(f\"Gene features: {n_genes}\")\n",
        "    \n",
        "    # Initialize model with actual gene count\n",
        "    model = DrugResponseGNN(cell_input_dim=n_genes)\n",
        "    model = model.to(device)\n",
        "    \n",
        "    print(f\"\\nModel parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    # Train Phase 1\n",
        "    model, phase1_history = train_phase(\n",
        "        model=model,\n",
        "        train_loader=phase1_loaders['train'],\n",
        "        val_loader=phase1_loaders['val'],\n",
        "        device=device,\n",
        "        phase_name=\"Phase 1: Pan-Cancer\",\n",
        "        num_epochs=50,\n",
        "        lr=1e-3,\n",
        "        weight_decay=1e-5,\n",
        "        patience=20,\n",
        "        scheduler_patience=10,\n",
        "        checkpoint_path=models_dir / \"phase1_best.pt\"\n",
        "    )\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    print(\"Evaluating Phase 1 on test set...\")\n",
        "    phase1_test_metrics, phase1_preds, phase1_targets = evaluate_model(\n",
        "        model, phase1_loaders['test'], device\n",
        "    )\n",
        "    \n",
        "    print(f\"Phase 1 Test Metrics:\")\n",
        "    print(f\"  R²: {phase1_test_metrics['r2']:.4f}\")\n",
        "    print(f\"  Pearson: {phase1_test_metrics['pearson']:.4f}\")\n",
        "    print(f\"  Spearman: {phase1_test_metrics['spearman']:.4f}\")\n",
        "    print(f\"  RMSE: {phase1_test_metrics['rmse']:.4f}\")\n",
        "    print(f\"  MAE: {phase1_test_metrics['mae']:.4f}\\n\")\n",
        "    \n",
        "    # Save results\n",
        "    plot_training_history(phase1_history, \"Phase 1\", results_dir / \"phase1_history.png\")\n",
        "    save_predictions(phase1_preds, phase1_targets, \"Phase 1\", results_dir / \"phase1_predictions.png\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # PHASE 2: BREAST/TNBC FINE-TUNING\n",
        "    # =============================================================================\n",
        "    print(\"=\"*70)\n",
        "    print(\"PHASE 2: BREAST/TNBC FINE-TUNING\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Load best Phase 1 model\n",
        "    checkpoint = torch.load(models_dir / \"phase1_best.pt\", weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"Loaded Phase 1 best weights\\n\")\n",
        "    \n",
        "    # Create dataset and loaders\n",
        "    if use_prebatched:\n",
        "        phase2_loaders = create_dataloaders(\n",
        "            None,\n",
        "            batch_size=128,\n",
        "            use_prebatched=True,\n",
        "            phase_name='phase2'\n",
        "        )\n",
        "    else:\n",
        "        breast_dataset = DrugResponseDataset(breast_cancer_data, drug_graphs, drug_col=drug_col)\n",
        "        phase2_loaders = create_dataloaders(\n",
        "            breast_dataset,\n",
        "            batch_size=128,\n",
        "            stratify_col='ModelID',\n",
        "            use_prebatched=False\n",
        "        )\n",
        "    \n",
        "    # Train Phase 2\n",
        "    model, phase2_history = train_phase(\n",
        "        model=model,\n",
        "        train_loader=phase2_loaders['train'],\n",
        "        val_loader=phase2_loaders['val'],\n",
        "        device=device,\n",
        "        phase_name=\"Phase 2: Breast/TNBC\",\n",
        "        num_epochs=20,\n",
        "        lr=1e-4,  # 10x lower\n",
        "        weight_decay=1e-5,\n",
        "        patience=15,\n",
        "        scheduler_patience=5,\n",
        "        checkpoint_path=models_dir / \"phase2_best.pt\"\n",
        "    )\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    print(\"Evaluating Phase 2 on test set...\")\n",
        "    phase2_test_metrics, phase2_preds, phase2_targets = evaluate_model(\n",
        "        model, phase2_loaders['test'], device\n",
        "    )\n",
        "    \n",
        "    print(f\"Phase 2 Test Metrics:\")\n",
        "    print(f\"  R²: {phase2_test_metrics['r2']:.4f}\")\n",
        "    print(f\"  Pearson: {phase2_test_metrics['pearson']:.4f}\")\n",
        "    print(f\"  Spearman: {phase2_test_metrics['spearman']:.4f}\")\n",
        "    print(f\"  RMSE: {phase2_test_metrics['rmse']:.4f}\")\n",
        "    print(f\"  MAE: {phase2_test_metrics['mae']:.4f}\\n\")\n",
        "    \n",
        "    # Save results\n",
        "    plot_training_history(phase2_history, \"Phase 2\", results_dir / \"phase2_history.png\")\n",
        "    save_predictions(phase2_preds, phase2_targets, \"Phase 2\", results_dir / \"phase2_predictions.png\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # PHASE 3: BL1 FINE-TUNING\n",
        "    # =============================================================================\n",
        "    print(\"=\"*70)\n",
        "    print(\"PHASE 3: BL1 TNBC FINE-TUNING\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Load best Phase 2 model\n",
        "    checkpoint = torch.load(models_dir / \"phase2_best.pt\", weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(\"Loaded Phase 2 best weights\\n\")\n",
        "    \n",
        "    # Create dataset and loaders\n",
        "    if use_prebatched:\n",
        "        phase3_loaders = create_dataloaders(\n",
        "            None,\n",
        "            batch_size=128,\n",
        "            use_prebatched=True,\n",
        "            phase_name='phase3'\n",
        "        )\n",
        "    else:\n",
        "        bl1_dataset = DrugResponseDataset(bl1_tnbc_data, drug_graphs, drug_col=drug_col)\n",
        "        phase3_loaders = create_dataloaders(\n",
        "            bl1_dataset,\n",
        "            batch_size=128,\n",
        "            stratify_col='ModelID',\n",
        "            use_prebatched=False\n",
        "        )\n",
        "    \n",
        "    # Train Phase 3\n",
        "    model, phase3_history = train_phase(\n",
        "        model=model,\n",
        "        train_loader=phase3_loaders['train'],\n",
        "        val_loader=phase3_loaders['val'],\n",
        "        device=device,\n",
        "        phase_name=\"Phase 3: BL1\",\n",
        "        num_epochs=20,\n",
        "        lr=5e-5,  # 20x lower than Phase 1\n",
        "        weight_decay=1e-5,\n",
        "        patience=15,\n",
        "        scheduler_patience=5,\n",
        "        checkpoint_path=models_dir / \"phase3_bl1_final.pt\"\n",
        "    )\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    print(\"Evaluating Phase 3 on test set...\")\n",
        "    phase3_test_metrics, phase3_preds, phase3_targets = evaluate_model(\n",
        "        model, phase3_loaders['test'], device\n",
        "    )\n",
        "    \n",
        "    print(f\"Phase 3 Test Metrics:\")\n",
        "    print(f\"  R²: {phase3_test_metrics['r2']:.4f}\")\n",
        "    print(f\"  Pearson: {phase3_test_metrics['pearson']:.4f}\")\n",
        "    print(f\"  Spearman: {phase3_test_metrics['spearman']:.4f}\")\n",
        "    print(f\"  RMSE: {phase3_test_metrics['rmse']:.4f}\")\n",
        "    print(f\"  MAE: {phase3_test_metrics['mae']:.4f}\\n\")\n",
        "    \n",
        "    # Save results\n",
        "    plot_training_history(phase3_history, \"Phase 3\", results_dir / \"phase3_history.png\")\n",
        "    save_predictions(phase3_preds, phase3_targets, \"Phase 3\", results_dir / \"phase3_predictions.png\")\n",
        "\n",
        "    # =============================================================================\n",
        "    # FINAL RESULTS SUMMARY\n",
        "    # =============================================================================\n",
        "    print(\"=\"*70)\n",
        "    print(\"FINAL RESULTS SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    results_summary = pd.DataFrame({\n",
        "        'Phase': ['Phase 1: Pan-Cancer', 'Phase 2: Breast/TNBC', 'Phase 3: BL1'],\n",
        "        'Test R²': [\n",
        "            phase1_test_metrics['r2'],\n",
        "            phase2_test_metrics['r2'],\n",
        "            phase3_test_metrics['r2']\n",
        "        ],\n",
        "        'Test Pearson': [\n",
        "            phase1_test_metrics['pearson'],\n",
        "            phase2_test_metrics['pearson'],\n",
        "            phase3_test_metrics['pearson']\n",
        "        ],\n",
        "        'Test Spearman': [\n",
        "            phase1_test_metrics['spearman'],\n",
        "            phase2_test_metrics['spearman'],\n",
        "            phase3_test_metrics['spearman']\n",
        "        ],\n",
        "        'Test RMSE': [\n",
        "            phase1_test_metrics['rmse'],\n",
        "            phase2_test_metrics['rmse'],\n",
        "            phase3_test_metrics['rmse']\n",
        "        ],\n",
        "        'Test MAE': [\n",
        "            phase1_test_metrics['mae'],\n",
        "            phase2_test_metrics['mae'],\n",
        "            phase3_test_metrics['mae']\n",
        "        ]\n",
        "    })\n",
        "    \n",
        "    print(results_summary.to_string(index=False))\n",
        "    print()\n",
        "    \n",
        "    # Save summary\n",
        "    results_summary.to_csv(results_dir / \"final_results_summary.csv\", index=False)\n",
        "    \n",
        "    print(f\"All results saved to {results_dir}\")\n",
        "    print(f\"All models saved to {models_dir}\")\n",
        "    print(\"\\nTraining pipeline completed!\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# AUTOMATIC TRAINING PIPELINE\n",
        "# =============================================================================\n",
        "# Automatically creates pre-batched data if it doesn't exist, then trains.\n",
        "# Just hit \"Run All\" and let it work!\n",
        "# =============================================================================\n",
        "\n",
        "def run_automatic_pipeline():\n",
        "    \"\"\"\n",
        "    Automatically handle pre-batching and training.\n",
        "    If pre-batched data doesn't exist, create it first.\n",
        "    \"\"\"\n",
        "    # Check if pre-batched data exists\n",
        "    phase1_train = prebatched_dir / \"phase1_train_batches.pt\"\n",
        "    phase2_train = prebatched_dir / \"phase2_train_batches.pt\"\n",
        "    phase3_train = prebatched_dir / \"phase3_train_batches.pt\"\n",
        "    \n",
        "    all_exist = phase1_train.exists() and phase2_train.exists() and phase3_train.exists()\n",
        "    \n",
        "    if not all_exist:\n",
        "        print(\"=\" * 70)\n",
        "        print(\"PRE-BATCHED DATA NOT FOUND\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"Creating pre-batched data first (one-time setup, ~5-10 minutes)...\")\n",
        "        print(\"This will make all subsequent training runs 5-10x faster!\\n\")\n",
        "        create_all_prebatched_data()\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"✓ PRE-BATCHING COMPLETE! Starting training...\")\n",
        "        print(\"=\" * 70 + \"\\n\")\n",
        "    else:\n",
        "        print(\"=\" * 70)\n",
        "        print(\"✓ PRE-BATCHED DATA FOUND\")\n",
        "        print(\"=\" * 70)\n",
        "        print(\"Using existing pre-batched data for fast training!\\n\")\n",
        "    \n",
        "    # Run training with pre-batched data\n",
        "    run_full_training_pipeline(use_prebatched=True)\n",
        "\n",
        "# Run the automatic pipeline\n",
        "run_automatic_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "5215c611",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# RESUME FROM PHASE 2 (Phase 1 already completed)\n",
        "# =============================================================================\n",
        "# Just run this cell - it loads your saved Phase 1 model and continues training\n",
        "\n",
        "# Setup paths\n",
        "output_dir = Path(\"/Users/tjalling/Desktop/Dev./Capstone/Model_Notebooks\")\n",
        "models_dir = output_dir / \"models\"\n",
        "results_dir = output_dir / \"results\"\n",
        "prebatched_dir = output_dir / \"prebatched_data\"\n",
        "\n",
        "# Device\n",
        "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "# Load pre-batched data to get n_genes\n",
        "phase1_batches = torch.load(prebatched_dir / \"phase1_train_batches.pt\", weights_only=False)\n",
        "n_genes = phase1_batches[0]['cell_batch'].shape[1]\n",
        "print(f\"Gene features: {n_genes}\")\n",
        "del phase1_batches  # Free memory\n",
        "\n",
        "# Initialize model\n",
        "model = DrugResponseGNN(cell_input_dim=n_genes)\n",
        "model = model.to(device)\n",
        "\n",
        "# Load Phase 1 checkpoint\n",
        "checkpoint = torch.load(models_dir / \"phase1_best.pt\", weights_only=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"✓ Loaded Phase 1 model (Best Val R²: {checkpoint['val_r2']:.4f})\")\n",
        "print(f\"✓ Phase 1 completed at epoch {checkpoint['epoch']}\\n\")\n",
        "\n",
        "# =============================================================================\n",
        "# PHASE 2: BREAST/TNBC FINE-TUNING\n",
        "# =============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"PHASE 2: BREAST/TNBC FINE-TUNING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load Phase 2 pre-batched data\n",
        "phase2_train = torch.load(prebatched_dir / \"phase2_train_batches.pt\", weights_only=False)\n",
        "phase2_val = torch.load(prebatched_dir / \"phase2_val_batches.pt\", weights_only=False)\n",
        "phase2_test = torch.load(prebatched_dir / \"phase2_test_batches.pt\", weights_only=False)\n",
        "\n",
        "print(f\"Phase 2 data: Train {len(phase2_train)} | Val {len(phase2_val)} | Test {len(phase2_test)} batches\")\n",
        "\n",
        "# Compute median IC50\n",
        "all_ic50 = torch.cat([b['ic50'] for b in phase2_train])\n",
        "median_ic50_p2 = all_ic50.median().item()\n",
        "print(f\"Median IC50: {median_ic50_p2:.3f}\\n\")\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "best_val_r2 = -np.inf\n",
        "patience_counter = 0\n",
        "phase2_history = {'train_loss': [], 'val_r2': []}\n",
        "\n",
        "print(\"Starting Phase 2 training...\\n\")\n",
        "for epoch in range(20):\n",
        "    # Train\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(phase2_train, desc=f\"Epoch {epoch+1}/20\", leave=False):\n",
        "        drug_batch = batch['drug_batch'].to(device)\n",
        "        cell_batch = batch['cell_batch'].to(device)\n",
        "        ic50_target = batch['ic50'].to(device)\n",
        "        \n",
        "        outputs = model(drug_batch, cell_batch, return_embeddings=True)\n",
        "        targets = {'ic50': ic50_target, 'embeddings': outputs['embeddings']['combined'].detach()}\n",
        "        loss, _ = compute_loss(outputs, targets, median_ic50_p2)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    avg_loss = epoch_loss / len(phase2_train)\n",
        "    phase2_history['train_loss'].append(avg_loss)\n",
        "    \n",
        "    # Validate\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in phase2_val:\n",
        "            drug_batch = batch['drug_batch'].to(device)\n",
        "            cell_batch = batch['cell_batch'].to(device)\n",
        "            outputs = model(drug_batch, cell_batch)\n",
        "            all_preds.append(outputs['ic50'].cpu())\n",
        "            all_targets.append(batch['ic50'])\n",
        "    \n",
        "    preds = torch.cat(all_preds).numpy().flatten()\n",
        "    targets = torch.cat(all_targets).numpy().flatten()\n",
        "    val_r2 = r2_score(targets, preds)\n",
        "    phase2_history['val_r2'].append(val_r2)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f} | Val R²={val_r2:.4f}\")\n",
        "    \n",
        "    scheduler.step(val_r2)\n",
        "    \n",
        "    # Save best model\n",
        "    if val_r2 > best_val_r2:\n",
        "        best_val_r2 = val_r2\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'val_r2': val_r2\n",
        "        }, models_dir / \"phase2_best.pt\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= 15:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "# Load best Phase 2 model\n",
        "checkpoint = torch.load(models_dir / \"phase2_best.pt\", weights_only=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"\\n✓ Phase 2 complete! Best Val R²: {checkpoint['val_r2']:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# PHASE 3: BL1 TNBC FINE-TUNING\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PHASE 3: BL1 TNBC FINE-TUNING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load Phase 3 pre-batched data\n",
        "phase3_train = torch.load(prebatched_dir / \"phase3_train_batches.pt\", weights_only=False)\n",
        "phase3_val = torch.load(prebatched_dir / \"phase3_val_batches.pt\", weights_only=False)\n",
        "phase3_test = torch.load(prebatched_dir / \"phase3_test_batches.pt\", weights_only=False)\n",
        "\n",
        "print(f\"Phase 3 data: Train {len(phase3_train)} | Val {len(phase3_val)} | Test {len(phase3_test)} batches\")\n",
        "\n",
        "# Compute median IC50\n",
        "all_ic50 = torch.cat([b['ic50'] for b in phase3_train])\n",
        "median_ic50_p3 = all_ic50.median().item()\n",
        "print(f\"Median IC50: {median_ic50_p3:.3f}\\n\")\n",
        "\n",
        "# Setup optimizer (lower LR)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, min_lr=1e-6)\n",
        "\n",
        "best_val_r2 = -np.inf\n",
        "patience_counter = 0\n",
        "phase3_history = {'train_loss': [], 'val_r2': []}\n",
        "\n",
        "print(\"Starting Phase 3 training...\\n\")\n",
        "for epoch in range(20):\n",
        "    # Train\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(phase3_train, desc=f\"Epoch {epoch+1}/20\", leave=False):\n",
        "        drug_batch = batch['drug_batch'].to(device)\n",
        "        cell_batch = batch['cell_batch'].to(device)\n",
        "        ic50_target = batch['ic50'].to(device)\n",
        "        \n",
        "        outputs = model(drug_batch, cell_batch, return_embeddings=True)\n",
        "        targets = {'ic50': ic50_target, 'embeddings': outputs['embeddings']['combined'].detach()}\n",
        "        loss, _ = compute_loss(outputs, targets, median_ic50_p3)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    \n",
        "    avg_loss = epoch_loss / len(phase3_train)\n",
        "    phase3_history['train_loss'].append(avg_loss)\n",
        "    \n",
        "    # Validate\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in phase3_val:\n",
        "            drug_batch = batch['drug_batch'].to(device)\n",
        "            cell_batch = batch['cell_batch'].to(device)\n",
        "            outputs = model(drug_batch, cell_batch)\n",
        "            all_preds.append(outputs['ic50'].cpu())\n",
        "            all_targets.append(batch['ic50'])\n",
        "    \n",
        "    preds = torch.cat(all_preds).numpy().flatten()\n",
        "    targets = torch.cat(all_targets).numpy().flatten()\n",
        "    val_r2 = r2_score(targets, preds)\n",
        "    phase3_history['val_r2'].append(val_r2)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f} | Val R²={val_r2:.4f}\")\n",
        "    \n",
        "    scheduler.step(val_r2)\n",
        "    \n",
        "    # Save best model\n",
        "    if val_r2 > best_val_r2:\n",
        "        best_val_r2 = val_r2\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'val_r2': val_r2\n",
        "        }, models_dir / \"phase3_bl1_final.pt\")\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= 15:\n",
        "            print(\"Early stopping!\")\n",
        "            break\n",
        "\n",
        "# Load best Phase 3 model\n",
        "checkpoint = torch.load(models_dir / \"phase3_bl1_final.pt\", weights_only=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "print(f\"\\n✓ Phase 3 complete! Best Val R²: {checkpoint['val_r2']:.4f}\")\n",
        "\n",
        "# =============================================================================\n",
        "# FINAL EVALUATION\n",
        "# =============================================================================\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nModels saved to: {models_dir}\")\n",
        "print(\"  - phase1_best.pt (Pan-cancer)\")\n",
        "print(\"  - phase2_best.pt (Breast/TNBC)\")\n",
        "print(\"  - phase3_bl1_final.pt (BL1 TNBC - FINAL)\")\n",
        "print(\"\\n✓ All done! Your model is ready.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
